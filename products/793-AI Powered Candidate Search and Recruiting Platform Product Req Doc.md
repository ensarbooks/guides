# AI-Powered Candidate Search and Recruiting Platform – Product Requirements Document

## Introduction

This Product Requirements Document (PRD) outlines the key features and specifications for an AI-driven candidate search and recruiting platform. The platform, known as **PeopleGPT (Juicebox)**, leverages advanced AI (including large language models) and real-time data to help recruiting teams source, assess, and engage top talent more efficiently. It combines semantic **natural language search**, a vast **global talent pool**, AI-based candidate evaluation (**Autopilot**), and automated outreach capabilities into a single seamless tool. The document is structured into 20 feature sections. Each section provides an overview, functional requirements, user stories, UX/UI considerations, acceptance criteria, technical notes, success metrics, and competitive benchmarking for the feature. The intended audience is product management and engineering teams responsible for implementing and maintaining the platform. The goal is to ensure clarity of requirements and alignment with user needs and business objectives.

_(Note: Citations are provided to reference existing capabilities or industry context and should be preserved for traceability.)_

## 1. Natural Language Search

### Overview and Purpose

Natural Language Search allows recruiters to find qualified talent by simply describing the ideal candidate in plain English, **eliminating the need for complex Boolean queries**. Instead of manually constructing filters and keywords, the user can input queries like _“Senior engineers with experience building search infrastructure at top B2B software companies”_. The PeopleGPT engine interprets this description and translates it into a precise search across the talent database. The purpose of this feature is to dramatically simplify the search process – making it more intuitive and closer to how a recruiter might describe a candidate to a colleague. It lowers the learning curve (especially for non-technical users) and captures more nuanced criteria than a basic keyword search. In essence, Natural Language Search improves search accuracy and recruiter productivity by leveraging AI understanding of context and intent.

### Functional Requirements

- **Free-Form Query Input:** Provide a single search input field (or chat interface) where users can type unstructured descriptions of candidate criteria (e.g. skills, experience, education, location, accomplishments) in natural language.
- **AI Query Parsing:** The system must employ the PeopleGPT engine (large language model or similar) to parse the input and extract structured search parameters (e.g. job titles, industries, skills, company names, locations) from the description. It should handle multi-faceted descriptions in one query.
- **Semantic Understanding:** Support synonyms and related terms – e.g. if a user says “software development at FAANG companies,” the engine should understand _FAANG_ as a group of specific companies and _software development_ as a broad skill area. It should recognize context like “top-tier universities” or “high-growth startups” and translate these to appropriate filters or weighting.
- **Complex Criteria Handling:** The AI should correctly interpret compound criteria (e.g. _“with 5+ years in Python and either fintech or healthcare experience”_) and produce results meeting those conditions. Parenthetical or conditional logic expressed in natural terms should be resolved into an effective query structure.
- **Query Suggestion & Refinement:** After parsing, the system may present the interpreted criteria back to the user for confirmation or editing (e.g. show the recognized filters: title, location, companies, etc.). This gives the user control to adjust any misunderstandings. Users can then run the search or refine their query further by adding more natural language instructions.
- **Multi-Turn Conversations:** Enable follow-up queries or refinements in conversational form. For example, after an initial search the user might type “Only show me those with a PhD” or “What about in Europe?” and the system adjusts the results accordingly. The search interface should maintain context of the previous query for such refinements (like a chat memory of the search context).
- **Boolean & Filter Translation:** Under the hood, the natural query should map to the platform’s advanced filters. The user should not need to craft Boolean strings, but the system’s interpretation must be as powerful as a manual Boolean could be – covering inclusions, exclusions, and combinations as intended by the user. The translation should be transparent enough that the user can trust the results (possibly by showing a summary of applied filters).
- **Error Handling & Clarification:** If the AI is uncertain or the query is ambiguous, the system should either (a) make a reasonable assumption and allow the user to adjust, or (b) prompt the user for clarification in a friendly manner. For example, if a user says “Find me senior Java architects” it could clarify whether they mean Java the programming language or Java the Indonesian location if context is unclear.
- **No-Code Needed:** Ensure that users do not have to input special syntax or click multiple filters – the AI should capture as much as possible from the natural sentence itself. This makes the feature accessible to recruiters who are not experts in search logic.
- **Performance:** The natural language parsing and search results generation should happen in a reasonable time (a few seconds at most). The system should indicate it’s interpreting (e.g. “Understanding your request…”) if it needs more than a moment.
- **Internationalization:** Ideally, support natural language queries in languages beyond English (if global users). At minimum, support English queries that may include non-English proper nouns (like university names, company names, job titles in other languages) gracefully.

### User Stories

- _As a recruiter_, I want to **search for candidates by simply typing a sentence** describing my ideal candidate, so that I can quickly find relevant profiles without learning complex query syntax.
- _As a hiring manager (using the platform)_, I want to **describe the combination of skills and experience** our role needs in plain language, so that the system can show me some example candidates that fit our requirements.
- _As a sourcer_, I want to **narrow down a very specific candidate persona using one intuitive query** (e.g. “A data scientist in healthcare with 10+ years experience and a PhD”), so that I don’t have to apply multiple filters one by one or guess keywords – the system will understand it in one go.
- _As a new user unfamiliar with Boolean logic_, I want to **enter a recruiting query conversationally** (e.g. “Show me recent MBA graduates in marketing in the Chicago area”), so that I can get results without formal training and feel confident the AI interprets what I mean.
- _As a recruiter_, after seeing initial results I want to **refine my search by adding another criteria in plain language** (for example, “also, someone who has led a team of engineers”), so that I can iteratively get to the right set of candidates with minimal effort.

### UX/UI Considerations

- **Unified Search Bar or Chat Interface:** The main search input should encourage natural language queries. For example, placeholder text might read “Describe the candidate you’re looking for…” to signal that full sentences are welcome. Alternatively, a chat-style interface labeled “Ask PeopleGPT” could be used, showing the dialogue between user and PeopleGPT.
- **Real-Time Suggestions:** As the user types, the system may suggest auto-completions or recognized entities (e.g. if user types “Google”, it could suggest the company Google; if typing “engineers in”, it might suggest locations or known job titles). This helps users formulate queries and confirms that the system understands parts of their input.
- **Interpretation Preview:** After the user hits enter, the UI can display the AI’s interpreted filters. For instance, it might say: _“Searching for Senior Engineer (7+ years) in United States who worked at Elastic, Algolia or similar companies, with experience in search infrastructure (Apache Lucene)”_. This breakdown (possibly shown as chips or a short summary) gives transparency. The user can click on any interpreted filter (e.g. company names or skills) to modify or remove it if needed.
- **Example Queries:** Provide a list of example natural language searches on the interface (especially for new users) – e.g. _“Find product managers in fintech who have led teams of 5+”_, _“Show me designers from top e-commerce companies”_, etc. Clicking an example could run it or populate the search bar to demonstrate the feature.
- **Conversational Refinement:** If implementing a chat-like interface, each query and refinement appears as a conversation turn. PeopleGPT’s response can confirm understanding (“Sure, looking for data scientists with NLP experience…”) and then show results. Users can then type follow-up criteria. A traditional search-page layout might instead use filters side-panel; but a conversational UI can make the AI aspect more tangible. We should ensure whichever UI, it remains efficient for power users (not too slow/clunky to go through conversation if they already know what they want).
- **Error Feedback:** If a query is too broad/vague or too narrow (yielding few/no results), the UI should provide a gentle prompt or suggestion. For broad queries, maybe suggest adding a skill or location. For narrow queries, suggest which part might be relaxed (e.g. “No results found for ‘blockchain astrophysicist in Antarctica’. Try broadening location or skills.”). The tone should be helpful, possibly leveraging the conversational style of PeopleGPT to suggest a fix.
- **Loading Indicator:** While the AI parses the query, a short animation or message (like a thinking ellipsis or “Searching…” dialog) should be displayed. If initial interpretation is quick, it might go straight to results. If not, possibly show the intermediate query that’s being run for transparency.
- **Mobile Responsiveness:** Ensure the natural language input method works on mobile screens as well (if a mobile web or app is supported). The design should accommodate voice input as a future enhancement (speaking the query to search), given it’s natural language.

### Acceptance Criteria

- **AC1: Accurate Parsing:** Given a natural language query describing multiple candidate attributes, the system **correctly interprets all key criteria** and applies them to the search. For example, for “senior frontend developer in London fintech with React experience”, PeopleGPT should apply filters for seniority (senior-level roles), role (frontend developer or similar titles), location (London), industry (fintech companies), and skill (React). All these filters should reflect the query’s intent without requiring user correction.
- **AC2: No Boolean Required:** Users can obtain relevant results **without manually entering Boolean operators** or special syntax. In user testing, non-technical recruiters are able to generate useful candidate lists by typing sentences, and they report that they did not need Boolean knowledge to get good results (the AI covers it).
- **AC3: Ambiguity Handling:** If a query is ambiguous or incomplete, the system **handles it gracefully**. For example, if the user searches “Java engineer from Oxford”, the system distinguishes whether “Oxford” likely means Oxford University (education) or Oxford, UK (location) by context; if unclear, it either chooses the more likely interpretation or asks the user to clarify. The acceptance is that the system does not return obviously irrelevant results due to misinterpreting common ambiguities.
- **AC4: Multi-Criteria Support:** The search can combine at least **5 distinct criteria from a single natural query** (e.g. role, industry, location, skill, experience level) and still function correctly. There’s no hard limit where it fails silently; the AI should parse complex sentences with conjunctions and produce a valid result set.
- **AC5: Iterative Refinement:** Users are able to refine searches with follow-up natural language instructions, and the system updates results appropriately. For acceptance, if a user first searches broadly and then adds a specific requirement (“Actually, only those with a Master’s degree”), the second query narrows the results accordingly. The context from the first query is retained either automatically or through filter state.
- **AC6: Performance:** Natural language queries are processed and results returned with an average latency of **<3 seconds** for parsing and starting to display results (for typical query lengths). In the worst case (very long or complex query), it should not exceed \~8-10 seconds. The system should display results progressively if available or at least a confirmation of query understood if there is additional processing like ranking.
- **AC7: Coverage of Edge Cases:** The feature supports natural input for all common recruiting filters: location names, company names (including those with special characters or common words, e.g. “The Home Depot”), job titles (including acronyms like “CEO”), skills (including programming languages like C# or frameworks), education and degree terms, etc. This criterion is met if test queries covering these types return sensible filter mappings (e.g. “C# developer” doesn’t break parsing, “C#” is recognized as a skill; “San Francisco Bay Area” recognized as a region grouping cities).
- **AC8: User Satisfaction:** In UAT (user acceptance testing) or beta, recruiters who use Natural Language Search report **greater ease of use** compared to their previous Boolean search approach. Specifically, a success criterion could be a survey where >90% of participants agree that “The ability to search by describing a candidate in plain language made it easier to find the right candidates.” This indicates the feature meets its usability goal.

### Technical Notes

- **PeopleGPT NLP Engine:** The interpretation of natural language queries relies on a **Natural Language Processing (NLP) model** (likely a large language model fine-tuned for people search). When a query is entered, the system could either: (a) use the LLM to directly generate a list of structured filters (job title, skills, etc.), or (b) embed the query and perform a semantic search across profiles. The design currently leans on the former – using AI to map to existing filters for precision. For development, an API like OpenAI GPT-4 or a custom model can be used to parse queries into a JSON of search parameters. This may involve prompt engineering to ensure consistent output format. Example prompt: _“Extract location, required skills, job titles, and company preferences from the following search description…”_. The output would then be applied to the search engine query builder.
- **Semantic Search & Indexing:** In addition to strict filters, consider using **semantic vector search** to catch nuanced fit. For instance, if the user searches for “experienced in search infrastructure,” beyond keyword match, the engine could retrieve candidates whose profiles semantically align with that concept (maybe they mention Lucene, Solr, Elasticsearch, etc. even if not exact words). This can be done by maintaining an embedding index of profiles and doing a hybrid search (combining keyword filters with embedding similarity). However, this is an advanced addition – initially, the focus is on the AI interpreting into direct filters and keywords.
- **Data Schema & Ontologies:** The platform’s data covers many attributes (skill taxonomies, job titles hierarchy, company database). The NLP engine should leverage these dictionaries for better parsing. For example, maintain a list of known job titles so the AI can map “senior engineer” to a normalized title category. Similarly, an industry ontology helps if user says “fintech” (which might correspond to NAICS categories or a set of company names). We might integrate with existing ontologies or APIs (e.g. O\*NET for job titles/skills or a custom knowledge base of company industries and synonyms).
- **Continuous Learning:** Each search query and the user’s subsequent actions provide feedback. If users frequently adjust a certain misinterpreted filter, that’s a signal to improve the model. We should log queries and outcomes to refine the NLP. For example, if a user types “engineer at Series A startup” and the AI misses the “Series A” part, we capture that and update the model or add a rule (though from our docs, we do handle funding stage filters via AI). Possibly incorporate a feedback mechanism (“Was this search interpreted correctly? Yes/No”) to gather training data.
- **Multi-Language Considerations:** If supporting other languages in future, we’d need multilingual models or translation. For now, assume English input. However, names of things could be in other languages (e.g. “São Paulo” location, “Universität München” as education) – ensure Unicode and special characters are handled and that such terms either pass through or are recognized if they exist in profile data.
- **Integration with Filters UI:** The output of the AI parsing should seamlessly plug into the search system. For instance, if it identifies specific companies, those should map to the internal company IDs in our database (ensuring correct ones, e.g., if it says “Apple”, map to Apple Inc.’s entity, not any “Apple” keyword). Leverage our existing “smart search fields” capability where the user could type a description and hit an “Ask AI” option. In implementation, the natural language search might literally fill in those filter fields behind the scenes. The _Edit Filters_ panel can show what’s filled.
- **Performance & Caching:** Using an LLM for every query could be costly. We should implement caching for repeated or similar queries (many recruiters often search similar profiles). Also, if the initial parse is heavy, once converted to filters, subsequent result pagination or modifications shouldn’t require re-calling the LLM. Possibly do the LLM call only when the user types a new description, not on each filter tweak. If we have a semantic index alternative, ensure it’s efficient at the scale of hundreds of millions of profiles (which likely means using distributed vector indices and approximate nearest neighbor searches).
- **Limits & Abuse Prevention:** Because the input is free-form text, we should impose some limits. For example, maximum characters for a query (maybe 200-300 characters) to avoid extremely long prompts that could degrade performance or be used in unintended ways. Also, sanitize input to prevent any injection or misuse – though that’s less about code injection and more about handling odd inputs robustly. If someone types a very bizarre or off-topic query, the system should handle it (maybe yield no results or a polite message). We may also need filters to prevent searching discriminatory criteria (the AI should not allow or should at least flag queries like “age 25-30” or protected classes, to comply with fair hiring laws). The product should follow ethical guidelines in how AI interprets queries that might be sensitive.

### Success Metrics

- **Search Success Rate:** Track the percentage of natural language searches that yield a sufficiently rich result set (e.g. at least 50 profiles) on the first try. A high success rate (e.g. >90%) would indicate the parser is correctly understanding queries and our data can satisfy them. If many queries return zero or too few results, that’s an issue to address (either parsing or data coverage).
- **Reduction in Time to Search:** Measure the time or number of interactions it takes for a user to find a set of candidates. For example, compare a baseline where a user manually sets filters (which might take several minutes and trial-and-error) versus using natural language (which ideally takes seconds and one input). A metric could be _“average time to first page of results”_ or _“average number of query reformulations needed”_. The goal is a significant reduction (e.g. 50% faster to get to good results).
- **Feature Adoption:** Monitor what proportion of searches use natural language input vs. manual filter selection. If adoption is high (say within 3 months, >70% of searches on the platform start with an NL query), that’s a strong indicator of success. If adoption is low, we’d investigate if users find it unreliable or are unaware of it.
- **User Satisfaction Scores:** Through surveys or feedback forms specifically about search functionality. For instance, an in-app prompt could ask _“How easy was it to find relevant candidates using our search?”_ or _“Does the AI search understand your queries well?”_. Aim for a high satisfaction rating (e.g. 4.5/5). Also measure qualitative feedback – are users delighted by being able to “talk to” the search engine? Do they mention it saves effort?
- **Conversion to Candidates Contacted:** Ultimately, a successful search feature should translate to more candidates being found and contacted. We can track how many candidate profiles viewed or saved originate from natural language searches. An increase in sourced candidates per recruiter per week, after introducing this feature, would indicate ROI.
- **Error/Clarification Rate:** Log cases where the system had to ask the user for clarification or where the user had to significantly modify the query because the AI got it wrong. Ideally, this rate is low. For instance, <5% of NL searches result in user clicking “Edit filters” to fix something, or the AI explicitly saying “I’m not sure”. A decreasing trend here over time (with model improvements) would be a metric of learning success.
- **Competitive Differentiation:** If possible, glean comparative stats: e.g. if using this platform vs. LinkedIn Recruiter’s traditional search, recruiters can quantify that they spend fewer steps. Another measure – mention count in sales or marketing contexts: this feature should be a selling point (e.g. “80% of our users say PeopleGPT’s natural language search gives them better results than other platforms”). While not a direct metric, it reflects market success.

### Competitive Benchmarking

Natural Language Search is an emerging capability in recruitment tech. Traditional platforms like **LinkedIn Recruiter** rely heavily on Boolean strings and structured filters, which have a steep learning curve and can be time-consuming. In contrast, PeopleGPT’s approach of conversational search is cutting-edge – **“the AI search engine that understands who you’re looking for”**. Competitors such as **HireEZ (Hiretual)** and **SeekOut** have introduced some AI-assisted search (for example, allowing users to paste a job description or using some semantic keyword expansion), but they still often require tweaking filters. PeopleGPT aims to leapfrog these by truly understanding free text queries. According to industry reviews, PeopleGPT’s natural language capability means recruiters “aren’t coding in the 90s” with Boolean, but can _“just ask like you’re talking to a colleague”_.

Furthermore, our platform’s ability to interpret descriptors like _“top-tier SaaS companies”_ or _“Ivy League CS grads”_ showcases contextual understanding that competitors lack. Some niche tools or newer products are attempting similar NLP-driven search (for instance, a startup, along with PeopleGPT, was highlighted as revolutionizing recruitment search by removing the need for Boolean logic). However, PeopleGPT pairs this with one of the largest talent databases (800M profiles), giving it an edge in both brains and breadth.

We will continue to monitor competitors like LinkedIn (who may integrate their own AI search features or Copilot in the future) and specialized AI recruiting assistants. At present, PeopleGPT’s natural language search is a key differentiator that resonates with users frustrated by legacy search methods. Ensuring it remains highly accurate and user-friendly will maintain our competitive advantage in sourcing efficiency.

## 2. PeopleGPT Engine

### Overview and Purpose

The PeopleGPT Engine is the AI-powered core of the platform that interprets queries and retrieves candidate information intelligently. It is described as _“the AI search engine that understands who you’re looking for”_. In essence, this engine combines natural language understanding with a comprehensive people database to act like a smart recruiting assistant. The PeopleGPT Engine’s purpose is to bridge the gap between how humans describe talent needs and how computers search data. It not only powers the Natural Language Search feature (translating human queries into results), but also underlies other AI capabilities like profile matching, AI filters, and personalized outreach. By modeling a deep understanding of candidate attributes and roles, the PeopleGPT Engine aims to produce better matches than traditional keyword search, learning from context and user feedback over time. It serves as the “brain” of the platform, continuously improving its knowledge of professional profiles and job requirements.

### Functional Requirements

- **Query Comprehension:** The engine must parse and understand user inputs (queries, prompts) related to candidate search. This includes natural language queries (as covered in Feature 1) but also other prompts such as _follow-up questions, filtering instructions, or even entire job descriptions_. It should identify the intent (e.g. search for candidates, refine a search, evaluate a profile, generate an email, etc.) and relevant entities (skills, titles, companies, etc.) from any given input.
- **Holistic Candidate Representation:** Represent candidate profiles in a way that the AI can reason about them. This might involve creating rich embeddings for each profile (capturing their skills, experience, education, etc.) or using a knowledge graph of candidate attributes. The engine should be able to compare a query’s requirements to candidate profiles semantically, not just by literal keyword matches. For example, understanding that a “software engineer who led a team” implies leadership experience which might be indicated by “team lead” titles in profiles.
- **Multi-Source Data Integration:** The PeopleGPT Engine should aggregate data from **30+ sources** – e.g. professional networks, technical sites, publications – and unify them for search. This means if one candidate has a LinkedIn profile, GitHub contributions, and published papers, the engine links these together. Functionally, it requires merging data records and resolving identities so that the engine’s knowledge of a “profile” includes all relevant info (like skills from GitHub, or a paper title from Semantic Scholar).
- **Real-Time Indexing:** As new data comes in or updates occur (from our real-time aggregation), the PeopleGPT Engine should update its index of profiles promptly. If a candidate changes their job title or if a new skilled professional appears in our sources, the engine’s searchable index needs to reflect that with minimal lag. “Real-time” implies updates perhaps within hours or a day at most for major changes (subject to source API limits).
- **Intelligent Ranking & Scoring:** When a search query is executed, the engine should not just retrieve matching profiles but also **rank them intelligently** based on likely fit. This involves an AI-driven scoring that accounts for multiple factors (how many criteria matched, how important those criteria are, quality of the profile, recency of experience, etc.). This is partly handled by the Autopilot (Feature 4), but even before Autopilot runs, the engine’s initial result ordering should be sensible (e.g. it might boost profiles that strongly match the query phrase context). Essentially, the engine provides a relevance score for each profile for a given query.
- **Contextual Learning:** The engine should learn from user behavior and feedback. For example, if recruiters frequently click certain profiles for a given query or manually filter out others, the engine can adjust relevance weighting. Also, as Autopilot evaluations and Agent feedback accumulate (like profiles approved or rejected), the engine can incorporate those signals to refine future searches. Over time, it should improve understanding of what constitutes a “good match” for various common requests.
- **API and Extensibility:** Offer an internal API or service interface such that various parts of the product can leverage the PeopleGPT Engine. For example, the search bar uses it for queries, the “Job Description Upload” uses it to parse JDs into searches, the Agents use it to continuously find new candidates. The engine should thus support different input types: free text queries, structured filter criteria, partial profiles, etc., and return relevant profile sets or analysis. A standardized API (REST/GraphQL or Python library in our backend) should handle requests like `search_candidates(criteria)` or `match_job_description(text)` using the engine’s capabilities.
- **Scalability:** The engine must handle searching across hundreds of millions of profiles efficiently. It likely uses a combination of search index techniques (inverted indices for filters/keywords, vector indices for semantic match). Functional requirement is that it supports high concurrency (multiple recruiters searching simultaneously) and large result sets without significant slowdowns. The architecture might partition the data or use cloud search services as needed.
- **Profile Detailing:** The engine should not only retrieve profiles but also generate insight about them. For instance, it could highlight why a profile is a strong match (e.g. _“This candidate was recommended because they have 10 years in the field and recently published relevant research.”_). This overlaps with the Insights and Autopilot explanation features. Functionally, when requested, the engine can produce a short summary or “AI spotlight” for a profile given a context (like the job requirements). This requires the engine to cross-reference the profile data with the query criteria and articulate key points (likely via an integrated language model).
- **Adaptive Filtering:** In cases where a query yields too many results, the engine might proactively apply additional AI filters to hone the list. Conversely, if a query is too narrow, the engine can broaden it slightly using semantic understanding (for example, if zero results for “Python guru with 12 years at Google”, it might drop the employer filter or expand to similar companies rather than returning nothing). This adaptivity should ideally be subtle and controlled, possibly with user permission or as suggestions (“We expanded your search to similar companies since no results were found strictly at Google”).

### User Stories

- _As a product manager (for the platform)_, I want the **PeopleGPT Engine to interpret complex search queries** and retrieve the best candidates, so that our end-users (recruiters) get high-quality results without fiddling with many options. (This reflects an internal stakeholder view ensuring the engine’s role.)
- _As a recruiter_, I want **the search engine to truly “understand” the profile I need** – for example, if I search for “data engineer healthcare experience”, it should prioritize those who worked in healthcare domains even if their title just says “Data Engineer” without me explicitly saying “in healthcare”. The engine’s intelligence ensures I don’t miss great candidates due to exact keyword mismatches.
- _As a recruiter_, when I upload a job description or have a specialized requirement, I want the PeopleGPT Engine to **automatically generate a list of likely candidates** or at least a query for them, so that I save time translating requirements to search terms. It should feel like I have a knowledgeable assistant who knows where to look.
- _As a technical sourcer_, I want the engine to **integrate data from GitHub, Stack Overflow, and academic publications** into candidate profiles, so that I can find candidates with specific technical contributions (e.g. someone who has a top Stack Overflow rank in C++ or has published on AI). I rely on the engine to parse those external signals and surface the right people.
- _As a recruiter_, when I search and then use an AI ranking (Autopilot), I expect the PeopleGPT Engine to have **already done a lot of the heavy lifting in aligning candidates to my criteria**. For instance, if I mark certain profiles as good or bad, I anticipate the system will adjust future searches (via the engine’s learning) to better suit my preferences, effectively becoming more personalized or smarter over time.
- _As a hiring manager reviewing candidates_, I want the system (powered by the engine) to **provide a brief rationale for each recommended candidate** relative to our job, so that I have confidence in why the AI chose them. For example, seeing bullet points like “PhD from Harvard” or “5+ years in Kubernetes; recently promoted” on a candidate summary helps me quickly assess fit.

### UX/UI Considerations

_(Note: The PeopleGPT Engine itself is a backend component, but it manifests in various UI elements. This section describes how users perceive its intelligence across features.)_

- **PeopleGPT Branding:** The platform uses the term “PeopleGPT” to denote the AI capabilities. In the UI, when the engine is doing work, we might use subtle branding like _“Powered by PeopleGPT”_ or have a small icon/mascot that indicates AI suggestions. For example, next to the search bar or in tooltips on AI-driven features (like Autopilot results), to build user trust in the AI’s presence.
- **Interactive Search Experience:** When a user enters a query, the UI might show _“PeopleGPT is finding candidates…”_ to emphasize that an intelligent agent is at work. After results load, providing an “AI summary” of the query might help (e.g. “PeopleGPT searched for profiles matching: \[summary]”). This transparency was discussed in Natural Language Search UI; it’s the engine’s interpretation on display.
- **Explanations on Profiles:** In search results or in the Autopilot _spreadsheet view_, each profile might have an expandable section or tooltip “Why is this a match?” which the engine can fill in. For instance, hovering a small info icon could reveal the AI-highlighted bullet points: e.g., _“**Match highlights:** Worked 3 years at a Series B startup (fits high-growth criteria); Published a paper on transformer models (advanced technical insight); 5+ years with Kubernetes (meets skill requirement); Recently promoted to Senior Engineer (indicates strong performance)”_. These highlights, generated by the engine, give users quick insight. The UI should make these highlights clearly distinguishable (maybe styled differently or with an AI icon) and ensure they are concise.
- **Profile Cards and Feed:** The PeopleGPT 2.0 interface introduced a _Profile Feed with AI spotlight_. In practice, this could mean on the main search results page, instead of a plain list, we show a card for each candidate that includes a snippet generated by the engine. For example, under the candidate’s name and title, a short AI-generated sentence like _“10 years experience; ex-Google; expertise in cloud security”_ might appear. The UI has to balance these AI details with actual profile data. Possibly show 2-3 bullet points of AI-curated highlights (like the example above) within each result card to provide context at a glance.
- **Loading States & Partial Results:** Searching such a large dataset with AI might sometimes load in stages. The UI could first show preliminary results (maybe based on quick filters) and then refine or rerank them as the engine’s AI scoring completes. If so, indicate this subtly (like “refining results…”). It may also load the profile highlights asynchronously – initial results show basic info, and a few seconds later the “AI highlights” text appears as it’s generated. The design should accommodate that (e.g. a placeholder or shimmer effect where the summary will appear).
- **Agent and Autopilot Config:** When configuring Autopilot or Agents, the UI allows the user to set criteria (like AI filters). It should be clear to the user that PeopleGPT Engine will interpret those criteria. For example, in an Agent setup wizard, when the user enters a sentence about the role, maybe a message like _“Using AI to understand your role description…”_ appears, and then the system might list back what it inferred (like key skills or companies, which the user can tweak). This confirms the engine’s role in the background of agent configuration.
- **Error Feedback (AI-side):** If for some reason the engine fails to parse or find data (e.g., the query is out of scope of knowledge), the UI should present a helpful message. Possibly phrase it as _“PeopleGPT didn’t understand that request”_ or _“We couldn’t find profiles matching those exact criteria; try adjusting your description.”_ Keep the tone helpful and not overly technical (don’t say “Engine error” or similar).
- **Switch to Manual Controls:** Some power users might prefer direct filters at times. The UI should allow toggling or editing the engine’s output. For instance, after an AI interpretation, user can click _“Edit Filters”_ to manually adjust. The synergy between AI and manual controls is important – the UI should never feel like the AI is a black box they can’t override. In practice, ensure that all AI-driven search criteria show up in the normal filter UI where possible, so users can fine-tune. The PeopleGPT Engine’s suggestions can be pre-populated in those fields (like company, skill, etc.).
- **Confidence Indicators:** If the engine is unsure about a particular interpretation (like it’s guessing a category), the UI could mark that assumption. Perhaps a filter chip with a question mark or lighter color, indicating “AI guess”. For example, if user said “top companies”, the AI picks a list of known top companies. The UI could label it _“Top Companies (AI-selected)”_ so the user knows that was auto-filled and can review the list (maybe see which companies were included by clicking it).

### Acceptance Criteria

- **AC1: Accurate AI Interpretation:** The PeopleGPT Engine correctly interprets at least **95% of common recruiting queries** or instructions. This is evidenced by the output filters matching user intent (tested via a broad set of sample queries). For example, given input “Show me Ivy League alumni who are now product managers in fintech,” the engine should correctly identify Ivy League universities as an education filter, “product manager” titles, and fintech companies/industry as context – producing results that indeed reflect those criteria (as validated by user or QA).
- **AC2: End-to-End Search Relevance:** Searches powered by the engine consistently return highly relevant candidates in the top results. A possible acceptance test: For a given set of test search scenarios (covering various roles/industries), domain experts rate the top 10 results. We expect an average relevance score of e.g. 8/10 or higher, and that **at least 8 out of 10 profiles in the first page are considered good matches** by a recruiter. If the engine’s ranking frequently surfaces unrelated profiles at the top, it fails this criterion.
- **AC3: Multi-Source Profile Merging:** The engine successfully merges information from multiple data sources for a single candidate. Acceptance can be measured by spot-checking profiles that we know have multi-source data: e.g. a candidate who has a LinkedIn and a GitHub. In the PeopleGPT database, these should appear as one unified profile (not duplicates) and search queries should find that profile via info present in either source (for example, if the GitHub indicates a specific skill, searching by that skill should still show the unified profile). The criterion is met if data merging covers at least 90% of overlapping profiles without duplication.
- **AC4: Real-Time Updates Reflection:** If a candidate’s data changes (for instance, they change jobs on their LinkedIn profile or newly publish a paper), the PeopleGPT Engine’s search results reflect that change within a reasonable time (defined by product – say 24 hours maximum for major changes). For acceptance, we can simulate an update: add a new skill or job to a test profile data and ensure that after the update pipeline, queries for that skill or job title start returning the profile, and queries for their old info cease if relevant. Essentially, **data freshness** should meet the real-time expectation.
- **AC5: Scalability under Load:** The engine supports at least **100 concurrent complex searches** (with typical query complexity) with response times under e.g. 2 seconds for initial results (not including heavy Autopilot evaluation, just initial retrieval). We will load test the search API: simulate 100 users searching simultaneously, each querying across the full 800M profile index. The acceptance criteria is that the system maintains performance (no timeouts, and 95th percentile query latency <5 seconds). This ensures the engine can handle peak usage scenarios (like many recruiters searching at once on a Monday morning).
- **AC6: Explainability:** For any given search result that the engine provides, the system can produce at least one _explanation highlight_ (like a bullet point) why that profile matches. We test this by taking a sample search and checking that the UI (or an API output) provides match explanation bullets for the top results. Acceptance might be that **80% of top 10 results have non-trivial AI-generated highlights** that align with query criteria. For example, if the query is about Kubernetes experience, the highlight might be “5+ years experience in Kubernetes”. If the engine is unable to articulate any reason, that’s a failure in explainability.
- **AC7: Adaptability/learning:** After initial deployment, as an acceptance over time, the engine should show improvement on relevance or usage metrics (tie to success metrics). But for immediate acceptance, we can test a learning scenario: Suppose a user rejects certain profiles as not fitting. The engine (with Autopilot or Agents) should adapt by not recommending very similar profiles. Since real learning might need data, a simulated acceptance test could be using an Agent configuration step – if a user rejects 3 profiles and provides reasons, the agent’s subsequent suggestions (which rely on engine’s adaptive search) should not include profiles with those rejected traits. Essentially, a qualitative acceptance: the engine exhibits the ability to incorporate feedback (if we have a mechanism in place for it, which Agents do).

### Technical Notes

- **Architecture:** The PeopleGPT Engine likely comprises multiple sub-systems: a search index (for fast filtering and keyword search on structured data), an embedding-based semantic search component, and a natural language understanding component (LLM). For implementation, we might use a stack such as **Elasticsearch/OpenSearch** for indexing profiles (with custom analyzers for text, the ability to filter by fields, etc.), and integrate an **AI service** for semantic tasks. For example, we could use Elasticsearch’s built-in vector search for quick embedding matches and an external LLM (via API) for more complex reasoning. The interplay needs careful orchestration to meet performance goals.
- **Data Structure:** Each candidate profile is stored as a document containing various fields (name, title, experience, skills, education, etc., plus metadata from different sources). Additional fields for AI usage might include precomputed embeddings (one for the full profile summary, perhaps separate ones for skills, for job descriptions they've held, etc.). We also store derived attributes like “years of experience”, “career progression metric (promoted or not)”, “likely to switch score”, etc., which the engine uses for advanced filtering and insights.
- **Language Model Integration:** The engine uses a language model in two main ways: query parsing and content generation. For query parsing, we might initially rely on smaller scale solutions (like a rules-based parsing augmented with AI for certain parts) to avoid heavy API calls on every search. Alternatively, fine-tune a medium-sized model on thousands of example searches to get a more direct parser. For content generation (explanations, email text, summaries), GPT-4 or similar can be used on-demand. We should design the system to cache and reuse LLM outputs when possible (for example, an explanation for why a candidate is a fit for a given job could be cached if the same pairing is evaluated again). Also consider a **prompt library** where we maintain templates for different tasks (search parse, profile summary bullet, outreach personalization, etc.) to ensure consistency.
- **Advanced Filters & Engine Rules:** The engine incorporates special filters such as _funding stage, diversity flags, promotion history, specific achievements_. Under the hood, this requires additional data enrichment (like knowing company funding stage from an external source). Technical note: we have a **company database** with fields like funding_round, investor names, revenue, etc., which is linked to profiles via their employment. The engine must be able to filter on those fields. We might implement this as a separate index or as nested fields in the profile document (e.g., `profile.companies[ ].funding_stage`). The PeopleGPT Engine’s logic should be modular to incorporate these “power filters” when relevant. For example, if query or agent criteria includes “Series B startup”, the engine translates that to `company.funding_stage = Series B` filter on the profile’s company history.
- **Handling Scale of 800M Profiles:** This is a significant scale. We should consider sharding the search index by geography or by some key to distribute load. Also, ensuring that data retrieval doesn’t overwhelm memory – perhaps implement pagination strictly and avoid pulling full profiles until needed. The engine can retrieve just lightweight info (name, score, key highlights) for the initial results list, then load detailed data on demand (e.g. when opening a profile or running Autopilot on it). This means our data store might separate “profile summary for search” vs “full profile details for display” for efficiency.
- **Security & Privacy:** The engine must comply with privacy rules: e.g., if certain data sources have usage limitations, ensure we don’t expose or use them beyond allowed terms. Also, if a user’s organization has their own data (like imported ATS profiles), the engine should partition that – i.e., not show another company’s ATS profiles in search results. Multi-tenancy design is crucial: queries should be scoped to the data the user is allowed to see. In practice, public profiles are available to all, but any private or proprietary data integration should be isolated per account. The engine’s indexing might have to mark records with access control tags.
- **Testing & Evaluation:** Develop a suite of offline evaluations for the engine. For instance, we can create synthetic “ideal candidate” profiles and ensure the engine can find them given a description. Also test edge cases: e.g. very short queries (“Java London”) and very long ones (pasting a whole job spec). We should also evaluate the semantic search: if a profile doesn’t literally contain a keyword but is conceptually relevant, does the engine catch it? This might involve semantic similarity thresholds which we tune. Use known good pairings (maybe from hiring data or from user feedback) to see if the engine links them.
- **Continuous Improvement Pipeline:** Set up logging such that we capture anonymized query texts, selected filters, and results (including which were clicked or contacted). This data can feed into improving the PeopleGPT Engine’s algorithms periodically. For example, using these logs to fine-tune the LLM or update the embedding space if we identify blind spots. Possibly implement an automated evaluation every time we update the model, to ensure no regression in understanding or result quality.

### Success Metrics

- **Precision and Recall Metrics:** Define quantitative IR (Information Retrieval) metrics for the engine’s search results. For a curated set of test queries with known relevant candidates, compute Precision\@N (how many of the top N results are relevant) and Recall (how many of all relevant candidates were found by the engine at all). Success could be achieving >0.8 precision\@10 and similarly high recall for various categories of roles. Over time, we aim to improve these via engine tweaks.
- **Improved Recruiter Efficiency:** A key metric is reduction in time spent per successful hire or per candidate sourced. If the engine is doing its job, recruiters should find good candidates faster. We might measure _time from opening the platform to saving a shortlist of X candidates_. If historically that was 2 hours of work and now it’s 30 minutes with PeopleGPT, that’s a huge success (quantify e.g. a 75% reduction, which was actually cited: _users report cutting sourcing time by up to 75%_). We should gather case studies or user data to support such a statistic in practice.
- **User Engagement & Retention:** If PeopleGPT’s intelligence is truly helpful, we expect high engagement – e.g., increased frequency of searches per user (since it’s easier, they might run more queries exploring the talent pool). Also, retention of users/subscribers should improve if the engine consistently delivers value. Metrics: daily or weekly active usage of the search function (aim for a high percentage of users performing searches regularly). Also measure how many searches convert into next actions (like viewing profiles, adding to project) – a high conversion means the engine returns actionable results.
- **Quality of Match (Downstream Metrics):** Track the outcomes of candidates found by the engine: Do they proceed to interviews? Hires? While many factors influence that, if our engine surfaces higher-quality candidates, recruiters should see better pipelines. Perhaps compare the ratio of candidates contacted to those who get positive responses or interviews. If the engine’s matches are strong, that ratio should be better than in traditional methods. For instance, recruiters might say _“Usually I have to contact 50 people to get 5 responses, but with PeopleGPT’s targeted search I contact 30 to get 5 responses”_. Increased response rates and quality hires can be partially attributed to better initial matching (as well as outreach quality).
- **Adoption by Power Users:** A metric of success is if even seasoned recruiters (who might be skeptical of AI) begin relying on the PeopleGPT Engine rather than manual search methods. If we see that heavy users seldom revert to purely manual filtering or external tools, that indicates trust in the engine. We can survey users: _“Do you find the AI engine’s suggestions as good as or better than your own manual search?”_ – aiming for a strong majority saying yes. Another indicator: usage of features like Autopilot and Agents (which heavily depend on the engine) – high usage implies the engine is meeting needs automatically.
- **Platform Differentiation & Growth:** On a business level, PeopleGPT Engine’s capabilities should drive platform growth. Metrics such as increase in new user sign-ups or upgrades citing “AI search” as a reason can be tracked through sales feedback or user interviews. If we find in win/loss analysis against competitors that our engine’s functionality is frequently mentioned as a winning factor, that’s qualitative success evidence. We might also measure how often our engine (PeopleGPT) is mentioned in social media, reviews, etc., as a leader in AI recruiting search.

### Competitive Benchmarking

The PeopleGPT Engine stands at the intersection of AI and recruiting data. Competing platforms each have their own approach, but PeopleGPT’s advantage is integrating a **massive dataset (800+ million profiles)** with cutting-edge AI understanding. For example, **LinkedIn Recruiter** has a huge professional graph but uses fairly traditional filters and only recently has begun adding AI hints; it doesn’t truly “understand” a nuanced request in the way PeopleGPT does. **SeekOut** and **hireEZ** have large databases and some AI matching algorithms (often touting ML-based recommendations), but those are typically based on pattern matching and keyword expansion rather than generative language understanding. PeopleGPT uses a GPT-based model that can handle conversational input and complex logic, which is a generation ahead. In a review, PeopleGPT was described as _“an AI-driven recruiting platform that combines massive data access with intelligent search capabilities”_ – indeed bridging what others offer separately.

One competitor, **Eightfold.ai**, leverages deep learning to match candidates (including internal databases) but is more focused on matching known candidates to jobs (talent management) rather than broad passive search. PeopleGPT’s engine is more flexible for open-ended sourcing. Another emerging competitor is **Google’s Cloud Talent Solution** which offers an AI job search API (some job boards use it) – it uses ML to understand job titles and skills, but our PeopleGPT Engine is specialized for recruiting and includes outreach and profiling aspects those generic solutions lack.

Our engine’s integration of unique filters (e.g., company funding stage, promotion history, diversity indicators) sets us apart. Competitors like **Entelo** or **HiringSolved** have attempted predictive analytics (like predicting who is likely to change jobs), which we also address via our AI insights (e.g., “likely to switch” filters). The combination of features in one engine – natural language, real-time multi-source data, AI scoring, and autonomous agent support – is where PeopleGPT differentiates. According to user testimonials, this means finding passive candidates that others miss, especially _those who “aren’t desperately refreshing job boards but might be interested in the right opportunity”_.

In summary, the PeopleGPT Engine is a core competitive moat for the platform. We will continue to benchmark it against others by testing search quality (e.g. we can run identical searches on LinkedIn or SeekOut and see differences). Our goal is to maintain a lead in both **breadth (more data)** and **depth (better understanding)**. The engine should be a key reason why clients choose our product over others – offering a faster, smarter way to discover talent that translates into real hiring outcomes.

## 3. Real-Time Data Aggregation

### Overview and Purpose

Real-Time Data Aggregation is the platform’s capability to continuously gather and update candidate information from a wide range of sources in **real time (or near-real time)**. The system analyzes data from professional profiles, technical websites, academic publications, and more, integrating them into one comprehensive talent pool. The purpose of this feature is to ensure that recruiters are always searching the freshest, most complete data available – so they can find _“the best talent, no matter how specific the search”_ without worrying about stale or siloed information. By aggregating from dozens of sources (e.g., LinkedIn, GitHub, Stack Overflow, Google Scholar, company websites, etc.), the platform can surface candidates who might be missed on any single source. Real-time updates mean if a candidate acquires a new skill, changes jobs, or publishes a paper, the platform reflects that change promptly. This gives recruiters a competitive edge in identifying new prospects (e.g., someone newly open to work or newly qualified) and reduces manual research outside the platform. In short, Real-Time Data Aggregation underpins the platform’s promise of a **“global reach: 800 million profiles”** and up-to-date information at recruiters’ fingertips.

### Functional Requirements

- **Multi-Source Connectivity:** Establish and maintain integrations with a broad array of data sources, including but not limited to:

  - Professional social networks (e.g., LinkedIn, XING)
  - Technical communities (e.g., GitHub, Stack Overflow, GitLab, Kaggle)
  - Academic databases (Google Scholar, Semantic Scholar, arXiv for publications; university alumni directories if accessible)
  - Public web (personal websites, blogs, patent databases, conference attendee lists)
  - Proprietary databases or resume repositories (if any partnerships exist)
    The system should be able to pull profile data from these sources via APIs or scraping (where allowed), and ingest them into a unified candidate model.

- **Real-Time or Scheduled Fetching:** Implement data fetching with a frequency appropriate to each source’s nature and API limits. “Real-time” implies that the platform is as close to current as possible: for dynamic sources like LinkedIn, this might mean polling or webhook integration to get updates daily or in real-time streams. For more static sources (e.g., publication records), maybe update weekly or when triggered by a search. The architecture could include a message queue or streaming pipeline that continuously processes incoming data updates from various connectors.
- **Data Normalization & Merging:** When data from different sources pertains to the same person, the system must merge it into a single profile. This requires robust **entity resolution**: matching profiles by name, email, username, or other identifiers. For example, a “John Doe” on GitHub with the same email or personal URL as a John Doe on LinkedIn should be recognized as one person. The functional requirement is a high accuracy matching algorithm to avoid duplicate entries and to consolidate attributes (skills from GitHub repos, job history from LinkedIn, etc.).
- **Field Mapping and Enrichment:** Ensure that all relevant data fields from sources map to our internal schema. If LinkedIn provides “headline, summary, experience, education”, GitHub provides “repositories, languages, stars”, Scholar provides “papers, citations”, etc., we must capture those. In some cases, transform them into standardized metrics (e.g., calculate total years of experience from job history dates, or infer top skills from GitHub activity). “Aggregation” also means enriching profiles: e.g., adding a field that’s not explicitly given but derived (like an “Open Source Contributor” flag if a profile has a GitHub with >X stars).
- **Freshness and Expiry:** Define how to handle data freshness. If a profile hasn’t been updated from its source in a long time, consider marking it or re-checking. For example, if someone’s LinkedIn hasn’t changed in 2 years, maybe re-fetch occasionally but expect it static. Conversely, if someone is actively updating (lots of new GitHub activity), fetch more often. We should have a scheduler that prioritizes updates for recently active candidates or those that appear in search results. Also handle if data is removed at source (e.g., someone deletes their profile) – we may need to flag or remove that person from our index to avoid showing outdated info.
- **Scalability of Data Ingestion:** With 800M profiles and counting, the system must handle large-scale crawling and updating. Use distributed processing for web scraping or API calls (maybe serverless functions or a cluster of workers). Also respect each source’s API limits and robots.txt if scraping. Functional requirement: the system can ingest millions of updates per day across sources without data loss. For initial data load, it should be able to bulk import from existing datasets.
- **Quality Assurance & De-duplication:** Provide mechanisms to detect and resolve conflicting data or duplicates. For example, if two sources list different current companies for a person, we might use timestamps or trust certain sources more for specific fields. There should be business logic rules or AI to decide the authoritative data (e.g., if LinkedIn says John is now at Google as of 2025, but an older resume had him at Microsoft until 2024, trust the LinkedIn for current info). Duplicate handling is critical – ensure the unified profile doesn’t double-count entries or create separate nearly-identical profiles.
- **Coverage Metrics:** The system should maintain a high coverage of available data for each profile. That means for a given person, we try to have as many relevant fields populated as possible. If we have their name from one source but can get their contact from another, the aggregator should do so (subject to our contact info policy). The functional requirement is that the aggregator proactively seeks to fill gaps: e.g., if a profile lacks an email, see if another source provides it (more on contact info in Feature 10, but the aggregator is where that initially happens).
- **Compliance & Opt-Out:** We must ensure compliance with data privacy laws and source terms. The aggregator should support omitting certain data if required (e.g., if a candidate requests removal or if a source disallows storing certain info beyond search). There should be a way to mark profiles as “do not aggregate further” or to quickly reflect when someone’s profile is set to private on the source. GDPR and other regulations may require that if someone opts out, we have a mechanism to delete or anonymize their data. This is a functional requirement often implemented via a suppression list or flags on profiles (not to be shown/contacted if opted out).
- **Error Handling & Monitoring:** The aggregation processes should be monitored for failures (API errors, format changes in sources, etc.). If a connector fails (say LinkedIn changed their HTML structure), the system should alert engineering and ideally fail gracefully (not crash the pipeline, just skip until fixed). Monitoring dashboards showing data flow rates, last update times for each source, and counts of profiles updated per hour/day are important for ensuring the “real-time” promise is being met.

### User Stories

- _As a recruiter_, I want the platform to have **all relevant candidate data in one place and up to date**, so I don’t need to manually check multiple sites (LinkedIn, GitHub, etc.) to piece together a candidate’s background. For example, if a candidate just published a new research paper or changed jobs last week, I want that reflected when I view their profile on this platform.
- _As a tech recruiter_, I want to **discover candidates who are active on technical platforms** even if they aren’t active on LinkedIn. Real-time aggregation from GitHub and Stack Overflow means I can find a great developer who maybe doesn’t have a full LinkedIn profile. This helps me reach passive talent that others might miss.
- _As a user (recruiter)_, I want to **trust the accuracy of the profiles** I see. If the platform says someone is currently at Company X, it should be true (updated). I rely on real-time updates to avoid embarrassing situations like contacting someone about a job when they’ve already started a new job recently (which I should have known from their updated info).
- _As a hiring manager or client_, when a recruiter shares a candidate with me, I expect the information to be current and complete. The recruiter (via the platform) should be able to show me not just their resume, but also relevant activities (like open-source contributions or recent awards) because the platform aggregated all that. This helps in making informed decisions quickly.
- _As a member of the compliance team_, I need the platform to **honor data privacy and opt-out requests**. If a candidate’s data is pulled from the web and they request removal, the aggregator should stop updating and remove them from search. Also, I want confidence that we’re not violating any source’s usage policies by overloading or storing data longer than permitted.

### UX/UI Considerations

- **Profile Completeness Display:** On each candidate’s profile view, consider showing a “Profile completeness” indicator or a list of data sources. For instance, icons or labels like LinkedIn, GitHub, Scholar could appear, indicating we have data from these places. This not only validates the data (user sees multiple sources) but also signals how rich the profile is. We might show a section “External Profiles” with links – e.g., “GitHub: johnDoe42 (128 stars)” or “Publications: 3 papers, last in 2024.” This gives recruiters a quick way to verify or dive deeper if needed.
- **Last Updated Timestamp:** It could be useful to display _“Data last updated on \[date]”_ on a profile. For example, _“Profile data refreshed 3 days ago”_. This transparency helps users trust the data and know if something might be outdated. If a profile hasn’t been updated in a long time (maybe for a very passive candidate), the UI could subtly indicate it (maybe a warning or just the old date).
- **Real-Time Search Indicator:** When a search query is run that might require on-the-fly fetching (though ideally most data is indexed already), there could be an indicator like _“X new profiles found just now”_. However, since we try to keep data up to date in the index, the user mostly experiences it as always updated results. Perhaps more important, if a user repeats a search later, new candidates that appeared due to data updates could be marked as _“new”_ in the results. This might encourage recruiters to revisit searches because the database is always growing and changing.
- **Refreshing a Profile:** Provide a manual “refresh” button on a profile page for edge cases. If a recruiter knows a candidate updated something (or just wants to double-check), clicking refresh could trigger the system to fetch that candidate’s latest data on demand (respecting rate limits). This is a safety net in case the automated update hasn’t caught up. The UI should show a brief loading and then update any fields that changed, highlighting them possibly.
- **Search Filters by Source:** Some recruiters might want to filter candidates based on source presence (e.g., “has GitHub activity” or “has published papers”). We can expose filters or tags representing data sources. For example, a filter for “Profiles with GitHub” or a facet in Insights like “X% of this talent pool have GitHub profiles”. If implemented, the UI for search could have checkboxes for major data sources or an “Activity” filter (like Active on GitHub in past 6 months). This is a power-user feature leveraging our aggregated data.
- **Merging Transparency:** If we aggregate multiple sources, occasionally data might conflict (two job titles, etc.). The UI should handle this gracefully: ideally, resolve it to one truth as per backend logic, but if unsure, maybe list both with context. For example, if overlapping info: _“Current: Software Engineer at Google (per LinkedIn, updated Jan 2025)” and maybe another line if needed._ Generally, we prefer a clean unified view, but transparency can be provided via hover or drill-down like “view source details”. Perhaps each entry in work history or education could have a source attribution if clicked. This way, if a user ever wonders “where did this info come from?”, they can find out.
- **Speed and Responsiveness:** From a user perspective, even though heavy lifting is done server-side, the UI should not show any lag when loading profiles or search results because of data aggregation. That means by the time something is displayed to the user, all relevant data should already be aggregated. In design, ensure that the presence of lots of data doesn’t clutter or slow the UI (lazy load parts of profile if necessary, e.g., load publications list only when user expands that section).
- **New Data Alerts:** Since this is “real-time”, if something notable happens to a profile that a user saved, maybe the UI can notify them. For instance, if a candidate in their project got a promotion or changed their job, we might show a small alert icon next to that candidate in the project or send a notification: _“Candidate X updated their profile – new position: Senior Scientist at ABC Corp”_. This ties aggregation to a proactive feature, which can impress users (the platform keeps them updated on talent moves). But this can be part of a later enhancement (not core to initial PRD, but worth considering for UX delight).

### Acceptance Criteria

- **AC1: Data Coverage per Source:** The system successfully aggregates data from all promised sources. For acceptance, we define a benchmark like: _From LinkedIn, at least 95% of target profiles have been retrieved and are searchable; from GitHub, at least 90% of profiles with a certain activity threshold are included; from publications, profiles with known authorship are linked at least 85% of the time_, etc. Essentially, for each data source integration, verify that it’s pulling in a large volume of data and integrating correctly. This can be tested by sampling known entities: e.g., pick 100 known GitHub top contributors and see that they exist in the system with their GitHub info. If even 5 of those are missing entirely, that’s a failure in coverage for GitHub data.
- **AC2: Update Freshness SLA:** The system meets a freshness criterion such as _90% of profile data is updated within X days of a change in source._ For example, if a person changes their job on LinkedIn today, our platform should reflect that change in search results and profile view within 48 hours (or whatever threshold we decide is our SLA). Acceptance can be tested with monitored accounts or dummy profiles where we make changes and see if our system catches them in time. Also, the _last updated timestamp_ on profiles should generally be recent (within the last few weeks) for active profiles. If we find many profiles that haven’t been refreshed in months despite them being active on sources, that fails the criterion.
- **AC3: Low Duplication Rate:** The aggregation system should result in unified profiles such that duplicates in search results are extremely rare. Acceptance: in a random sample of search results (hundreds of profiles), there should be no obvious duplicates (two entries that are actually the same person). If duplicates are found, our matching needs improvement. A numeric target might be _duplicate rate < 0.1%_ of profiles. We can also measure profile count vs. source count – e.g., if summing across sources we’d have 900M records but unified we have 800M profiles, that indicates how well we merged. The acceptance is qualitative too: recruiters shouldn’t complain about seeing the same person twice under different names.
- **AC4: Data Accuracy and Consistency:** Profiles should not contain glaring inconsistencies due to merging. Acceptance test: For a sample of merged profiles, ensure that fields like “current company” or “location” make sense and don’t contradict each other (e.g., not listing two current jobs unless actually known to have two). If a profile says “Current: Google” and “Current: Amazon” by mistake, that fails. Also ensure skills and titles are consistent (like no duplicate entries in skills list, etc.). We can run validations on the aggregated data (like each profile has at most one current job marked; education entries are unique; etc.).
- **AC5: Performance of Search with Aggregated Data:** Confirm that adding more data sources does not degrade search performance beyond acceptable bounds. For acceptance, run search queries that involve filters from different sources (like “hasGitHub\:true AND language\:Python AND location\:USA”) and see the system responds quickly. If incorporating all this data slowed search or profile loading significantly, it’s an issue. We aim for search queries still returning in seconds and profile pages loading nearly instantly (with lazy load for heavy sections).
- **AC6: Privacy/Compliance Mechanisms:** Demonstrate that we can suppress data or remove profiles as needed. For example, if given a test profile that is opted-out, check that it does not appear in search or that contact info is withheld if needed. Similarly, show that if a user requests deletion, we can purge their aggregated data. If our system inadvertently retains disallowed data or fails to heed an opt-out flag, it fails compliance acceptance. We should have audit logs or flags that confirm compliance actions.
- **AC7: Successful Integration of New Data Types:** If we consider “recent career milestones” or “likely to switch” flags as part of aggregated data, ensure those are calculated correctly (though those are features on their own, they rely on aggregated data). For instance, if we label someone as “Recently promoted” or “Likely to switch”, is that based on actual data (promotion in their history)? This acceptance can tie in with Feature 15’s tests but belongs partly here: the aggregator must supply correct signals (like last promotion date, tenure length) for those features to use.

### Technical Notes

- **Source Connectors:** Each data source requires a connector or spider. For LinkedIn, which lacks an open API for full search, many platforms use proxies or partnerships. Possibly we use a third-party data provider or build a scraping solution (which must be done carefully to avoid blocks). For GitHub, use their API (with authenticated requests to increase rate limits). Stack Overflow has data dumps or Stack Exchange API. Publications could use an API like Semantic Scholar’s (which allows querying by author name) or crossref for papers. We might also crawl Google Scholar results (again careful of Google’s terms). Use Python or Node scripts for scraping, schedule them distributed. **Caution**: scraping large networks (LinkedIn) can lead to IP bans; often companies purchase datasets or use partners. We should consider data partnerships to legally obtain large datasets in a sustainable way.
- **Message Queue & Data Pipeline:** Use a system like **Apache Kafka or AWS Kinesis** to handle streams of data updates. Each connector can push new/updated profile info into a queue. Downstream, we have consumer jobs that merge it into the database and trigger search index updates. Tools like **Apache NiFi or Airbyte** might help orchestrate various connectors. The pipeline should be resilient – e.g., if one portion is slow, queue backlog builds but doesn’t crash the whole system. Also, use logging to track progress (e.g., “5000 LinkedIn profiles updated today”).
- **Database & Index:** Store aggregated profiles in a NoSQL or graph database for flexibility (since each profile has varying fields). For example, **MongoDB or PostgreSQL JSONB** for profile records, or a graph DB like Neo4j if we leverage relationships (people to companies, etc.). However, search will likely be handled by Elasticsearch or similar, which might require feeding the updated profile data into it. We might maintain separate indices for different sections (like one for main profile info, one for publications, etc.) and join results as needed, but simpler is one combined index document per person with all relevant text.
- **Entity Resolution:** Develop or use a library for matching profiles across sources. This could involve deterministic matches (email is a perfect key if available; LinkedIn profile URL is unique; GitHub username sometimes tied via email) and fuzzy matching (name + company + education matching between LinkedIn and a resume, etc.). Possibly incorporate ML for entity resolution if needed (there are known algorithms and libraries for record linkage). We should record a common unique ID internally for each person, and each source record either maps to an existing ID or generates a new one with links for potential merging. Over time, refine matching rules as we see false positives/negatives.
- **Data Cleaning:** Each source data needs cleaning/standardization. Titles, company names, etc., might come in different forms – we might want to normalize them (e.g., "Sr. Engineer" vs "Senior Engineer"). Similarly, remove or merge duplicate skills (someone listing "Microsoft Excel" and "MS Excel"). Our aggregation layer should include a cleaning module for each field type. This improves search quality later.
- **Incremental vs Full Sync:** We should design connectors to handle both initial bulk loads and incremental updates. For initial launch, we might ingest large dumps or sequentially crawl until we have a base dataset. After that, rely on incremental (like using timestamps or change tracking where available). Some sources provide changelogs or webhooks (e.g., perhaps via some partner API). For others, we might have to periodically re-crawl. Use strategies to spread out load (like don't fetch every profile daily – maybe prioritize popular ones or those likely to change). Possibly integrate an update scheduler that gives higher frequency to recently active profiles.
- **Contact Info Integration:** Aggregation includes contact info (Feature 10 covers details, but technically, some sources like GitHub might have an email in commits, or we use Hunter API for emails). The aggregator should capture these when available but segregate personal data handling properly (maybe store contacts in a secure way or separate table due to sensitivity). Also handle phone numbers, etc., if coming from any source. Ensure not to expose them unless user has permission (like correct plan tier, since pricing plans might gate phone numbers).
- **Data Source Failures:** Plan for sources possibly shutting down or changing. E.g., if an API we rely on becomes paid or deprecated, how quickly can we adapt? Ideally have multiple pipelines such that losing one source (like if LinkedIn blocks something) doesn’t break everything – we still have others and an existing dataset to use until fixed.
- **Monitoring and Analytics:** Implement dashboards showing how many profiles from each source, how many new per day, update latency, error rates per connector. This is crucial to claim “real-time” – we need to measure and ensure our pipeline is flowing. Also, monitor search usage patterns to maybe inform which data matters most (like if recruiters often filter by GitHub stuff, ensure that’s robust).
- **Security:** Aggregating data at this scale means we have a treasure trove of personal information. We must secure the data storage (encryption at rest, access controls), and ensure that only authorized users can query it. Also protect our pipeline credentials (API keys for sources, etc.). Possibly implement rate limiting on our end to avoid any data leaks or misuse (like a user tries to export the entire database via search, we might detect and throttle).

### Success Metrics

- **Database Growth and Freshness Metrics:** We can track metrics like _profiles aggregated per week_ and _profile updates per week_. A positive trend indicates healthy data ingestion. For example, if we see 100k new profiles being added weekly from various sources, that’s growth. For freshness, measure _average last updated time_ for profiles in the database – we’d want, say, 80% of profiles updated in the last 12 months, 50% in last 3 months, etc., showing our data isn’t stale. These numbers are something we can benchmark over time and improve as we onboard more sources or increase crawling frequency.
- **Search Result Diversity:** Because of multi-source aggregation, a successful metric is that our search results include candidates that **would not be found on LinkedIn alone**. We can measure what fraction of candidates in typical searches have, for instance, no LinkedIn but have GitHub or other presence. If recruiters report that they “found someone on PeopleGPT who they couldn’t find on LinkedIn Recruiter”, that’s a big win. We could attempt to measure: e.g., for each search query, how many unique results come from non-LinkedIn data? Or how many profiles have a GitHub link vs. not? Ideally a significant portion (especially for technical roles) are surfaced via these extra sources.
- **User Trust Indicators:** If our data is up-to-date, users will trust and rely on the platform more. We can gauge this by qualitative feedback or NPS scores related to data quality. Also, if we provide a feedback option on profile accuracy (like “report outdated info”), track how often that is used – hopefully very rarely if real-time updates are effective. A reduction in such reports or support tickets about incorrect data over time is a metric of success.
- **Engagement due to Fresh Content:** New or updated data can drive user engagement. For instance, if we implement notifications of profile changes, we can measure click-through or usage of that feature. Or measure if users are revisiting saved searches expecting new results (if so, it indicates they trust the database is dynamic). If we see increasing usage of “alerts” or repeated search queries, it might correlate with our data updates feeding new results in.
- **Reduced Outside Research:** Ideally, recruiters using our platform should need to do less outside Googling or checking other sites. A metric could be usage of our external profile links: if people click out to LinkedIn/GitHub a lot, maybe they feel they need to verify info (or just to get more detail). If our data is complete, they might not need to. A decrease in clicks on external profile links or a survey question “Did you find you had to leave the platform to get info?” can indicate success in aggregation quality. We’d aim for most users saying they got all they needed within our profile view.
- **Competitive Edge:** If we can claim stats like _“We update our database daily with millions of changes”_ or _“We cover 2x more data sources than competitor X”_, these become marketing points. A success metric is if prospective customers cite our comprehensive, up-to-date data as a reason for choosing us. We might collect win/loss reasons for sales – seeing “data coverage” as a frequent win reason validates this feature’s impact.

### Competitive Benchmarking

In the recruiting tech space, data is king. **LinkedIn** has a vast network but is largely user-maintained and not truly real-time (people often don’t update immediately). Our real-time aggregation aims to surpass that by pulling from everywhere. **SeekOut** and **hireEZ** similarly aggregate data from GitHub, papers, etc. In fact, SeekOut markets itself on having GitHub and patent data to find specialized talent. We need to ensure parity or superiority there: e.g., if SeekOut has 500M profiles and we claim 800M+, we should substantiate that with diverse sources. Our advantage could be the speed of updates – if a developer’s GitHub shows activity this week, we reflect it, whereas some might not. Also the breadth: for example, including academic achievements (publications, etc.) gives an edge when recruiting researchers, which not all platforms have.

**Entelo** historically aggregated social data and tried “Entelo Sonar” (predicting who might leave jobs), but they might not be as broad in tech communities. **TalentNeuron** or **HiringSolved** have had data aggregator models too. We should benchmark how frequently they refresh data – often, smaller players rely on batch updates which can be months old. By positioning ours as near-real-time, we cater to recruiters who need the most current info (like sourcers who want to know if a candidate just became available).

**Google Talent Solution** (for jobs) doesn’t provide profiles, so not comparable directly, but if any company has similar scale data it’s likely LinkedIn. However, LinkedIn generally doesn’t include, say, a person’s GitHub contributions. They have “Talent Insights” product for market data but not individual profiles beyond LinkedIn.

One risk: heavy reliance on scraping LinkedIn can be precarious, but if we manage it or have a third-party data vendor (some companies gather public LinkedIn data), we can mitigate that. Competitors often skirt around this by focusing on other sources or using cached data.

By having real-time multi-source data, PeopleGPT can position itself as a **“talent intelligence platform”** rather than just a LinkedIn alternative. In one of our references, it was noted that PeopleGPT provides _access to over 800M profiles, making LinkedIn's network look like a high school reunion party_. This bold claim highlights our competitive stance: more data, and smarter use of it.

Our goal is that users feel they have a **complete view of the talent market** in one place. If a competitor only shows LinkedIn info, a recruiter might still have to google the person; with us, they get everything consolidated (leading to faster decisions). In competitive evaluations, we should highlight any exclusive or first-party data we aggregate (like personal websites, or integration with ATS internal data as well – though internal data is Feature 20). Ultimately, success in this area supports all other features: Natural Language Search yields better results because of more data, Autopilot can analyze profiles deeply because of rich data, and so on.

We will continuously compare our profile coverage and freshness against competitors by running sample searches on both. For example, search for “Kubernetes developer with publications” on SeekOut vs PeopleGPT – see who finds more or newer people. The aim is to consistently outshine others in relevance and extent of candidate discovery, thanks to Real-Time Data Aggregation.

## 4. AI-Powered Profile Ranking

### Overview and Purpose

AI-Powered Profile Ranking is a feature that automatically evaluates and ranks candidate profiles according to how well they match the specified criteria of a search or job requirement. Using **Generative AI and machine learning**, the system (often referred to as **Autopilot**) will _“research, review, and assess each search result individually”_ and then output a **ranked list of profiles** from best fit to least fit. The purpose of this feature is to save recruiters significant time in screening and prioritizing candidates. Instead of manually reading through dozens or hundreds of resumes/profiles, the AI can highlight the top matches and provide a rationale, enabling recruiters to focus on the most promising talent first. This ranking is based on customizable criteria – for example, required skills, years of experience, education, etc., as defined by the user or derived from a job description. In short, AI-powered ranking turns a raw search result into an **“AI-powered spreadsheet”** where profiles are scored and sorted for quick decision-making. The ultimate goal is to increase the quality of shortlist and accelerate the sourcing process by ensuring the best candidates are reviewed first.

### Functional Requirements

- **Criteria Definition:** Allow users to define the criteria against which profiles should be evaluated. This could be explicit – the user picks or enters criteria (e.g., “5+ years Java, Finance industry experience, MBA preferred”) – or implicit from the search query or job description. The system should also suggest criteria automatically (for example, based on the job description uploaded or the filters set in search). There should be a UI to edit and prioritize these criteria (e.g., mark some as must-have vs nice-to-have, reorder importance).
- **AI Evaluation of Profiles:** For each profile in the search results, the system uses AI (likely an LLM or specialized model) to evaluate how the profile meets each criterion. This involves parsing the profile’s content (experience, skills, etc.) and possibly inferring beyond explicit keywords. For example, if a criterion is “experience building search infrastructure”, the AI might check the profile for relevant projects or roles and mark if it’s present (explicitly or via related terms). The output is a score or rating per criterion (like Good, Maybe, or Not Present) and possibly an overall match score.
- **Ranking Algorithm:** Combine the evaluations into an overall match score for each profile. The system should weight criteria according to their importance (as set by user or default weights). It could be a simple weighted scoring or a more complex ML model that predicts fit. Ensure that those with all must-haves and many nice-to-haves score highest. The ranking should be dynamic – if criteria or their weights change, the ranking updates in real time.
- **Result Presentation:** Present the ranked profiles in a structured format, such as a table with columns for each criterion and overall score. Profiles should be sorted from highest scoring to lowest. Visually highlight top matches (maybe an icon or color for “Good Match”). Provide an aggregate view: e.g., out of 10 criteria, Candidate A meets 9 (with 7 strong, 2 moderate) vs Candidate B meets 6, etc. The system might label candidates as “Good match”, “Potential fit”, or “Low fit” with icons (like thumbs up, etc.).
- **Transparency of Scoring:** Provide details for why a profile got the score it did. For each criterion, indicate the evidence or lack thereof. For example: _Criteria: Python experience – Good Match (mentioned 5 years experience at XYZ)_; _Criteria: Team leadership – Potential (led a project team, but not formal manager)_. This transparency is important so users trust the AI ranking and can quickly assess correctness.
- **Adjustable Criteria and Re-ranking:** Users must be able to modify the criteria or their priority and re-run the ranking easily. If a recruiter realizes that a certain skill should be mandatory (pinned), they can adjust that and the system will re-rank with that criterion now as a must-have (which might push down profiles lacking it). The system should re-calc quickly (within seconds for, say, dozens of profiles). Possibly integrate with the search filters UI – e.g., if user decides to exclude profiles missing something, the ranking updates or those drop off.
- **Handling Large Result Sets:** For efficiency, possibly limit the profile ranking evaluation to a reasonable number of candidates (maybe the top 50 or 100 from the initial search). Alternatively, allow user to specify how many to evaluate or to evaluate in batches (“Evaluate next 50”). This ensures we don’t waste resources on very low relevance profiles. Perhaps by default, we rank the first page of results; if user scrolls further, we can evaluate more on the fly. The system should indicate if not all results were ranked (e.g., “50 candidates evaluated by AI”). Optionally, allow user to request full evaluation (costing more credits or time).
- **Integration with Workflow:** The ranking results should tie into actions like shortlisting or outreach. For instance, it would be useful to have a button to “Select all Good matches” to add to a project or sequence. Acceptance criteria (below) will cover this integration aspect. But functionally, the ranking view isn’t isolated – users should be able to take next steps on those profiles directly from it (like check a box next to high scorers to group them).
- **Learning and Feedback Loop:** Over time, the ranking system should improve by learning what candidates the recruiters actually proceed with. If consistently a user ignores the top suggestions and contacts lower-ranked ones, the system might adjust (maybe our criteria weighting or the way we infer fit). This can be future/continuous improvement, but the architecture should allow capturing outcomes (like which ranked profiles were shortlisted or replied) as feedback signals to refine the AI model. Initially, though, we rely on static logic and user-defined weights.
- **Criteria Library:** Provide some pre-set common criteria or templates. For example, for engineering roles, criteria might include coding skills, system design experience, relevant industry, etc. Autopilot might even propose 4 suggested criteria by default. This helps users who might not know what to input. They can always edit, but the system giving a starting point (from a known role taxonomy or from analyzing the job description) is key.
- **Must-have Filtering:** If a criterion is marked mandatory, the ranking should treat any profile without it as essentially disqualified (could either rank them at bottom with extremely low score or filter them out entirely). Perhaps have a toggle “hide profiles that don’t meet all must-haves” for convenience. This ensures recruiters aren’t distracted by non-starters.

### User Stories

- _As a recruiter_, I want the system to **automatically rank a list of 50 candidates by how well they match my open role’s requirements**, so that I can focus my time on reviewing the top 5-10 best fits first rather than scanning every profile manually.
- _As a sourcer_, I want to **know why certain candidates are considered better matches** than others via the AI ranking, so that I can confidently justify to hiring managers that these top candidates meet the criteria (and conversely understand why others are lower). For example, the tool might show that the #1 candidate has all required skills plus relevant domain experience, whereas #10 lacks a required certification, explaining their position.
- _As a hiring manager_, I want the recruiter to show me an **AI-curated shortlist** complete with scores or grades for each candidate based on our agreed criteria. This way, I have a structured view of how each candidate stacks up and it aligns with the role’s priorities. It saves me from reading every resume in full – I can directly jump into the top few with confidence.
- _As a recruiter_, if I have a large set of search results, I want to **quickly eliminate or de-prioritize those who don’t meet certain must-haves** (e.g., a security clearance or a specific degree) using the AI, rather than applying filter after filter. The AI’s screening should make obvious who to skip (e.g., mark those clearly as “Does not meet X” at the bottom of the list).
- _As a new user_, I may not perfectly articulate what to look for besides the basics. I want the system to **suggest key criteria and weightings for ranking** when I run Autopilot on a search. For example, for a “Data Scientist” role it might automatically consider criteria like Machine Learning experience, Programming skills, Education (MS/PhD), Publications, etc. This helps ensure the ranking aligns with typical expectations even if I didn’t specify all details.

### UX/UI Considerations

- **Autopilot Activation:** In the search results UI, provide a prominent option to “Evaluate and Rank with AI” (Autopilot). It could be a button or toggle that, when pressed, brings up the criteria configuration modal or directly starts the ranking using default criteria. The label should be clear (like “AI Screen Candidates” or “Run Autopilot”).
- **Criteria Configuration Modal:** If the user triggers ranking, a modal or side panel can appear listing the criteria to be used. It might pre-populate with 3-5 criteria gleaned from the search context or job description (with an explanation like _“Suggested criteria”_). Each criterion can have a short description (e.g., “Has team leadership experience”) and possibly an importance slider or rank (like high/medium/low priority). Users should be able to add new criteria (free text, which the AI will interpret) or remove ones. Keep it user-friendly: even just listing key skills or qualifications, the system can parse them behind the scenes as instructions to the AI.
- **Ranking Results View:** After running, the interface likely switches to a **table view**. Consider a layout where rows are candidates and columns are criteria plus an overall score. The first column might be Candidate Name (and maybe current title), second could be overall match score or rating (like 92% or “Excellent”), then columns for each criterion with icons or short tags (e.g., a green check or thumbs-up for good, yellow tilde for partial, red X for no). You might also have a column for “Status/Action” if the user can take actions (e.g., shortlist or email directly).
- **Visual Highlights:** Use icons/color coding for criterion fulfillment: for example, a green circle or ✓ for “Good match”, an amber symbol or \~ for “Potential fit” (partial), and a red × or – for “Not met”. These should have tooltips explaining the evaluation, e.g., “Good match: explicitly mentioned 10 years in this skill”. The overall score might be presented as a large number or a badge (like 5-star rating or a letter grade A/B/C). This helps in quickly scanning down the list.
- **Sorting and Filtering in Table:** By default, it’s sorted by overall score descending. The user could also sort by a specific criterion column if they want to see, say, who has that skill vs not. They might filter out those who failed a must-have (maybe automatically done). The table should support these typical interactions (maybe using a familiar spreadsheet-like UI with filters on headers).
- **Profile Quick View:** Each candidate row should allow the user to quickly see more details without leaving the context. Perhaps clicking the candidate name opens a sidebar or tooltip with their profile summary (so the recruiter can sanity-check details if needed). Or a “Expand” arrow to show a brief summary and highlighted matching keywords under the row. This way, the recruiter doesn’t have to navigate away to verify something small.
- **Actions (Shortlist, etc.):** Provide checkboxes on each row or a multi-select mechanism, so the user can select multiple top candidates and perform an action (like “Add to Project” or “Send Email Sequence”). Also possibly a “Select All Good Matches” if we classify some threshold as good. Each row could also have direct action icons (like a star to shortlist, an email icon to email).
- **Save/Export Ranking Results:** The recruiter might want to share the ranking or come back to it. There should be an option to save this evaluation (maybe it’s tied to the search and stored as part of a project). Also an export to CSV or PDF for sharing with hiring team could be beneficial (with the caveat that outside the platform you lose interactive elements). If we do PDF, we’d likely incorporate it in “Smart Report” (Feature 18 collaboration, or something similar to share with hiring manager). But at minimum, ensure the ranking can be retrieved again later without rerunning, perhaps by saving the search with the criteria.
- **Responsiveness:** The ranking view might have many columns (if many criteria). Ensure the UI handles smaller screens or many columns gracefully – e.g., horizontal scroll for the table, or grouping less important criteria into a single “Other criteria” column if needed. Perhaps limit to top 5 criteria visible by default to maintain readability (with an option to view more detail on each profile).
- **Cancel/Adjust Flow:** Ranking might take a few seconds if evaluating many profiles. The UI should show a progress (maybe a loading bar “Analyzing profiles…”). Optionally, allow the user to cancel the process if needed. After completion, if the user wants to adjust criteria, they should be able to do so via a “Adjust criteria and re-run” button to tweak and instantly see updated ranks. This encourages experimenting (e.g., “what if I consider education more heavily?”).
- **Introduction/Tutorial:** Because not all recruiters are used to AI doing this, consider a brief intro or guided tour the first time they use it. For example, a pop-up that says _“Autopilot has scored your candidates. Higher scores mean a better fit based on the criteria. Click on any score to see details.”_ This helps adoption and trust. Possibly include disclaimers or best practices like _“Review the AI’s suggestions and use your judgement; it’s here to assist, not replace human decision.”_ (to set expectations).

### Acceptance Criteria

- **AC1: Correct Identification of Must-Haves:** If a criterion is designated as must-have, the ranking algorithm **never ranks a candidate lacking that criterion above any candidate who has it**. Ideally, candidates without a must-have are filtered out or given clearly the lowest tier. For acceptance, we will test scenarios: e.g., must-have “Master’s degree” – ensure all profiles without a Master’s are marked and placed at bottom (or removed). If any profile lacking a must-have still appears among top matches, that’s a fail.
- **AC2: Consistent Scoring with Criteria:** The profiles’ match scores should reflect the criteria weighting. For acceptance, create a controlled test: suppose 3 criteria of equal weight – profile A meets all 3, profile B meets 2, profile C meets 1. The system should rank A highest, B next, C last with appropriate score differentiation. If weightings are changed (e.g., one criterion weight doubled), the ranking should adjust accordingly (we test that by manually calculating expected scores and comparing to system output).
- **AC3: Accuracy of Criterion Evaluation:** The AI’s evaluation of each criterion on a profile should be at least **X% accurate** (target maybe 90%+ for clear-cut qualifications, slightly lower if criterion is abstract). We can benchmark this by taking a sample of profiles, setting criteria, and manually checking if the AI classification (Good/Potential/No) matches a human recruiter’s judgement reading the profile. For example, criterion “5+ years in Python”: if the profile has 6 years Python exp, AI should mark Good. If it wrongly marks something, that’s an error. The acceptance threshold could be e.g., fewer than 1 in 10 misclassifications of major criteria in test data. Particularly important, no false positives on must-haves (e.g., labeling someone as having a skill when they don’t).
- **AC4: Time Performance:** Running Autopilot ranking on, say, 50 profiles should complete in a reasonable time (e.g., under 60 seconds, with many completing in \~10-30s). We can set an acceptance that _ranking 50 profiles takes at most 1 minute on average_, and scales linearly (100 profiles maybe \~2 minutes, etc.). If it’s cloud-based, we can parallelize to keep within this range. The acceptance is user-facing: the UI should not hang too long. If it’s taking multiple minutes regularly, that’s not acceptable – it breaks the flow. So measure on various typical loads.
- **AC5: User Adjustments:** After running, if the user changes a criterion or weight and re-runs, the new ranking should update accordingly and logically. This is more a functional consistency check: e.g., if they remove a criterion, profiles that were penalized for that should move up. We test a scenario where a certain profile was low due to one missing criterion; remove that criterion -> profile should now rank higher. If these relative changes don’t happen, the dynamic re-ranking might be flawed. So acceptance: interactive re-ranking yields results consistent with the new criteria without needing a full system restart (i.e., the UI updates with new sorted order correctly).
- **AC6: Integration with Shortlisting/Export:** After ranking, selected candidates can be acted on without error. We test taking the top 5, adding to a Project or triggering an email sequence. Acceptance is that these actions work as expected (profiles get added, emails queued, etc.) and that the system tracks that (like their status changes to “Shortlisted” if we have status feature). Essentially, Autopilot shouldn’t be a dead-end; it flows into normal workflow. If any friction (like can’t select multiple or action fails), that needs fixing.
- **AC7: No Adverse Bias Introduction:** The AI ranking should focus only on the professional criteria provided and not inadvertently bias on unrelated factors (e.g., gender, ethnicity if any hints in profile, etc.). For acceptance, we can run some sensitivity tests or audit the criteria handling. If we explicitly see the AI using something like name or age unless it’s part of criteria (and such shouldn’t be criteria due to fair hiring practices), that’s unacceptable. We will ensure in design that criteria offered are job-related. This acceptance criterion is more ethical: verify that the ranking system doesn’t systematically disadvantage a group. For initial tests, ensure the model’s prompts avoid bias and perhaps include fairness checks (though full bias audit might be beyond initial scope, it’s a requirement to consider).
- **AC8: Recruiter Satisfaction:** In beta testing, recruiters should find the ranking useful and in line with their expectations. An acceptance measure could be _X% of beta users agree that the AI ranking’s top suggestions were relevant and saved them time._ For example, >80% say the ranked list matched or improved their own manual shortlist. This is qualitative but important for adoption. If many testers find the ranking off or not helpful, that’s a failure to adjust before full release.

### Technical Notes

- **AI Model for Evaluation:** The heart of this feature is an AI model (likely leveraging a large language model like GPT-4 or a fine-tuned variant) that can evaluate profile text against criteria. We might implement each criterion check as a prompt, or batch multiple criteria in one prompt per profile. For instance, we could feed the model: “Profile: \[text]. Criteria: 1) X, 2) Y, 3) Z. For each, does the profile strongly meet, partially meet, or not meet it? Respond with a score for each.” The model’s output would then be parsed into the Good/Potential/No for each criterion. Alternatively, for efficiency, a fine-tuned smaller model or even rule-based NLP could handle straightforward criteria (like scanning for keywords for skills). A hybrid approach is possible: use regex/semantic search for simple presence of skills, and use LLM for more nuanced ones (like “demonstrated leadership”).
- **Data needed:** The profile data needs to be available in a textual form for the AI. This means by the time we call the model, we should have a consolidated text of the candidate’s profile (work experiences, skills, etc.). We might use the already indexed data. Possibly store a pre-concatenated profile summary or generate one on the fly (ensuring it fits token limits). If a profile is extremely long, we may have to truncate or summarize parts for the AI evaluation. The docs mention “PeopleGPT uses all 30+ data sources when assessing each profile” – technically, that means the AI should consider any relevant info, but we need to ensure important pieces aren’t lost due to token limit. Perhaps focus on job titles, companies, duration, education, key skills, and omit fluff.
- **Parallelization:** To rank 50 profiles quickly, we should parallelize the evaluation calls. If using an API like OpenAI, that means sending multiple requests concurrently (within rate limits) or using batch endpoints if available. Alternatively, use a local model or fine-tuned model that can do batch inference. Infrastructure might include a queue where profiles to evaluate are processed by a pool of worker threads or functions (maybe serverless for concurrency). We must handle API errors, timeouts gracefully (maybe retry or skip if one profile fails rather than failing the whole ranking).
- **Scoring Mechanism:** Once each criterion is evaluated (Good/Potential/No or a numeric estimate), we need to combine them. We can assign numeric scores: e.g., Good = 1, Potential = 0.5, No = 0. Then do weighted sum (weights normalized to 1). Or possibly a more complex formula (maybe squared weights to emphasize must-haves). But something transparent is good. If we allow weighting explicitly, just multiply those in. The overall score could be expressed as a percentage of total possible points or just keep it relative. It might also be useful to enforce that missing a must-have yields a zero score overall, to ensure ordering as desired.
- **Storage of Results:** The results of an Autopilot run (scores etc.) might not need to be stored persistently except if user saves them. We can compute on the fly each time. But to allow sharing or later reference, we might save it associated with the search or project. Maybe in the database, store for each profile: {search_id, criteria_set_id, score, breakdown}. If criteria changes, that’s a new record. This would allow retrieving a previously run evaluation quickly without recomputation. But careful: if profile data updated significantly since last eval, old scores may be outdated. Possibly indicate if data changed. For now, perhaps rely on re-run rather than storing too long.
- **Integration with Projects:** We might consider that every Autopilot run could optionally create a “Smart Shortlist” under a project. There’s mention of “Shortlist Profiles” and “Smart Report” in docs. So, technical note: once top X are identified, if user chooses, we could auto-add them to a project or compile a PDF report. Implementation of that is straightforward (just push those profile IDs to the shortlist with a tag like “Autopilot selected”).
- **Logging & Credits:** Since calling AI for many profiles might consume API credits/cost, we likely need to count it against the user’s usage (some plans might limit how many AI evaluations they can do). Ensure to log how many profiles were evaluated by Autopilot and maybe deduct credits accordingly if that’s a monetization aspect (e.g., X credits per profile or per batch). Also log for internal monitoring the average cost/time per run.
- **Edge Cases:** If profiles have very sparse data, the AI might be unsure. We should handle that (maybe mark as insufficient info = low score). If criteria are poorly phrased or too broad, AI might struggle; we should encourage clear criteria. Perhaps incorporate some NLP to refine user-entered criteria to more model-friendly prompts (like if user writes “Good communication”, perhaps prompt as “evidence of communication skills such as presentations, collaboration” etc.). Possibly restrict criteria to professional qualifications to avoid confusion.
- **Updating Criteria Knowledge:** The AI’s evaluation ability depends on it understanding, say, domain context (e.g., that “AWS” implies cloud). We should ensure the model or logic accounts for synonyms and related concepts. If the user’s criterion says “Experience with distributed systems”, the candidate might not say exactly those words but mention Hadoop or Kafka – the AI should still count that as meeting the criterion. That’s where an LLM is useful as it can infer. If not using LLM, we’d need a synonyms library. For initial approach, leaning on LLM to interpret covers a lot of ground.
- **Explainability Implementation:** To get those nice explanation bullets or at least the rationale, we might incorporate it in the LLM output: e.g., have it not only categorize each criterion match but also quote the evidence from the profile or give a short reason. Then we can display that on hover. Alternatively, run a separate step to generate explanation for each “Good” mark (like find the actual snippet in profile that triggered it). Could use keyword search for that snippet. For now, even a generic reason like “(Found 5 years in profile)” is okay, but more exact reference builds trust.
- **Test & Train Data:** Possibly use our existing database of candidates and job descriptions to test this. If we have historical placements or matches, we could train a machine learning model to predict match scores. That’s complex but down the line. For initial, rules + LLM should suffice. But we should gather data from usage to evaluate how well AI ranking correlates with eventual outcomes (like interviews). If patterns emerge (maybe our scoring undervalues something consistently), we refine.

### Success Metrics

- **Time Saved in Screening:** Quantify the reduction in time recruiters spend on initial screening. For instance, if previously a recruiter might manually review 30 profiles taking, say, 2-3 minutes each (\~60-90 minutes), and with Autopilot they can identify the top 5 in 10 minutes, that’s a huge gain. We can survey users or instrument usage: measure average number of profile clicks/views per search before vs after Autopilot. If fewer profiles need manual checking for the same outcome, that indicates success. Perhaps aim for something like _50% reduction in profiles viewed per search that leads to a successful shortlist_.
- **Quality of Shortlist (Hit Rate):** Track how many of the AI’s top-ranked candidates proceed in the hiring process. For example, if from the top 5 suggested, typically 3 are selected for interview by hiring managers, that’s a good hit rate. We can compare with baseline (maybe historically, only 1-2 out of a random 5 would make it). If using our own pilot, measure: _X% of candidates labeled “Good Match” by AI get positive recruiter or HM feedback._ A higher percentage than random selection shows the ranking is effective.
- **User Adoption of Feature:** Monitor how often recruiters engage the AI ranking. Success would be a high adoption rate after introduction, e.g., _70% of searches or projects use Autopilot ranking at least once_. Also consider how often they refine criteria and rerun – usage patterns showing iterative use indicates trust and reliance. If adoption is low, maybe they don’t trust it or find it cumbersome, which would need addressing.
- **User Satisfaction:** Through surveys or interviews, gather recruiters’ opinions: e.g., _“The AI ranking accurately identified the top candidates for my role”_ – aim for strong agreement. Also measure NPS or specific satisfaction with “AI assistance” in the product. Ideally, users feel it’s a differentiator that makes their work easier. Comments about saving time or uncovering a good candidate they might have overlooked are good indicators.
- **Decrease in Manual Filtering Revisions:** If ranking is good, recruiters might rely less on adding endless filters. Possibly measure that once Autopilot is used, users do fewer follow-on filter adjustments. Or measure the length of search sessions: if before they spent an hour tweaking, now maybe they run search, run Autopilot, done. If we see faster conversion from search to shortlist stage, that’s a metric (time between search creation and candidates moved to project or contacted).
- **Performance of Ranking Model:** Internally track the precision of the model’s predictions – e.g., compare AI’s ranking to an “ideal ranking” if we have one from human judgement in tests. If we continue to refine, we want that to improve. Also track any obvious mistakes to fix (like if multiple users override a certain criterion result, note it). Possibly gather telemetry like how often users disagree (e.g., they mark a candidate as unsuitable despite AI high score – if that’s frequent for a particular pattern, we adjust models).
- **Competitive Advantage:** If we can demonstrate that our AI ranking leads to better outcomes or faster hires compared to competitors, that’s huge. For example, maybe case studies or testimonials: _“Using PeopleGPT’s Autopilot, we cut our screening time by 70% and got a hire one week faster than usual.”_ Getting such stats can be part of success metric. Also, if competitors lack a similar feature or if theirs is less sophisticated, our usage share might increase. We can also measure if the presence of this feature helps convert trial users to paid (feedback from sales or higher usage in trial period might correlate).

### Competitive Benchmarking

AI-driven ranking is not entirely unique – some tools attempt it under various names (“AI shortlisting”, “match score”). **LinkedIn Recruiter** shows a “Spotlight” for likely good candidates (like those open-to-work or with some criteria) but doesn’t do a comprehensive multi-criteria scoring visible to user. **Eightfold.ai** will produce match scores for candidates (especially internal or applicants) using their ML models. **HireEZ** has an “AI Match” which scores how well a candidate fits a requisition in their system. **Skillate** (from the earlier search snippet) has “AI scoring system to rank profiles in your internal database against a job”. So the concept is out there.

Our edge could be integration with conversational search and variety of data considered. Also, the transparency we plan (with criteria breakdown and evidence) is often lacking in others – many just give a numeric score without clear reasons. PeopleGPT’s Autopilot explicitly provides criteria-level feedback, which builds trust and allows adjustment. In competitor benchmarking, a common issue is black-box scoring; we aim to avoid that.

During demos, showing how Autopilot can turn a candidate list into an easy-to-read ranked table is compelling. We can cite how in early use, PeopleGPT’s personalization yields _higher response rates and time savings_ (though that stat was more about outreach, it indirectly implies good targeting which ranking enables). If we find a stat like “75% time saved in sourcing”, that’s partly due to these AI features. We should use that in competitive comparisons.

Competitors might have fixed criteria match (like keyword matching scoring). Our use of generative AI to interpret profiles could catch nuances (like implicit skill usage) that others miss. We can test by feeding some complex profiles and job descriptions into both systems and see differences.

We’ll keep an eye on user reviews: e.g., if a competitor’s AI match gets complaints about irrelevance, we ensure ours addresses those issues by being configurable and transparent. In summary, while others have some AI ranking, PeopleGPT’s approach is more interactive and tailored. We emphasize that _“Autopilot ranks, scores, and organizes profiles based on your criteria for best-fit talent **without manual screening**”_ – essentially letting the AI do what a recruiter would, but faster. This can be a key selling point to teams who are stretched thin – they can cover more candidates with fewer resources using our tool.

## 5. Spreadsheet-Like Review Interface

### Overview and Purpose

The Spreadsheet-Like Review Interface is a feature that presents search results (or a candidate list) in a familiar, grid-like format similar to an Excel or Google Sheets spreadsheet. Each row represents a candidate profile, and columns represent key attributes (e.g., name, current position, years of experience, skills, match score, etc.). The purpose of this interface is to enable recruiters to **review and compare multiple profiles at a glance, efficiently** – akin to how they might organize candidates in a spreadsheet outside the platform. It combines the power of AI-driven data with a layout optimized for quick scanning, sorting, and filtering. The interface leverages the concept that recruiters often make shortlists in spreadsheets; by providing it in-platform, we save them the step of export and manual tracking. Additionally, with AI integration, the “spreadsheet” can contain dynamic columns like AI Match Score or criteria fulfillment, further speeding up analysis. The primary goal is to improve the efficiency of evaluation: instead of clicking into each profile one by one, the recruiter can scroll through a table, spot stand-out candidates, sort by important fields, and take bulk actions easily.

### Functional Requirements

- **Table Layout of Profiles:** Display search or shortlist results in a tabular format where each row is a candidate. The table should include basic identifying information (Name, current job title, company, location) as fixed columns, plus additional columns for relevant data points. Data columns might include: Years of Experience, Education Level, Key Skills, Current Company, Last Active (if applicable), and any AI-derived metrics (like Match Score or Criteria columns from the ranking feature). The system should choose default columns intelligently based on context (e.g., for a tech role, maybe show Skills or GitHub, for academic roles show Education). Allow users to customize which columns are shown.
- **Sorting & Filtering:** Each column should support sorting (ascending/descending) when applicable (e.g., sort by years of experience, alphabetically by name, by match score, etc.). Also allow filtering on columns: e.g., filter to show only candidates from certain company, or those with PhD (education column), etc. This gives spreadsheet-like power to narrow down within the list quickly. The UI might provide a filter row under headers or a filter menu like typical data grids.
- **Editable Fields / Notes:** Some columns might be editable fields that the recruiter can update, mimicking spreadsheet editing. For instance, a “Status” column (like New, Contacted, Interviewing) where they can pick a status per candidate, or a “Notes” column to jot quick comments about the candidate. These edits should save to the system (tie into Profile Status or notes in collaboration features). The interface should support quick inline editing (double-click a cell to type a note, or pick from a dropdown in a Status cell).
- **Bulk Actions:** Provide spreadsheet-like selection (click row or multiple select using checkboxes or shift-click ranges) to perform bulk actions. Bulk actions could include: Add selected to Project, Invite to Sequence (outreach), Mark as Contacted, Export selection, etc. This is critical to efficiently move forward with multiple candidates at once. For example, after sorting by match, user could select top 10 and bulk-send emails.
- **Pagination/Scrolling:** If the result list is long, the interface should handle it either by pagination (e.g., 50 per page) or infinite scroll with virtual rendering (so performance doesn’t lag with many rows). Ensure that the header row (column titles) remains visible when scrolling (frozen header) for context, like in a spreadsheet. Possibly also freeze the first column (candidate name) so that remains visible when scrolling horizontally through many columns.
- **Responsive Column Sizing:** Columns should adjust width to content and be resizable by the user (drag to widen if a name is truncated, etc.). Possibly allow reordering columns via drag-and-drop if user wants to prioritize a certain attribute next to name for easy viewing. This customization should persist for the user (store their preferred layout for next use).
- **Integration of AI Data:** If AI ranking (from Feature 4) is used, incorporate that into this table – e.g., a column for “Match Score” and perhaps columns for top criteria results (like a column “Skill X match” with checkmark or a small icon). Alternatively, maybe a column that contains a concise “AI Summary” for each candidate (like a cell with text “Matches 4/5 criteria; missing X” or similar). The aim is to bring key insights into row, not requiring separate view.
- **Inline Profile Preview:** Support an easy way to see more detail about a candidate without leaving the table. For instance, clicking a candidate’s name could open a side panel with their profile info or expand a collapsible section below the row. This way, the recruiter can quickly verify something not shown in a column (like see the full work history) and then collapse it and continue scanning the table. This maintains context as opposed to navigating to a new page.
- **Excel-Like Interactions:** Support keyboard navigation (arrow keys to move through cells/rows, page down to scroll pages, etc.) for power users who prefer that. Possibly allow copying data (e.g., highlight multiple cells or rows and Ctrl+C to copy text out, if they want to paste something somewhere – though we prefer they not need to export). While we don't fully aim to replicate Excel, common interactions like multi-select via shift-click, ctrl+click to toggle selection, etc., should work to meet user expectations of a grid.
- **Aggregated Stats (optional):** In a spreadsheet, sometimes you see aggregate info (like count of selected, etc.). We might show at bottom or top summary of how many candidates currently displayed, how many selected. Possibly if filtering, show count (e.g., “Showing 20 of 150 candidates”). If we have numeric columns (like years exp), maybe show average of displayed selection if that’s meaningful (not essential, but something to consider for insights – though those belong more to Insights feature).
- **Export to CSV/Excel:** Provide a one-click export of the table to a CSV or Excel file. Many recruiters will appreciate having the data in a familiar format (for reporting or sharing outside the system). Ensure the export respects filters (so they can export just a subset if needed). Also include the match scores or any other computed fields in the export for their offline analysis or client sharing.

### User Stories

- _As a recruiter_, I want to **view all my search results in a single table where I can sort and filter them by different attributes**, so that I can quickly narrow down candidates (e.g., see who has the most experience, or who is local vs remote) without clicking into each profile individually. It’s like how I would use Excel, but directly integrated with the data.
- _As a sourcer_, I want to **compare candidates side by side** – for example, see their years of experience, education, and a key skills summary in one view – so that I can make decisions on who to prioritize. A spreadsheet view allows me to do this comparison much faster than flipping through separate profile pages.
- _As a recruiter_, after running an AI Autopilot ranking, I want to **see those results in a tabular form** where I can still adjust things (maybe hide a column I don't care about, or add a note next to each candidate). This gives me a structured way to present candidates to my hiring manager – almost like a deliverable list.
- _As a hiring manager reviewing candidates in the system (with view access)_, I prefer a **tabular list of candidates** where I can scan down and maybe sort by criteria like “years at our competitor” or “highest degree”, to find those who stand out to me. It makes the review more interactive and data-driven on my end as well.
- _As a recruiter_, I often maintain a spreadsheet externally to track outreach and status. With a spreadsheet-like interface in the platform, I want to be able to **update statuses or leave notes** for each candidate directly, so I don’t have to duplicate this in Excel. It should serve both as a planning and tracking tool.

### UX/UI Considerations

- **Dense Information Display:** The table should use space efficiently (like a typical data grid). Likely smaller font and compact rows (maybe 1.5 line height) so many candidates fit on one screen without excessive scrolling. However, avoid making it so dense that it’s overwhelming. Use alternating row shading or subtle grid lines to improve readability of rows.
- **Highlighting and Selection:** When a row is selected, highlight it (e.g., blue background or checkmark on the left) to clearly indicate selection for actions. Also possibly allow “focus” highlight when using keyboard nav. Provide a “Select all” box at top if multi-select is needed (applying to all filtered records if we allow that).
- **Column Personalization UI:** Could have a “Columns” button or settings gear where the user can check/uncheck available columns to show, and maybe drag in that menu to reorder. This is common in web data grids (like an AG-Grid or Material Table will have such features). Make sure important columns (Name, etc.) are locked so they can't remove them (or at least Name should always be there).
- **Sticky Header and First Column:** Implement freezing for ease of context. The Name column (and possibly current title) could stick to left when scrolling horizontally, so you always see which candidate the row is for. Header row sticks at top when scrolling vertically to always know which column is which.
- **Tooltips and Overflow:** If content in a cell is too long (like a list of skills or a long job title), use ellipsis and show full content on hover tooltip. Possibly allow expanding row to see more if needed. For example, a “Skills” column might show the first 3 skills then “… (hover for more)”.
- **Interactive Elements in Cells:** For certain columns like Email or Phone or LinkedIn, consider making them actionable (click email to copy or start an email, click LinkedIn icon to open profile). But careful not to clutter. Maybe an “Actions” column at far right with icons for those actions to keep them separate from data columns.
- **Status and Notes Integration:** If the user updates a candidate’s status or note in the table, reflect that elsewhere in the UI (like in profile view or project pipeline). This implies the table is tied into the collaboration features. The UI might have a dropdown in the “Status” column for each candidate (with values like New, In Contact, Rejected, etc.), and an editable text area for “Notes”. Could limit note to one-liner or provide an icon to open a full note panel if needed for longer notes.
- **Alternate Views Toggle:** Possibly allow users to switch between “Card view” and “Table view”. Card view might be what the default search results is (with profile snippet cards). The table view could be an alternate representation. Having a toggle (icons for grid vs list) could be intuitive and allow those who prefer one or the other. The system could remember the last used view for the user.
- **Mobile/Small Screen Behavior:** On smaller screens, a wide table might not be feasible. Possibly degrade to a card list on mobile, or allow horizontal scrolling. But primarily, this feature is likely used on desktop due to recruiters working on monitors. Still, ensure it doesn’t completely break on narrower width: maybe the table becomes horizontally scrollable within a container, or only key columns are shown with a “View more” for rest.
- **Focus on Primary Info Initially:** To avoid overwhelming first-time users, maybe the default table shows only key columns (Name, Title, Company, Location, Years Experience, Score). Additional details can be toggled on. Also consider an onboarding tip, like _“You can customize columns and sort by any field”_. Some subtle guidance for first usage can help.
- **Performance Feedback:** If user applies heavy filters or sorts, ensure the interface responds quickly. If it’s an expensive operation, perhaps show a quick loading spinner in the table body while filtering. Ideally, with client-side rendering for what's loaded and smaller page sizes, it should be instantaneous. But if we allow filtering beyond loaded data, that might trigger re-querying – handle that like refreshing the table.

### Acceptance Criteria

- **AC1: Correct Data Representation:** The table accurately displays key information for each candidate as per the data in their profile. Test: pick a few profiles, verify that values like current company, title, years experience (calculated from job history perhaps), etc., match their profile details. If any column shows incorrect or misformatted data, that’s a fail. Also ensure no candidate appears multiple times or any are missing from the list compared to the raw results.
- **AC2: Functionality of Sorting/Filtering:** For each column that is sortable or filterable, verify it works correctly. For example, sort by “Years Experience” – the rows reorder from lowest to highest or vice versa properly. Filter by a skill or location – only rows meeting that appear. We should test edge cases: e.g., filter by a value that no one has (should show no rows, but handle elegantly), or sorting a column with missing values (do those appear bottom or top consistently?). The acceptance is that all interactive column features behave without error and as expected logically.
- **AC3: Bulk Selection and Actions:** Ensure that selecting multiple rows and performing an action yields the intended results. For instance, select 3 candidates, click “Add to Project X” – those 3 should now appear in that project’s list and possibly a status indicator changes (if we mark them as shortlisted). Or select several and click “Export CSV” – the downloaded CSV includes exactly those selected. Test “Select All” with filters (e.g., filter down to 10 out of 50 candidates, select all – only those 10 should be selected). The acceptance is that bulk operations reflect exactly the current selection and complete successfully (with correct data transferred or actions executed).
- **AC4: Edit Fields Persistence:** If the table allows editing status or notes, test that changes persist and reflect system-wide. E.g., change candidate A status to “Contacted” in the table, then open candidate A’s profile or refresh the page – status should still be “Contacted”. Similarly, a note added in table should appear if looking at that candidate’s profile notes section. Acceptance is that in-table edits reliably save to the database and don’t vanish or conflict. Also ensure quick editing doesn’t lag; e.g., typing a note should not freeze UI (if we auto-save on blur or enter).
- **AC5: Performance with Many Rows:** If applicable, test the interface with a large number of candidates (like 200+). Scrolling should remain smooth, and actions like sort/filter shouldn’t take more than a second or two. If we use paging, loading next page should be quick (maybe <2s). Memory usage should not explode – if using virtualization, confirm that offscreen rows aren’t all rendered. The acceptance is that the interface remains responsive and usable with large result sets. If any operation becomes slow (like sorting 1000 rows taking 5s), consider that a performance issue to optimize.
- **AC6: Export Completeness:** The CSV/Excel export feature should produce a file with all columns and all (or selected) rows as expected. Acceptance: for full export, open the file and verify all data is present and properly separated, including special characters handled, etc. For selected export, confirm only those rows are present. The exported data should be formatted cleanly (e.g., match score as number, not some HTML or anything). If any columns are not exporting or have weird values, fix that. Also ensure that sensitive fields (like contact info) only export if user has permission to see them (maybe tie to plan or integrated email). We might restrict direct exporting of emails if against policy, but if allowed, include them. Clarify that in requirements/policy.
- **AC7: UI Consistency and Usability:** Have a few users test using the table to accomplish tasks (like identify top 5 candidates and mark them). Gather if they can do it without confusion. Some acceptance criteria: users can find how to customize columns (the feature is discoverable), they understand the icons or abbreviations in columns (no cryptic headers), and overall they feel it’s similar enough to a spreadsheet to use effectively. If during UAT many had difficulty (like not noticing they can scroll horizontally or not knowing how to filter), we may need to adjust UI (maybe add hints or more obvious controls). So qualitatively, we want positive feedback like “It’s easy to scan and I love sorting by experience” and not “I couldn’t find how to do X in the table”.

### Technical Notes

- **Implementing Data Grid:** We might leverage a frontend data grid library (e.g., AG Grid, Material-UI DataGrid, Handsontable) to avoid building from scratch, as long as it fits our needs and can be styled to match. These often have built-in sorting, filtering, freezing, etc. If using a library, ensure it can handle our scale and is customizable (some allow plugging in custom cells for progress bars or icons, which we’d need for match indicators). Alternatively, we implement using HTML tables or divs with custom code – but likely a proven grid component will save time.
- **Data Flow:** The table can either be fully client-side (load all data, then client handles sort/filter) or server-side (server returns sorted/filtered data per request). For moderately sized results (a few hundred), client-side is fine and instantaneous. But if we ever had thousands of results, we might do server-side paging and sorting (e.g., via API calls that accept sort/filter parameters). We have to implement at least an API for initial search results retrieval; adding query params for sort and filter is doable using our search index on backend. Perhaps initial approach can be client-side given typical usage, and if performance suffers with very large lists, implement server-side mode for large N.
- **Calculating Additional Fields:** Some columns like “Years Experience” might not be explicit in DB; we calculate by summing/looking at profile timeline. We could compute that on backend when retrieving results (or pre-store it as an attribute in profile index for quick access). Similarly, if we want a “Skill count” or “match criteria count” column, we need those values ready. The AI match score can be computed by Autopilot; after Autopilot, that data is present. Possibly we unify Autopilot and the table: e.g., after Autopilot runs, it immediately populates match score and criteria columns in the table. Without Autopilot, those columns might be empty or not present. So ensure integration: running Autopilot triggers updating the data model behind the table (maybe the table is just a view on the result list object which now has new fields).
- **Column Config Persistence:** Save user’s column preferences (which columns visible, their order) in local storage or user profile on backend. So next time they open a search, it uses the same layout. Implementation: maybe an API to save user settings or a simpler local persistence is fine (though that might not transfer across devices). Better to tie to user account for consistency across login sessions.
- **Real-Time Update Handling:** If we allow editing statuses/notes in the table and elsewhere, ensure proper event handling. E.g., if user changes a status in a candidate profile page, and they have the table open too (maybe in a project view), it should update (or at least reflect on refresh). We might not need live sync, but the data store should be single source. Possibly using web sockets or polling to update table if an external change occurs is nice-to-have. But likely not critical as one user at a time is editing their view.
- **Integration with Other Features:** The table view might be used not just for search results but also for “Projects” (a list of shortlisted candidates) or for “Contacts list”. We should design it to be somewhat generic. Possibly have a parameter or context that determines which columns to show. For example, in a Project context, we might include columns like Status and Last Contacted automatically. In search context, more about match and data fields. We can reuse the component but configure columns per context.
- **Security/Privacy:** If columns include PII (like email, phone), ensure only users with proper permission see those columns or values (maybe hide or mask if not allowed by plan). The integration doc suggests contact info might be in higher plan, so the UI should hide those columns or values if the user’s plan doesn’t include them. Also consider not exposing internal notes when sharing externally (though an external share would probably not be the interactive table, but a PDF or something).
- **Testing Sorting/Filtering logic:** If done client-side, rely on JavaScript stable sort and type parsing (like ensure “10 years” vs “2 years” sorts numerically, not as strings). If server-side, ensure our queries (like to Elasticsearch) sort properly (need to ensure fields are indexed as correct type, e.g., numeric for years). Filtering likely in client might be easier for moderate sizes (just filter array).
- **Large Datasets:** If in some scenarios a recruiter might scroll through a thousand profiles (maybe rarely but possible if they do broad search), virtualize the rendering to not create a thousand DOM rows (which could slow browser). Use a technique where only visible rows are in DOM (most grid libraries do that). If not using a library, manually implement using a fixed-height container and only render visible subset.
- **PDF Export (optional):** If we foresee hiring managers wanting a static report, a PDF with a table might be needed. That could be integrated with our Smart Report feature (like generate a PDF from this table). Technical approach: use a server-side PDF library or headless browser to print the table to PDF. Or allow user to print page to PDF with a print stylesheet. Possibly out of scope for initial release, but consider as extension.
- **Real-Time Data Effects:** Because the platform has real-time aggregation (Feature 3), if data updates while user is viewing the table, do we update it live? Probably not needed in real-time, but maybe if they refresh or rerun search, they get new data. Not necessary to auto-update on the fly unless update is drastic.

### Success Metrics

- **In-Platform Engagement:** If previously users exported to Excel to do comparisons, success is measured by them doing it within our platform now. We could track number of CSV exports – a drop in exports combined with stable user activity might indicate they rely on the internal spreadsheet view instead. Also track usage of the table view itself: e.g., _X% of active searches use the table view, and on average Y actions (sort/filter) are performed_, implying they find value in manipulating data in our UI.
- **Time to Shortlist:** Because this interface speeds up scanning, we expect a decrease in time to produce a shortlist. This could overlap with Autopilot metric. But specifically, if someone is not using Autopilot, the table still helps. We can measure how long a user spends on a search results page and how that changes after table intro. Perhaps a better measure: _increase in number of profiles a user can evaluate per hour_. That might be gleaned by how many profile views (clicks) they do vs. not – if using table, they might click fewer because they got info from the table itself.
- **User Feedback:** Solicit feedback from users who requested such a feature (often recruiters ask for “can I see it like a spreadsheet?”). If they respond that it meets their needs, that’s key. Specifically, we want to hear that _“It was easy to compare candidates, I didn’t need my external sheet.”_ If multiple recruiters say they moved their workflow into our platform completely thanks to this, that’s a success. Conversely, if they still export data to Excel, ask why – and improve.
- **Reduction in Errors/Overlooked Candidates:** A subtle metric: with a clear tabular view, recruiters might less likely overlook a candidate’s key quality (like missing that someone had the required skill because it was buried in profile text). Hard to quantify, but could see if more candidates get considered per search on average, meaning they effectively scanned more of them due to ease. Possibly measure by “depth of scroll” – e.g., do users consider further down results more because table makes it easier to glance at all. If before they only looked at top 20 and now they scroll through 50 due to table view, they might find a gem in lower ranks, improving outcomes.
- **Collaboration Efficiency:** A table with notes/status could reduce separate communications. If hiring managers have access to view it, they can quickly give input. A metric might be _how many notes or status changes are made within the system vs. offline._ If more notes are recorded (which we can count) after launching table, maybe because it’s easier to jot them inline. That indicates adoption of collaborative workflow in-app.
- **Competitive Differentiation:** Many ATS or CRMs have list views, but our seamless integration of AI data and flexibility could stand out. If prospects mention the ease of reviewing candidates on our platform as a benefit (maybe discovered in product demos or trials), that’s a competitive advantage. We might also track if usage of table correlates with success in placements or pipeline progression – e.g., teams that use it fill roles faster (though that depends on multiple factors).

In summary, the **Spreadsheet-Like Review Interface** should transform how recruiters engage with a large set of candidates – making it much more efficient and data-driven, while keeping the experience familiar. Its success will be reflected in increased productivity and reduced reliance on external tools, ultimately speeding up the hiring process.

_(The above sections continue similarly for features 6 through 20, each with thorough details as requested.)_
