# Introduction to Observability

In modern microservices, observability is crucial for understanding system behavior and diagnosing issues. Unlike traditional monolithic apps, microservices generate a distributed web of events that can be hard to trace without proper tooling ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=When%20all%20of%20an%20application%27s,Distribution%20changes%20everything)) ([Spring Boot Logs Aggregation and Monitoring Using ELK Stack](https://auth0.com/blog/spring-boot-logs-aggregation-and-monitoring-using-elk-stack/#:~:text=Having%20a%20good%20log%20monitoring,in%20case%20an%20error%20comes)). **Observability** is defined as how well you can infer a system’s internal state from its outputs (logs, metrics, traces) ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=Microservices%20observability%20refers%20to%20the,time)). It rests on three key pillars: **Logging, Metrics, and Tracing** ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=Observability%20in%20microservices%20architecture%20involves,three%20fundamental%20pillars)). Each pillar provides a different view of system health and together they offer a holistic picture ([The 3 pillars of observability: Logs, metrics and traces | TechTarget](https://www.techtarget.com/searchitoperations/tip/The-3-pillars-of-observability-Logs-metrics-and-traces#:~:text=There%20are%20many%20potential%20data,most%3A%20logs%2C%20metrics%20and%20traces)):

- **Logs:** Immutable, timestamped records of discrete events (e.g. errors, state changes). Logs are rich in context and answer the “what happened when” questions ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=1.%20Logs%3A%20Detailed%2C%20time,as%20they%20flow%20through%20multiple)). In a microservice chain, consistent logging (with correlation IDs) helps follow a request across services ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=,again%20using%20the%20same%20correlation_id)).
- **Metrics:** Numerical measurements collected over time, representing system state or performance (e.g. request count, CPU load, memory usage) ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=2,identify%20bottlenecks%20and%20optimize%20performance)). Metrics are used to observe trends and trigger alerts on threshold breaches. They capture the “quantitative” health of services (throughput, latency percentiles, error rates, etc.) ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=2,identify%20bottlenecks%20and%20optimize%20performance)).
- **Traces:** End-to-end records of a request’s path through multiple services, capturing timing and causal relationships between components ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=to%20understand%20trends%2C%20set%20thresholds,identify%20bottlenecks%20and%20optimize%20performance)). A trace is composed of spans (per-service/per-operation timing segments) that together show where latency occurs in a workflow ([Distributed tracing with Zipkin and springcloud-sleuth](https://rogerwelin.github.io/zipkin/java/tracing/2017/08/06/distributed-tracing-with-zipkin.html#:~:text=Zipkin%E2%80%99s%20design%20is%20based%20on,sleuth%20%28more%20on%20this%20later)). Tracing answers “where and why did a request slow down or fail” by linking events across services ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=3.%20Traces%3A%20End,identify%20bottlenecks%20and%20optimize%20performance)).

**Why Observability?** In a complex microservice architecture, any single user action can invoke many services. Observability allows you to pinpoint issues in such distributed flows. It reduces MTTR by correlating logs, metrics, and traces to identify root causes quickly ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=,problems%20before%20they%20impact%20users)). For example, metrics might show elevated latency, traces can highlight which service is the bottleneck, and logs from that service will detail the error or exception ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=Metric%20payment_service_latency_milliseconds%7Bquantile%3D)) ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=%E2%94%94%E2%94%80%20Span%3A%20database_query%20,Bottleneck%20identified%20here)). In short, robust observability is essential for fault diagnosis, performance tuning, and maintaining reliability in production ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=,problems%20before%20they%20impact%20users)).

# Setting Up Observability in Spring Boot Services

Spring Boot offers extensive support for all three pillars of observability through its Actuator, Micrometer, and integration with tracing libraries. An advanced setup involves configuring structured **logging**, distributed **tracing**, and application **metrics** collection, and funneling these to the appropriate backends (e.g. ELK for logs, Prometheus for metrics, Zipkin/Jaeger for traces). Below is a step-by-step guide for implementing these in Spring Boot:

## Structured Logging (SLF4J, Logback, and ELK Stack)

**Enable Structured Logging:** By default, Spring Boot (via SLF4J + Logback) prints logs in a human-readable text format. For better analysis, especially in centralized logging systems, use _structured logging_ – formatting logs as JSON or other machine-readable structure ([Structured logging in Spring Boot 3.4](https://spring.io/blog/2024/08/23/structured-logging-in-spring-boot-3-4#:~:text=Structured%20logging%20is%20a%20technique,for%20structured%20logging%20is%20JSON)). JSON logs include key–value fields (timestamp, level, message, service name, etc.) that log aggregators can index and query efficiently. For example, Spring Boot 3.4+ supports JSON logging using the Elastic Common Schema (ECS) with a one-line config (`logging.structured.format.console=ecs`) ([Structured logging in Spring Boot 3.4](https://spring.io/blog/2024/08/23/structured-logging-in-spring-boot-3-4#:~:text=With%20Spring%20Boot%203,it%20with%20your%20own%20formats)) ([Structured logging in Spring Boot 3.4](https://spring.io/blog/2024/08/23/structured-logging-in-spring-boot-3-4#:~:text=)). In earlier versions, you can achieve JSON logs by adding Logback encoders. For instance, include the Logback JSON and Jackson dependencies and define a `logback-spring.xml` with a JSON layout:

```xml
<appender name="JSON" class="ch.qos.logback.core.ConsoleAppender">
    <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
        <layout class="ch.qos.logback.contrib.json.classic.JsonLayout">
            <timestampFormat>yyyy-MM-dd'T'HH:mm:ss.SSSX</timestampFormat>
            <jsonFormatter class="ch.qos.logback.contrib.jackson.JacksonJsonFormatter">
                <prettyPrint>true</prettyPrint>
            </jsonFormatter>
        </layout>
    </encoder>
</appender>
<root level="INFO">
    <appender-ref ref="JSON"/>
</root>
```

This configuration, combined with the corresponding `logback-json-classic` and `logback-jackson` dependencies, outputs logs in structured JSON format ([logging - Is there a recommended way to get spring boot to JSON format logs with logback - Stack Overflow](https://stackoverflow.com/questions/53730449/is-there-a-recommended-way-to-get-spring-boot-to-json-format-logs-with-logback#:~:text=%3Cappender%20name%3D,%3CprettyPrint%3Etrue%3C%2FprettyPrint%3E%20%3C%2FjsonFormatter)) ([logging - Is there a recommended way to get spring boot to JSON format logs with logback - Stack Overflow](https://stackoverflow.com/questions/53730449/is-there-a-recommended-way-to-get-spring-boot-to-json-format-logs-with-logback#:~:text=%3Cdependency%3E%20%3CgroupId%3Ech.qos.logback.contrib%3C%2FgroupId%3E%20%3CartifactId%3Elogback,jackson%3C%2FartifactId%3E%20%3Cversion%3E0.1.5%3C%2Fversion)). Each log event becomes a JSON object containing fields like timestamp, log level, logger name, thread, and the log message, which is ideal for ingestion into log analytics systems.

**Include Context and Correlation IDs:** It’s a best practice to enrich logs with context data (using Mapped Diagnostic Context – MDC). Spring Cloud Sleuth (in Spring Boot 2.x) or Micrometer Tracing (Spring Boot 3.x) can automatically add trace and span identifiers to MDC, so every log line includes tracing info for correlation ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=Spring%20Cloud%20Sleuth%20sets%20up,the%20logging%20for%20your%20microservice)). For example, with Sleuth, your logs will show entries like:

```
2025-02-26 22:10:00 [INFO] [payment-service,traceId=4f8de729b1e3f19c,spanId=4f8de729b1e3f19c] Payment succeeded for Order 12345
```

Here `traceId` ties together logs across services for the same transaction ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=Spring%20Cloud%20Sleuth%20sets%20up,the%20logging%20for%20your%20microservice)). If you generate a correlation ID at the edge (e.g., in API Gateway) and pass it through service calls (via headers or MDC), it further helps link logs across microservices ([Microservices Observability: Leveraging Logs, Metrics, and Traces for Enhanced System Performance](https://openobserve.ai/blog/microservices-observability-logs-metrics-traces#:~:text=,again%20using%20the%20same%20correlation_id)). Ensure all services log this ID so you can search logs in Kibana by the ID to reconstruct a distributed transaction.

**Centralize Logs with ELK:** Once logs are structured, ship them to a central store for analysis. The ELK stack (Elasticsearch, Logstash, Kibana) is a popular choice ([Spring Boot Logs Aggregation and Monitoring Using ELK Stack](https://auth0.com/blog/spring-boot-logs-aggregation-and-monitoring-using-elk-stack/#:~:text=But%20it%20becomes%20very%20complex,centralized%20log%20aggregation%20and%20analysis)). You can use Logstash or Beats (e.g. Filebeat) to collect log files and send them to Elasticsearch. For example, Filebeat can tail your Spring Boot app’s log file and push JSON entries to Elasticsearch. Logstash can also parse and buffer logs. In Elasticsearch, each log field (timestamp, level, service, message, etc.) becomes indexed, allowing powerful searches and filtering. Kibana then provides a UI to search and visualize the logs. For instance, you could filter logs by `service.name` or `ERROR` level across all microservices. This setup makes debugging easier in distributed environments, where a single action might generate logs in many services ([Spring Boot Logs Aggregation and Monitoring Using ELK Stack](https://auth0.com/blog/spring-boot-logs-aggregation-and-monitoring-using-elk-stack/#:~:text=Having%20a%20good%20log%20monitoring,in%20case%20an%20error%20comes)) ([Spring Boot Logs Aggregation and Monitoring Using ELK Stack](https://auth0.com/blog/spring-boot-logs-aggregation-and-monitoring-using-elk-stack/#:~:text=But%20it%20becomes%20very%20complex,centralized%20log%20aggregation%20and%20analysis)).

_Example:_ Suppose a request fails in a chain of services. With centralized logging, you can query Kibana for the correlation ID or trace ID and immediately retrieve log entries from Service A, B, and C related to that request, even if they ran on different hosts. This beats ssh’ing into individual boxes to gather logs. Make sure to set up index rotation and retention in Elasticsearch (e.g. delete or archive indices older than X days) to manage storage costs – log volume grows quickly in microservices.

**Tip:** Use logging levels wisely – INFO for high-level events, DEBUG for development diagnostics, WARN/ERROR for problems. In production, excessive DEBUG/TRACE logging can flood your system, so enable them selectively (perhaps temporarily when troubleshooting). Always mask or omit sensitive data (personal info, credentials) from logs for security.

## Distributed Tracing (Spring Cloud Sleuth, Zipkin, Jaeger)

**Add Tracing Libraries:** To trace requests across services, integrate a tracing library that generates and propagates trace IDs/spans. In Spring Boot 2.x, the go-to solution was _Spring Cloud Sleuth_ ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=Spring%20Cloud%20Sleuth%20%28%60org.springframework.cloud%60%3A%60spring,automatically%20instruments%20common%20communication%20channels)), often paired with Zipkin as the backend trace collector. Spring Boot 3.x introduces _Micrometer Tracing_, which is Sleuth’s successor, offering an API compatible with OpenTelemetry. Both approaches achieve a similar goal: instrument the application to report trace data.

For Sleuth + Zipkin (Boot 2): add the `spring-cloud-starter-sleuth` and `spring-cloud-sleuth-zipkin` dependencies. By default, Sleuth will generate a unique **trace ID** for each incoming request and propagate it downstream (via HTTP headers like `X-B3-TraceId` and `X-B3-SpanId`). It auto-instruments common communication channels: incoming HTTP requests (Spring MVC controllers), RestTemplate calls, WebClient, messaging consumers/producers (Kafka, RabbitMQ), etc. ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=Spring%20Cloud%20Sleuth%20%28%60org.springframework.cloud%60%3A%60spring,automatically%20instruments%20common%20communication%20channels)). This means you get trace IDs flowing through message queues and REST calls without extra code. Sleuth also configures logging to include trace IDs, as noted earlier, and it samples traces (10% by default) to avoid overwhelming the system with every request trace. You can adjust `spring.sleuth.sampler.probability` as needed.

On application startup, if it detects the Zipkin client on the classpath, Sleuth will send completed trace data to a Zipkin server. By default, it tries `http://localhost:9411` (Zipkin’s default URL), but you can configure `spring.zipkin.base-url` to point to your Zipkin collector. With this in place, running a Zipkin server (via Docker or jar) will start receiving traces from your services automatically.

For Micrometer Tracing + OpenTelemetry (Boot 3): include `spring-boot-starter-actuator` (which brings in Micrometer) and the Micrometer Tracing bridge for OpenTelemetry (for example, `micrometer-tracing-bridge-otel` and the OpenTelemetry exporter for Zipkin or Jaeger). Spring Boot 3 autoconfigures Micrometer Tracing if these are present ([Observability with Spring Boot 3](https://spring.io/blog/2022/10/12/observability-with-spring-boot-3#:~:text=The%20upcoming%20Spring%20Boot%20%603.0.0,helps%20to%20label%20the%20observations)) ([OpenTelemetry Tracing on Spring Boot](https://dzone.com/articles/opentelemetry-tracing-on-spring-boot-java-agent-vs-micrometer-testing#:~:text=match%20at%20L417%20Tracing%20started,of%20Micrometer%20Tracing%2C%20as%20it)). This setup uses OpenTelemetry under the hood for context propagation (W3C Trace Context) and can export traces to various backends. For example, to use **Jaeger** (an open-source tracing system), you might include the OpenTelemetry Jaeger exporter. Spring’s Micrometer bridge can send spans to a local OpenTelemetry Collector, which in turn forwards to Jaeger. (Alternatively, you can run the Jaeger all-in-one container and have the app send spans directly to it via gRPC or HTTP.) Recent guidance suggests migrating from the older Sleuth/Brave (Zipkin format) to OpenTelemetry standard for Jaeger support ([How to send traces from Spring Boot to Jaeger? : r/kubernetes - Reddit](https://www.reddit.com/r/kubernetes/comments/146lwkg/how_to_send_traces_from_spring_boot_to_jaeger/#:~:text=How%20to%20send%20traces%20from,the%20OpenTelemetry%20Java%20Agent%20Instrumentor)). In practice, this means using the Micrometer/OTel setup or the OpenTelemetry Java Agent.

**Configure and Verify Traces:** Once instrumentation is in place, you should start seeing trace logs in the console (trace IDs in log statements) and can verify that trace data is reaching the backend. For Zipkin, open the Zipkin UI (usually at `http://<zipkin-host>:9411`) and search for traces by service name. You should find traces that show the timeline of a request across your Spring Boot services. For Jaeger, open the Jaeger UI (typically `http://<jaeger-host>:16686`), select a service, and you can query recent traces. Each trace will display a **waterfall view** of spans – for example, a trace might show: `OrderService -> InventoryService -> PaymentService`, with each segment’s duration. This helps pinpoint where delays occur. If you see no traces, ensure that sampling isn’t disabling all traces (set it to 1.0 for testing to sample 100%), and check that the app can reach the collector endpoint.

**Propagation in Messaging:** If your services communicate via Kafka or RabbitMQ, tracing libraries handle propagation by injecting trace context into message headers ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=Spring%20Cloud%20Sleuth%20%28%60org.springframework.cloud%60%3A%60spring,automatically%20instruments%20common%20communication%20channels)). For example, Sleuth will add trace IDs to the message headers so that when another service consumes the message, it can continue the trace. As a result, a trace can start with an HTTP request in Service A, continue through a Kafka topic to Service B, and maybe through another queue to Service C, all under one trace ID. In the trace UI, you’ll see spans for the Kafka produce and consume operations linking the services. This is extremely valuable for debugging asynchronous processing flows. (Make sure your message producers and consumers use the Spring Cloud Stream or Spring Integration components that Sleuth can hook into, or manually pass the tracing context.)

**Choose Zipkin vs Jaeger:** Both are popular distributed tracing systems. **Zipkin** is simpler to set up; it uses a Zipkin-specific data format (though OpenTelemetry can export to Zipkin). **Jaeger** (originated by Uber) implements the OpenTracing standard and has a more feature-rich UI (with trace comparisons, adaptive sampling, etc.). If you already use Sleuth, Zipkin is plug-and-play. For Jaeger, consider migrating to Micrometer Tracing (OpenTelemetry) as mentioned, or run Jaeger’s collector with Zipkin-compatible HTTP endpoint (Jaeger can ingest Zipkin format spans as well). There are also commercial SaaS solutions (Datadog, etc.) which the OpenTelemetry exporter can send traces to, but those are beyond our scope here.

## Collecting Application Metrics (Micrometer & Prometheus)

**Spring Boot Actuator and Micrometer:** Spring Boot’s Actuator module, when added, automatically integrates with **Micrometer** – a facade for metrics instrumentation that supports many backends ([54. Metrics](https://docs.spring.io/spring-boot/docs/2.0.0.RELEASE/reference/html/production-ready-metrics.html#:~:text=Spring%20Boot%20Actuator%20provides%20dependency,supports%20numerous%20monitoring%20systems%2C%20including)). Micrometer is the library through which Spring Boot captures metrics like request counts, JVM stats, etc., and you choose a registry (backend) to export those metrics. For a typical open-source stack, **Prometheus** is an excellent choice for metrics storage. Boot provides a Micrometer registry for Prometheus. By adding the dependency `io.micrometer:micrometer-registry-prometheus` to your project (and Actuator), Boot will auto-configure a Prometheus scrape endpoint ([54. Metrics](https://docs.spring.io/spring-boot/docs/2.0.0.RELEASE/reference/html/production-ready-metrics.html#:~:text=54)) ([spring boot - HikariCp metrics collection using micrometer in Java springboot application by configuring it in - Stack Overflow](https://stackoverflow.com/questions/72556111/hikaricp-metrics-collection-using-micrometer-in-java-springboot-application-by-c#:~:text=Now%2C%20you%20can%20simply%20add,metrics%20data%20from%20your%20system)). Specifically, you’ll get an Actuator endpoint `/actuator/prometheus` that exposes all metrics in Prometheus plaintext format.

Out of the box, Spring Boot with Actuator contributes a wealth of metrics: HTTP request rates and timings (`http.server.requests`), JVM memory and garbage collection stats, CPU usage, thread pool stats, and more ([Monitoring Spring Boot Application With Prometheus | by Nikhil YN](https://medium.com/@nikhil.nagarajappa/monitoring-spring-boot-application-with-prometheus-8b5c1b7aacce#:~:text=Monitoring%20Spring%20Boot%20Application%20With,database%20connection%20pool%20usage)) ([spring boot - HikariCp metrics collection using micrometer in Java springboot application by configuring it in - Stack Overflow](https://stackoverflow.com/questions/72556111/hikaricp-metrics-collection-using-micrometer-in-java-springboot-application-by-c#:~:text=,of%20your%20custom%20metrics%20instrumentation)). If you use a database connection pool like HikariCP, metrics like active connections, idle connections, and pool usage are also collected (under `jdbc.connections.*`) ([Spring hikari datasource metrics not visible on prometheus endpoint](https://github.com/spring-projects/spring-boot/issues/40048#:~:text=endpoint%20github,as%20jdbc_connections_max%20in%20actuator)). Similarly, if you use Spring MVC or WebFlux, Actuator captures metrics on request throughput and latency. All these are available without writing extra code. For example, the metric `http.server.requests` has tags like `exception`, `method`, `outcome`, `status`, and `uri`, and tracks count and timing – you can use it to graph request rate or 95th percentile latency per endpoint.

**Expose Metrics to Prometheus:** The `/actuator/prometheus` endpoint provides a unified metrics output. Now install Prometheus and configure it to scrape your Spring Boot service. In Prometheus’s config `prometheus.yml`, add a job for your service, e.g.:

```yaml
scrape_configs:
  - job_name: "order-service"
    metrics_path: "/actuator/prometheus"
    static_configs:
      - targets: ["hostname:port"]
```

Replace `hostname:port` with where your app runs (e.g. `localhost:8080`). Prometheus will pull metrics at the defined interval (e.g. every 15s). Ensure that Actuator’s HTTP port is accessible (you might need to enable the endpoint and possibly security if in production).

**Visualize with Grafana:** With metrics in Prometheus, you can use **Grafana** to create dashboards. Grafana natively supports Prometheus as a data source. Once connected, you can write PromQL queries to populate graphs. For example, to graph HTTP 5xx error rate: `sum(rate(http_server_requests{status=~"5.."}[1m]))` per service, or to monitor average latency: `avg(rate(http_server_requests_sum[5m])) / avg(rate(http_server_requests_count[5m]))`. Grafana’s flexibility lets you plot these over time, create singlestat panels for current values (e.g. memory usage), and set up alerts. There are even pre-built Grafana dashboards for Spring Boot metrics (for instance, a community dashboard that charts JVM memory, CPU, threads, and web request metrics) – you can import those to jumpstart your monitoring.

**Custom Metrics:** If your application has domain-specific metrics (e.g. “orders.processed” count, or “cache.hit.rate”), you can instrument those using Micrometer’s API. For example, inject `MeterRegistry` and use `counter = registry.counter("orders_processed_total")` and increment it in your business logic. These custom meters will appear alongside built-in metrics. Always choose clear metric names and tag keys (e.g. `region`, `service`, etc.), and use units in names (e.g. `_milliseconds`, `_bytes`).

**Metric Best Practices:** Focus on key **KPIs** that reflect service health. Common ones are the “Golden Four” or **RED** metrics: Rate (throughput, e.g. requests/sec), Errors (error count or rate), Duration (latency), and Saturation (resource usage like CPU, memory) ([Monitoring Distributed Systems - sre golden signals - Google SRE](https://sre.google/sre-book/monitoring-distributed-systems/#:~:text=SRE%20sre,facing%20system%2C%20focus%20on)). Identify what success means for your service (e.g. < 2s response time, error rate < 1%) and track those. In Micrometer/Prometheus, utilize percentile timers (Actuator’s default timers give you 95th and 99th percentiles) to catch tail latencies, and use counters for things like incoming requests or processed messages. By setting up dashboards for these, you can quickly spot anomalies (like a spike in errors or drop in throughput).

## Example: Bringing It Together for a Spring Boot Service

_(This subsection provides a brief hands-on scenario integrating logging, tracing, and metrics for one service.)_

Suppose we have an **Order Service** (Spring Boot). We enable Actuator and add Sleuth and Zipkin. We also configure Logback JSON logging. When the service starts, it registers with our Prometheus server and begins sending traces to Zipkin. A user places an order, which goes through the Order Service and then calls Payment Service. Here’s what happens observability-wise:

- The Order Service logs an INFO message “Order received” with a trace ID. The log is in JSON and immediately shipped to Elasticsearch; it includes `traceId: abc123` and `service: order-service`.
- The same trace ID is propagated to Payment Service via an HTTP header. Payment Service logs an ERROR “Payment failed” with `traceId: abc123`. Now in Kibana, we can filter logs where `traceId = abc123` and see the entire story across services.
- Sleuth generates spans for the Order -> Payment call. Zipkin records a span for Order Service (e.g. 120ms) and one for Payment Service (e.g. 450ms). In the Zipkin UI, we see a trace “POST /orders” taking 570ms, with breakdown: Order Service 120ms, Payment Service 450ms. We notice Payment Service’s span is long; drilling into logs (or the trace detail) shows an SQL query timeout.
- Micrometer has been measuring our Order Service’s performance. In Prometheus/Grafana, we see an **error rate spike** on the “create order” API and an increase in the latency. An alert is triggered because the 99th percentile latency exceeded 1s. This correlates with what we saw in tracing.

Through this example, we used logs to pinpoint the error, metrics to quantify the impact, and tracing to locate the latency. In practice, setting this up requires some configuration, but Spring Boot’s automation covers most of it as described.

# Database Observability

Databases often become a bottleneck in microservices, so it’s important to monitor them closely. Spring Boot/Actuator provides basic health and metrics, but you should extend this with database-specific observability.

**Actuator DB Metrics & Health:** The Actuator `health` endpoint includes a database health check by default (it will attempt a test query to confirm connectivity). This is useful for automated checks (or container orchestration liveness probes). For metrics, if you’re using a DataSource that Micrometer knows (HikariCP is auto-instrumented), you will have metrics like `hikaricp.active`, `hikaricp.idle`, `hikaricp.pool.max` etc. exported ([Spring hikari datasource metrics not visible on prometheus endpoint](https://github.com/spring-projects/spring-boot/issues/40048#:~:text=endpoint%20github,as%20jdbc_connections_max%20in%20actuator)). These show the number of active connections, how many are idle, and the pool size. Actuator/Micrometer will also track things like the count of SQL queries if you use Spring Data repositories (as a gauge of throughput). All these can be scraped by Prometheus and graphed. For example, a Grafana panel could show DB connection pool usage over time – if you see active connections constantly hitting the max, you know your DB is under pressure or connections aren’t returning (possible leak).

**SQL Performance Metrics:** Out of the box, you might not get query times as individual metrics (beyond what your app code might measure or what the JDBC driver exposes). However, you can leverage tools: For instance, if using PostgreSQL, you could run a **Postgres Exporter** (outside the app) which provides Prometheus metrics on query throughput, locks, cache hit rate, etc. Similarly, MySQL has a Performance Schema and an exporter to expose its internals. Integrating these into your Grafana dashboards gives a more complete view (app metrics + DB metrics). In Spring Boot, an alternative is using an JDBC interceptor like _P6Spy_ to log all queries and their execution times – these can then be gathered (though that’s more of a logging solution, not metrics).

**Integrate DB metrics into Grafana:** Combine application-level metrics and database metrics on the same dashboard for context. For example, you might have a graph of HTTP request rate and on the same timeline a graph of active DB connections or slow query count. That helps see cause and effect (e.g. spike in traffic leads to spike in DB usage). If you send custom metrics from the app (like “number of orders in queue” or “DB query time for X operation”), those can complement the generic metrics.

**Slow Query Analysis:** Metrics might indicate the database is the hotspot (e.g. a specific trace shows a DB span took 4s). To analyze **slow queries** in depth, you typically use database-side tools or APM profilers. Enabling the MySQL Slow Query Log or using Postgres’s `log_min_duration_statement` setting can capture queries exceeding a threshold. Those logs, when aggregated, identify which SQL statements are frequently slow. Tools like **pgAdmin** (Postgres) or MySQL Workbench can help inspect running queries and their execution plans. You might run an `EXPLAIN ANALYZE` on the problematic query to see if it’s missing an index, for example. In production, an **APM tool like New Relic** can greatly help – New Relic’s APM will automatically instrument DB calls and provide a breakdown of time spent on each SQL query in each transaction ([Diagnose slow database queries | New Relic Documentation](https://docs.newrelic.com/docs/tutorial-improve-app-performance/slow-database-queries/#:~:text=If%20your%20site%20relies%20on,kind%20of%20solution%20you%20need)). The New Relic UI (or similar tools like AppDynamics, Datadog APM) can list the top slow queries, how often they occur, and even show the full SQL with obfuscated parameters ([Diagnose slow database queries | New Relic Documentation](https://docs.newrelic.com/docs/tutorial-improve-app-performance/slow-database-queries/#:~:text=If%20your%20site%20relies%20on,kind%20of%20solution%20you%20need)). This is incredibly useful for pinpointing inefficiencies in ORM-generated queries or long-running reports. If you prefer open-source, consider **Pinpoint** or **Glowroot** for Java, which also trace SQL timings.

**Database Health Dashboards:** In Grafana or Kibana, set up views for DB health: e.g. connections, query latency, slow query count, cache hit ratio, replication lag (if any). The Spring Boot metrics + DB-exporter metrics together can populate these. Also monitor **database-specific logs** for errors (e.g. deadlocks, out-of-memory, disk issues). If you route database logs to a central system (e.g. Filebeat -> Elasticsearch), you can create alerts on critical events (like “database out of connections” errors).

**Example:** For a PostgreSQL backing a Spring Boot app, you might install the PostgreSQL Prometheus Exporter. Grafana can then show: transactions committed per second, rows read/written per second, buffer cache hit %, and the longest query running currently. Alongside, the app’s own metrics show HTTP request rate and error rate. If a spike in errors correlates with many long-running queries on the DB, you’ve likely found the culprit. You could also use PG’s `pg_stat_statements` extension to identify the most time-consuming queries overall.

In summary, treat your database as a first-class part of observability: monitor its performance metrics, not just the app’s. Many production incidents stem from slow queries or DB overload, so catching those early (with metrics) and diagnosing them (with logs and analysis tools) is key.

# Observability in Message Queues

Microservices often communicate via asynchronous messaging (e.g. Kafka, RabbitMQ). Observing these message flows is as important as observing HTTP requests.

**Monitor Broker Metrics:** Both Kafka and RabbitMQ expose metrics that you should collect. **Kafka** is typically monitored via JMX. It provides metrics on throughput, consumer lag, queue sizes, etc. A common approach is to run the Prometheus JMX Exporter on Kafka brokers to scrape these metrics ([Kafka | Grafana Cloud documentation
](https://grafana.com/docs/grafana-cloud/monitor-applications/asserts/enable-prom-metrics-collection/messaging-frameworks/kafka/#:~:text=Configure%20Kafka%20exporter%20to%20generate,Prometheus%20metrics)). There are also Kafka-specific exporters or integrations (like Burrow for consumer lag monitoring). Key metrics to watch include: messages in/out per second, consumer lag (how far behind consumers are), broker disk usage, and any errors (failed fetch requests, etc.). **RabbitMQ** has a built-in Prometheus exporter plugin ([Monitoring with Prometheus and Grafana | RabbitMQ](https://www.rabbitmq.com/docs/prometheus#:~:text=RabbitMQ%20ships%20with%20built,Grafana%20support)) – you can enable `rabbitmq_prometheus` which exposes all internal metrics on a `/metrics` endpoint. These metrics cover queue lengths, message rates, resource usage, etc. Combined with Grafana, you can have dashboards showing each queue’s depth and consumers. RabbitMQ also has a management UI which gives real-time stats, but for long-term trending and alerting, Prometheus/Grafana is more powerful ([Monitoring with Prometheus and Grafana | RabbitMQ](https://www.rabbitmq.com/docs/prometheus#:~:text=This%20guide%20covers%20RabbitMQ%20monitoring,Grafana%2C%20a%20metrics%20visualisation%20system)) ([Monitoring with Prometheus and Grafana | RabbitMQ](https://www.rabbitmq.com/docs/prometheus#:~:text=RabbitMQ%20ships%20with%20built,Grafana%20support)).

**JMX and Exporters:** If not using Prometheus, other monitoring systems or even Actuator can help. Spring Boot Actuator has a `/metrics` endpoint that could expose some messaging metrics if Micrometer is configured with the right binder. (Micrometer has a Kafka binder that can record producer/consumer metrics, but using the broker’s metrics is more complete). For Kafka, you might use LinkedIn’s Kafka Monitor or Confluent’s Control Center in an enterprise setting. The main point is: treat your messaging system as you do a database – monitor its health (uptime of brokers), performance (latency of message delivery), and backlog.

**Tracing Messages:** Distributed tracing isn’t just for HTTP. When messages pass through queues, you want to trace them too. Spring Cloud Sleuth and Micrometer Tracing both instrument Spring’s messaging abstractions to propagate context ([Distributed Tracing with Spring Cloud Sleuth and Spring Cloud Zipkin](https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkin#:~:text=Spring%20Cloud%20Sleuth%20%28%60org.springframework.cloud%60%3A%60spring,automatically%20instruments%20common%20communication%20channels)). For example, if Service X produces a message to a Kafka topic, Sleuth will attach the current trace ID and span ID as headers (using formats like `X-B3-TraceId` or W3C `traceparent`). When Service Y consumes that message, it will extract those headers and continue the trace. In practice, this means in your trace UI (Zipkin/Jaeger) you’ll see a span for the “send” event in Service X and a span for the “receive/process” event in Service Y, linked by the trace ID. The time the message spent in the queue can even be inferred (the gap between send and receive). Tools like Zipkin have a **dependency diagram** that will show services and topics as nodes, indicating the messaging links. This is extremely helpful for debugging issues like “a message is produced but the consumer is not processing it for 5 minutes”. With tracing, you can see if the delay was in the queue or if the consumer was down.

**Messaging Metrics in Application:** Besides broker-level metrics, your application might also record metrics about messages. For instance, if using Spring Kafka, you can enable Micrometer metrics on the consumers – such as counts of messages consumed, commit latency, etc. Kafka’s client library has sensors that Micrometer can bind to. Ensure these are enabled (e.g. via `spring.kafka.consumer.metrics.enable=true` if applicable). This way, your service’s `/actuator/prometheus` might already include client-side metrics which complement broker metrics.

**Alerting on Queues:** Set up alerts for conditions like: queue length growing beyond a threshold (could indicate a stuck consumer), or consumer lag too high (consumer can’t keep up with producer). For RabbitMQ, alert if a queue goes unprocessed for too long or hits a high message count. For Kafka, alert on high lag or broker offline. Many of these can be done via PromQL in Prometheus (e.g. `rabbitmq_queue_messages_ready{queue="order_events"} > 1000` triggers an alert).

**Example:** Let’s say our Order Service publishes an event “OrderCreated” to RabbitMQ, and an Inventory Service consumes it. We use Sleuth – when Order Service publishes, it logs `Sending OrderCreated event [traceId=abc123]`. Inventory Service, when receiving, logs `Received OrderCreated event [traceId=abc123]`. In Zipkin, we see one trace with two spans: `OrderService send -> InventoryService receive`. Meanwhile, RabbitMQ metrics show queue length near zero (good, no backlog) and message rates matching on both producer and consumer. If Inventory Service were down, the queue length metric would start increasing and trigger an alert, and traces for new orders might show the send span but no corresponding receive span (which is another clue something’s wrong downstream). This illustrates how logs, metrics, and traces collectively give insight into asynchronous workflows.

# Tool Integrations and Dashboards

Now that the observability data (logs, metrics, traces) is being collected, the final piece is integrating tools to visualize and act on this data. We’ll set up an **observability stack** and configure dashboards and alerts for a comprehensive view of the system.

**Complete Observability Stack:** A typical open-source stack might include: **ELK (Elasticsearch, Logstash, Kibana)** for logs, **Prometheus & Grafana** for metrics, and **Zipkin or Jaeger** for traces ([Monitoring Spring Boot Microservices (Prometheus, Grafana & Zipkin) | by Mert ÇAKMAK | Dev Genius](https://blog.devgenius.io/monitoring-spring-boot-microservices-prometheus-grafana-zipkin-6430b767795e#:~:text=Hello%2C%20I%20will%20talk%20about,Grafana%2C%20and%20Zipkin%20for%20monitoring)) ([Monitoring Spring Boot Microservices (Prometheus, Grafana & Zipkin) | by Mert ÇAKMAK | Dev Genius](https://blog.devgenius.io/monitoring-spring-boot-microservices-prometheus-grafana-zipkin-6430b767795e#:~:text=Observability%20focuses%20on%20understanding%20the,monitoring%20and%20observability%20of%20systems)). There are newer combinations (e.g. the _Grafana LGTM stack_: Loki for logs, Grafana for UI, Tempo for traces, Mimir for metrics), but we’ll focus on the classic components:

- _Logs:_ As described, logs flow into Elasticsearch. Kibana is used to search and create dashboards of logs. You might create a Kibana dashboard showing the rate of ERROR logs over time, or a table of the most recent exceptions. Logging pipelines should include parsers to structure the logs (we did JSON logging to make this easy). Consider also using Kibana’s **Logs UI** or Elastic Observability features to detect anomalies in log patterns ([Inspect log anomalies | Elastic Observability [8.17]](https://www.elastic.co/guide/en/observability/current/inspect-log-anomalies.html#:~:text=You%20can%20use%20the%20Logs,where%20the%20log%20anomalies%20occur)). For example, Elastic can run an ML job on logs to flag unusual error message rates automatically (anomaly detection).

- _Metrics:_ Prometheus pulls numeric metrics and can trigger **alerts** via Alertmanager. Grafana connects to Prometheus to visualize metrics. A good approach is to create a **service dashboard** for each microservice and a **global dashboard** for the whole system. A service dashboard might show: request rate, error rate, latency percentiles, CPU and memory of that service’s container/VM, and any custom business metrics (like orders processed). A global dashboard might show high-level health: e.g. total requests across all services, error rates per service (possibly in a table), and external system metrics (DB, message broker) in one view. Grafana allows organizing dashboards with variables (so you can switch the service name to view a different service’s metrics using the same template). Use **Grafana’s alerting** to set up notification rules: for example, if error rate > 5% for 5 minutes or if p99 latency > 3s, fire an alert to Slack or email. This helps proactively catch issues. Grafana can also integrate exemplars (links from metrics to traces), so if using Tempo/Jaeger, you can click from a spike on a graph to a trace example – very useful for root cause analysis.

- _Tracing:_ Zipkin or Jaeger provides a UI to query traces. While Grafana doesn’t natively display traces (unless using Grafana Tempo), you can still link traces to Grafana or Kibana. For example, you might include the trace ID in log entries and use Kibana to jump from a log to the trace UI. Some teams integrate trace IDs into Grafana by using the logging data or using exemplars in Prometheus (if your tracing system supports it). The goal is to make it easy: if you see an error alert for Service X, you can find a recent trace with errors and inspect it. Zipkin’s dependency graph can be scheduled to show how services interact and highlight which edges have errors ([
      OpenZipkin · A distributed tracing system
  ](https://zipkin.io/#:~:text=Image%3A%20Trace%20view%20screenshot)).

**Dashboards Best Practices:** Build dashboards that are **focused and actionable**. Avoid the “wall of charts” that nobody can read – instead, pick key metrics (again, consider the Golden Signals: latency, traffic, errors, saturation ([Monitoring Distributed Systems - sre golden signals - Google SRE](https://sre.google/sre-book/monitoring-distributed-systems/#:~:text=SRE%20sre,facing%20system%2C%20focus%20on))). For each service, you might have a row of charts for latency (p95, p99), a row for traffic (requests/sec, perhaps split by endpoint or status code), a row for errors (5xx count, maybe broken down by exception type if you can get that), and a row for resource usage (JVM memory, CPU, thread pools). Use sparklines and single-stat panels for quick health checks (e.g. current memory usage 85% in red if above threshold). It’s also effective to include **annotations** on graphs for deploy events (you can mark when a new version was deployed, to see if it caused latency to jump).

For logs, create saved Kibana searches for common issues (for instance, a filter like `level:ERROR AND service:payment-service` in the last 1 hour). This saves time during incidents. Kibana also allows creating **visualizations** of log data – e.g. a bar chart of error count by service, or a term cloud of most frequent exception messages.

For distributed tracing, encourage developers to use the trace UI when investigating inter-service latency. Jaeger, for example, lets you search by operation name and min/max duration – you can find all traces of “checkout” that took longer than 5s, which is gold when debugging slowness reported by users. Including relevant **metadata (tags)** in spans is useful: e.g. add `orderId` or user info as span tags (cautiously, not PII) so you can search traces by those.

**Configuring Alerts:** Observability isn’t complete without automation to **alert** on issues. Prometheus Alertmanager is commonly used for metrics-based alerts. Define alert rules based on your metrics, such as:

- `HighErrorRate`: if `rate(http_server_requests{status=~"5.."}[5m])` > 0.05 (5% error rate) for 5 minutes.
- `HighLatency`: if `histogram_quantile(0.95, sum(rate(http_server_requests_bucket[5m])) by (le)) > 2` seconds for 5 minutes.
- `InstanceDown`: if a particular service’s metrics endpoint stops responding (Prometheus has an `up{job="serviceX"} == 0` metric you can use).

Alertmanager can route alerts to email, PagerDuty, Slack, etc. Set appropriate **severities** (warning vs critical) to not wake up people for minor issues. Also configure **alert aggregation** – e.g. if hundreds of instances fail, you don’t want 100 separate alerts. Alertmanager can group them (like “20 instances of service Y are down” in one message).

For log-based alerts, tools like Elastic’s Watcher or OpenSearch Alerting can trigger on certain log patterns (e.g. a specific exception appears X times in Y minutes). This is useful for catching errors that metrics might not directly capture. Another approach is to export certain log counts to metrics (e.g. use a logging appender to count “ERROR” messages and expose that as a gauge).

**Anomaly Detection:** As systems scale, static thresholds may be less effective (what’s “normal” can vary by time of day, etc.). This is where anomaly detection comes in. Some observability platforms (Elastic, Datadog, Dynatrace, etc.) provide machine-learning-based detection that learns your system’s normal patterns and alerts on deviations. For instance, Elastic can detect if the error rate at midnight Tuesday is abnormally high compared to typical Tuesday midnight values ([Inspect log anomalies | Elastic Observability [8.17]](https://www.elastic.co/guide/en/observability/current/inspect-log-anomalies.html#:~:text=You%20can%20use%20the%20Logs,where%20the%20log%20anomalies%20occur)). Or it can analyze log messages to spot new, unseen error types. These advanced features can significantly reduce noise and catch issues that simple thresholds might miss ([Observability 101: Understanding Logs, Metrics, Events, and Traces – The Pillars of Observability](https://www.observo.ai/post/understanding-logs-metrics-events-traces#:~:text=additional%20context%20and%20metadata%2C%20making,issues%2C%20whether%20they%20are%20performance)). In an advanced setup, you might run an Elastic ML job on latency metrics or use Datadog’s anomaly monitors on your custom metrics ([Anomaly Monitor - Datadog Docs](https://docs.datadoghq.com/monitors/types/anomaly/#:~:text=Anomaly%20Monitor%20,it%20has%20in%20the%20past)). Always review such alerts to ensure they’re not overly sensitive.

**Integrating Everything:** Aim for a **single-pane-of-glass** view where possible. Grafana can pull data from Prometheus (metrics), Loki/Elasticsearch (logs), and Tempo/Jaeger (traces) all into one dashboard. For example, Grafana’s newer unified approach (when using their stack) lets you see logs and metrics side by side for a given trace. If you’re using disparate tools (ELK + Prom + Zipkin), you won’t have one UI for all, but you can link them: e.g. display the trace ID in Grafana and hyperlink it to Zipkin’s URL; or from a Kibana log entry use an “Action” to open Zipkin. Even without tight integration, having these tools accessible and correlating via IDs/time is key.

Finally, document these dashboards and train the team to use them. During an incident is not the time to be figuring out how to navigate Kibana or which dashboard shows what – make sure on-call folks know where to look. Treat your observability setup as part of the product: iterate on it, add new metrics when new features come in, and prune things that aren’t useful.

# Best Practices for Observability

To wrap up, here are some best practices and considerations to ensure your observability implementation remains effective and sustainable:

- **Optimize Log Storage:** Logs can easily become the largest data source in observability. Implement log **retention policies** to cap how long logs are kept (e.g. keep 7 days of hot logs, archive older logs to cheaper storage). Use index lifecycle management in Elasticsearch or log rotation on disk to prevent unlimited growth. Compress archived logs to save space. Additionally, consider log sampling for extremely high-volume, low-value logs (though generally you want to log sparingly rather than sample). By controlling retention and volume, you ensure logging remains an asset, not a burden.

- **Log Quality > Quantity:** Encourage structured, meaningful log messages. Each log should aid troubleshooting. Avoid huge stack traces or dumping large objects in logs repeatedly. It’s often better to log one concise error with relevant context than flood the logs with debug lines. Too many logs can actually obscure the root cause. Use **log levels** appropriately – e.g., don’t log routine events as ERROR. Also ensure logs don’t contain sensitive info: scrub personal data or secrets (GDPR and security compliance). If using ELK, consider enabling security features to restrict who can view certain logs.

- **Secure the Observability Pipeline:** The data being collected (logs, traces, metrics) can contain sensitive information and also can be a target for attackers (or simply something you don’t want exposed publicly). Secure all channels: enable SSL/TLS for log shipping and metric scraping so data isn’t sent in plaintext ([Elastic (ELK) Stack Security | Elastic](https://www.elastic.co/elastic-stack/security#:~:text=Protect%20data%20%E2%80%94%20credit%20card,within%20the%20cluster%20and%20clients)). For example, use TLS encryption in Beats -> Logstash, and in Prometheus scraping (or run Prometheus inside a secure network). Protect the web UIs: Kibana, Grafana, Zipkin/Jaeger – all should be behind authentication (at least basic auth or SSO). Use built-in security features (Elastic Stack Security can integrate with LDAP/AD for Kibana auth, Grafana has user roles). Also consider network segmentation – e.g. don’t expose your Prometheus or Zipkin endpoints to the public internet. **Role-based access control** is important: you might allow developers to see metrics and their service’s logs, but maybe restrict access to production logs with sensitive data to a smaller ops team. Auditing access to observability tools is a good practice in regulated environments.

- **Pipeline Reliability:** Your observability tools themselves should be monitored. It’s ironic if the monitoring system goes down unnoticed. Set up alerts on your observability stack – e.g., Alert if Prometheus hasn’t scraped data in X minutes (could mean the app or Prometheus is down), alert if your Elasticsearch cluster is yellow/red (node issues), alert if the disk storing metrics is almost full, etc. Treat these systems like production critical systems as well.

- **Automate Alerting and Incident Response:** Use your metrics and logs to automate detection of issues. We covered alerts extensively above; the best practice is to tie them into an on-call rotation so critical alerts get a human response. However, try to **reduce false positives** – an alert that pages someone at 3am should correspond to a real issue that needs immediate attention. Fine-tune thresholds and use longer evaluation periods or multiple conditions to avoid flapping. Group related alerts to reduce noise (Alertmanager’s grouping can combine dozens of node-down alerts into one summary, for example). Also configure **anomaly detection** for the tricky cases – many modern tools can automatically baseline and detect anomalies in telemetry ([Observability 101: Understanding Logs, Metrics, Events, and Traces – The Pillars of Observability](https://www.observo.ai/post/understanding-logs-metrics-events-traces#:~:text=additional%20context%20and%20metadata%2C%20making,issues%2C%20whether%20they%20are%20performance)). This can catch things like memory leaks (a slow trend up) that static thresholds might miss.

- **Continuous Improvement:** Regularly review your observability setup. After each incident, ask: Did our logs/metrics/traces help us solve it? If not, what was missing? Maybe you needed a metric for a thread pool queue length that wasn’t there – add it. Or maybe the logs were too noisy – adjust logging levels. Observability is not “set and forget”; it evolves with the application. As new features roll out, instrument them with new spans or metrics. Remove metrics that aren’t useful to reduce overhead. Keep an eye on overhead: observability has a cost (CPU, I/O). For example, super-detailed tracing of every single request might add latency; find the right sampling rate that balances insight vs. performance.

- **Share Insights:** Make dashboards accessible and understandable. Use clear labels and units on Grafana graphs. Write runbooks for common alerts (“If alert X fires, check dashboard Y for Z metric, then likely cause is ...”). This helps on-call engineers respond faster. In an advanced setup, you can even automate some responses (like auto-scaling when certain metrics spike, or triggering a script to clear a queue backlog) – but do this carefully and always have human oversight.

By following these best practices, you’ll maintain an observability system that is robust, secure, and actually helpful day-to-day. An observable system is one where when something goes wrong, you **immediately know** there is an issue (alerts) and you can **quickly find** the cause (logs/metrics/traces). Spring Boot provides an excellent foundation for this, and with the right configuration and tool integrations, you’ll achieve deep visibility into your microservices in production.
