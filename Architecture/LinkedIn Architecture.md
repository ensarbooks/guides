# LinkedIn Architecture: A Comprehensive Software Architect’s Guide

## Introduction and System Scale

LinkedIn operates one of the world’s largest online professional networks, with a user base that has grown to hundreds of millions of members globally. Designing an architecture for a platform of this magnitude requires meeting extreme scale and performance requirements. Every day, LinkedIn’s systems handle **billions of events and requests**, demanding high availability, low latency, and fault tolerance. For perspective, LinkedIn’s event streaming infrastructure (built around Apache Kafka) processes on the order of **trillions of messages per day** ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=LinkedIn%E2%80%99s%20scale,surpassed%207%20trillion%20per%20day)). In fact, by 2019 LinkedIn reported handling over **7 trillion Kafka messages per day** across **100+ clusters and 4,000+ brokers** ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=LinkedIn%E2%80%99s%20scale,surpassed%207%20trillion%20per%20day)). Likewise, the site’s API layer sees on the order of **10^11 service calls per day** (e.g. over **100 billion REST calls daily** by 2015) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Today%2C%20LinkedIn%20has%20over%20975,day%20across%20all%20our%20datacenters)) – a number that has only grown since. These numbers illustrate the sheer volume of data flowing through LinkedIn’s architecture and the need for careful design to maintain performance at scale.

LinkedIn’s growth has driven a continual evolution of its architecture. From a single-server monolithic application in the early days to a complex distributed system today, LinkedIn’s architecture has been re-architected multiple times to address scaling pain points. Early on, the site (“**Leo**,” LinkedIn’s original monolith) ran all features on one application and database. As traffic climbed and features expanded, this monolith was split into an ecosystem of microservices to improve reliability and scalability ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Image)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%20built%20frontend%20servers%20to,we%20have%20over%20750%20services)). By 2010, LinkedIn had ~150 services; by 2015, over **750 distinct services** ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%20built%20frontend%20servers%20to,we%20have%20over%20750%20services)). Today, LinkedIn is powered by **thousands of microservices** owned by different teams ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=LinkedIn%20is%20powered%20by%20thousands,to%20our%20members%20and%20customers)), distributed across **multiple data centers worldwide** ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Being%20a%20global%20company%20with,a%20geographically%20close%20data%20center)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Multiple%20data%20centers%20are%20incredibly,additional%20PoPs%20around%20the%20globe)). Each data center hosts a full stack capable of serving user traffic, with **point-of-presence (PoP) edge servers** to route users to the nearest site for low latency ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Being%20a%20global%20company%20with,a%20geographically%20close%20data%20center)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Multiple%20data%20centers%20are%20incredibly,additional%20PoPs%20around%20the%20globe)).

LinkedIn’s performance requirements are equally stringent. Pages like the feed, profile, or job listings must load in milliseconds despite requiring aggregation of data from dozens of backend services. The architecture emphasizes horizontal scaling (adding more servers) over vertical scaling, allowing the system to handle spikes by distributing load. Services are designed to be **stateless** where possible so they can be cloned behind load balancers for elasticity ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Being%20stateless%2C%20scaling%20could%20be,it%20could%20take%2C%20and%20built)). Key data is partitioned and replicated to handle read/write load – for example, the **social graph** (connections network) is sharded and kept in memory for fast traversal ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Member%20Graph)), and the primary member profile database was transitioned from a single master into **partitioned databases** to handle write scaling ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=An%20easy%20fix%20we%20did,versus%20the%20main%20master%20DB)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=%2A%20While%20the%20master,since%20moved%20to%20partitioned%20DBs)). Caching layers and pre-computed data also play a crucial role in meeting performance goals (reducing expensive computations on the fly), though LinkedIn has refined its caching strategy over time to balance speed with consistency ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=LinkedIn%20was%20seeing%20hypergrowth%20and,with%20precomputed%20results%20when%20appropriate)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Over%20time%2C%20we%20actually%20removed,and%20reduces%20the%20cognitive%20load)).

In summary, LinkedIn’s architecture is engineered to meet massive scale: **hundreds of millions of users, tens of billions of daily page views and API calls, and data streams in the trillions**. The following sections provide a deep dive into how LinkedIn’s systems are structured – covering the high-level architecture, web and mobile frontends, the microservice backend and data layers, real-time streaming and offline processing, machine learning infrastructure, the social graph, deployment and observability tooling, security and compliance, experimentation platform, public APIs, and the open-source technologies that underpin many of these components. Real-world scenarios, design diagrams, and the evolution of LinkedIn’s architecture over time are included to illustrate how LinkedIn meets its scaling and performance requirements.

## High-Level Architectural Overview

At a high level, LinkedIn’s platform follows a **distributed, multi-tier architecture** comprising presentation layers, application logic layers, and data storage layers. It has evolved from a monolithic design into a **service-oriented architecture** (SOA) and now a **microservices architecture** to support independent development and scaling of different product features. **Figure 1** provides a conceptual view of LinkedIn’s multi-tier architecture, including the separation of frontend web applications, mid-tier services, backend data services, and various data systems:

([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin)) _Figure 1: Simplified multi-tier service-oriented architecture at LinkedIn (circa mid-2010s). Frontend web apps (blue) handle HTTP requests from browsers/mobile apps and call mid-tier services (gray) which aggregate data from various backend data services (red). Data services interface with storage systems like databases, key-value stores, or Hadoop (blue cylinders), and Kafka (green) enables asynchronous messaging between components ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%20built%20frontend%20servers%20to,we%20have%20over%20750%20services)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Kafka))._

**Monolith to Microservices:** In LinkedIn’s early years, a single J2EE monolith (“Leo”) handled all functions – serving web pages, implementing business logic, and interfacing with the database ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=LinkedIn%20started%20as%20many%20sites,a%20handful%20of%20LinkedIn%20databases)). This simplicity became a liability as usage grew: the monolith was a single point of failure and difficult to scale or update without affecting everything ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=As%20the%20site%20began%20to,small%20functional%20and%20stateless%20services)). The first step in LinkedIn’s architectural evolution was to break out critical components into separate services. One of the earliest extractions was the **member graph service** (“Cloud”), which managed the connections network in-memory for fast graph queries, communicating back to Leo via RPC ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Member%20Graph)). Search was the next service (powered by Lucene) to be split out ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=It%20was%20around%20this%20time,new%20search%20service%20running%20Lucene)). Over time, the “kill Leo” initiative systematically peeled off domains (profile, invitations, groups, messaging, etc.) into their own services ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Image)). By adopting a **service-oriented architecture**, LinkedIn gained isolation between domains and the ability to scale or troubleshoot parts of the site independently ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Image)).

In the SOA, **vertical feature stacks** emerged – for example, a set of services for the profile domain, another set for the feed/news, another for search, and so on ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Engineering%20started%20to%20extract%20micro,emerged%20for%20each%20functional%20area)). Each feature area would have a **frontend service** (handling web requests and UI rendering for that feature), one or more **mid-tier services** providing business logic or API access to data, and dedicated **backend data services** abstracting the persistence layer (databases or caches) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Engineering%20started%20to%20extract%20micro,emerged%20for%20each%20functional%20area)). This layering allowed, say, the profile page to aggregate data via a profile service, a social graph service, a recommendations service, etc., rather than a single gigantic app doing everything. By 2015 LinkedIn reported having over **750 services** running in production ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%20built%20frontend%20servers%20to,we%20have%20over%20750%20services)), and today it operates in the **thousands** ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=LinkedIn%20is%20powered%20by%20thousands,to%20our%20members%20and%20customers)), illustrating the depth of decomposition.

**Stateless Scaling and Load Balancing:** A key principle in the high-level design is that most services are **stateless** and thus easily horizontally scalable ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Being%20stateless%2C%20scaling%20could%20be,it%20could%20take%2C%20and%20built)). Any service that handles requests can be replicated across many machines – behind hardware or software load balancers – to increase throughput. Because no instance holds unique state, instances can be added or removed based on demand, and failures can be routed around. LinkedIn developed an automated service discovery and load balancing mechanism called **D2 (Dynamic Discovery)** which works with their REST frameworks (discussed later) to allow clients to find service instances without manual configuration ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)). This provides client-side load balancing and failover – the client will automatically discover new service instances and distribute calls, improving scalability and resiliency ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)).

**Data Partitioning and Replication:** At the data tier, LinkedIn employs both replication and partitioning to scale. Originally, the primary member profile database became a bottleneck, so read traffic was offloaded to **replica databases** kept in sync via a change capture pipeline (an early version of Databus) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=An%20easy%20fix%20we%20did,versus%20the%20main%20master%20DB)). This master-slave replication improved read throughput ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=An%20easy%20fix%20we%20did,versus%20the%20main%20master%20DB)). Eventually, LinkedIn moved beyond a single master DB by **sharding (partitioning) data** across multiple databases ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=%2A%20While%20the%20master,since%20moved%20to%20partitioned%20DBs)). For example, instead of one huge user table, users might be split by some key range across many DB instances. Partitioning spreads write load and data volume, while replication (often **multi-master or master-master replication** in modern systems like **Espresso**) provides high availability across data centers ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Many%20of%20our%20databases%20run,much%20of%20the%20difficult%20replication)). LinkedIn’s in-house distributed data store Espresso was built with multi-master, multi-datacenter support to simplify this at the storage layer ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Many%20of%20our%20databases%20run,much%20of%20the%20difficult%20replication)).

**Geographic Distribution:** LinkedIn serves a global user base, so it operates multiple active data centers in different regions (e.g. historically three major data centers in the US, with additional PoPs worldwide) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Multiple%20data%20centers%20are%20incredibly,additional%20PoPs%20around%20the%20globe)). The architecture supports **geo-distributed active-active traffic**: users are “pinned” to the nearest data center for their session to reduce latency ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Being%20a%20global%20company%20with,a%20geographically%20close%20data%20center)). Data is replicated between data centers so that if one site goes down, others can seamlessly take over. By 2015, LinkedIn had the core site running out of **three main data centers with global Points of Presence (edge proxies)** to route traffic ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Multiple%20data%20centers%20are%20incredibly,additional%20PoPs%20around%20the%20globe)). The edge layer uses DNS and load balancers to direct user requests to the optimal data center, and **caching proxies (CDNs and Traffic Servers)** at the edge help serve static content or even cache dynamic content for anonymized requests ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Beyond%20the%20application%20code%2C%20we%E2%80%99ve,server%20side%20rendering%2C%20and%20more)).

**Super Blocks – Managing Service Fan-Out:** As the number of microservices grew, an issue arose: handling a single user request (like loading the LinkedIn homepage) could involve calling dozens of downstream services – a complex **call graph** or fan-out pattern ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Service%20oriented%20architectures%20work%20well,getting%20more%20and%20more%20unruly)). Large fan-outs increase latency and failure risk. LinkedIn’s solution was to introduce **“Super Blocks”**, which are essentially aggregation layers that group related backend services behind a single logical API ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=getting%20more%20and%20more%20unruly)). For example, a “Profile Super Block” might wrap the profile data service, social graph service, and others, exposing a unified endpoint that returns all data needed for a profile view. The Super Block team can internally optimize how those calls are made. This reduces the number of calls the frontend must make and encapsulates complexity ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=getting%20more%20and%20more%20unruly)). It’s analogous to the **Backend-for-Frontend (BFF)** pattern or API gateway concept: simplifying client-service interactions by providing a tailored facade for broad domains.

**Caching Strategy:** To meet performance targets, LinkedIn uses caching at multiple layers, but with careful design. Early on, many mid-tier services introduced caches (like memcached or Couchbase) to store frequently used results in memory ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=LinkedIn%20was%20seeing%20hypergrowth%20and,with%20precomputed%20results%20when%20appropriate)). LinkedIn also employed **distributed key-value stores (e.g. Project Voldemort)** to cache **precomputed results** for expensive queries ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=LinkedIn%20was%20seeing%20hypergrowth%20and,with%20precomputed%20results%20when%20appropriate)). For example, an expensive calculation (such as “people you may know” suggestions or aggregated analytics) might be computed offline and stored in a fast key-value store, so serving it is just a quick lookup. However, mid-tier caches that store derived data from multiple sources became hard to keep consistent and added complexity in invalidation ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Over%20time%2C%20we%20actually%20removed,and%20reduces%20the%20cognitive%20load)). Over time, LinkedIn moved toward a philosophy of keeping caches **closer to the source of truth** and simplifying the caching layers ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Over%20time%2C%20we%20actually%20removed,and%20reduces%20the%20cognitive%20load)). Many intermediate caches were removed in favor of caching at the data-service level (where data is more easily invalidated or updated) and leveraging client-side and CDN caches for presentation-layer assets ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20rethought%20our%20frontend%20approach%2C,blocking%20asynchronous%20one)). Today, caching is implemented in places where it yields clear benefits – e.g., CDN and browser caching of static content and templates, in-memory caches in databases or storage systems, and application-level caches for specific high-read workloads – but LinkedIn avoids overly complex multi-tier caching that can stale data ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Over%20time%2C%20we%20actually%20removed,and%20reduces%20the%20cognitive%20load)).

In summary, the high-level architecture can be characterized by **distributed microservices**, each responsible for a specific business capability, communicating via well-defined APIs, and relying on a variety of distributed data stores. The platform is **horizontally scalable and geo-distributed**, using replication and sharding to handle data volume and throughput. Layers of the architecture (from web frontends to mid-tier aggregators to backend data services) provide separation of concerns. This overview sets the stage; next, we’ll delve into each major aspect of the architecture in detail, starting with how LinkedIn’s frontend and client applications interface with this complex backend.

## Frontend and Mobile Architecture

LinkedIn’s frontend architecture encompasses the web application that renders LinkedIn in browsers and the native mobile apps (iOS and Android) that most members use on phones. The frontend must provide a rich, dynamic user experience while efficiently communicating with the backend. Over the years, LinkedIn’s frontend has transitioned from server-driven page rendering to a mix of server and client rendering, and it has adopted modern techniques like client-side templates, single-page application frameworks, and GraphQL for data fetching.

**Web Application (Desktop and Mobile Web):** In the past, LinkedIn’s web interface was rendered mostly on the server using JSPs/Servlets in the frontend services. Around mid-2010s, LinkedIn began **“rethinking our frontend approach”** to deliver more interactive, app-like experiences on the web ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20rethought%20our%20frontend%20approach%2C,blocking%20asynchronous%20one)). They introduced **client-side templating** and **AJAX** techniques to the stack. For example, instead of always sending fully rendered HTML, the server could send JSON data and have the browser assemble the view using a JavaScript template. LinkedIn open-sourced **Dust.js**, a templating engine, as part of this push toward client-side rendering. By sending JSON and letting the browser do more work, LinkedIn could cache templates on the CDN and browser, reuse them, and reduce server CPU usage ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20rethought%20our%20frontend%20approach%2C,blocking%20asynchronous%20one)). Specific pages like the Profile and University pages were early adopters of this “SPA-like” approach ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20rethought%20our%20frontend%20approach%2C,blocking%20asynchronous%20one)).

LinkedIn also adopted techniques like **Facebook’s BigPipe** for progressive rendering of pages ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=pages%20%29,blocking%20asynchronous%20one)). BigPipe allows a page to be sent in segments so that the browser can begin showing parts of the UI while other parts are still being generated or fetched. This improves perceived performance. Along with BigPipe, LinkedIn experimented with the **Play Framework** (a reactive, non-blocking web framework) to serve certain pages asynchronously ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=browser,blocking%20asynchronous%20one)). The move from a traditional threaded model to a **non-blocking asynchronous model** on the web tier helped the servers handle more concurrent requests efficiently ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=browser,blocking%20asynchronous%20one)).

In modern times, LinkedIn’s web frontend has converged with single-page application (SPA) principles. Circa 2015, LinkedIn undertook “Project Voyager” – a complete rewrite of the LinkedIn mobile app and web experience. They embraced an architecture where the client (browser or app) could fetch data via APIs and render UI components dynamically. **Ember.js**, a JavaScript MVC framework, was used to build parts of LinkedIn’s web app (notably, the feed) as an SPA around that time. This made interactions smoother (no full page reloads for each action). However, LinkedIn had to carefully balance SEO and initial load time, so server-side rendering was still used for some content (especially for first loads or for search engine crawlers).

**Native Mobile Apps:** LinkedIn’s mobile apps are native (Objective-C/Swift on iOS, Java/Kotlin on Android) to provide the best performance and utilize device capabilities. Earlier, LinkedIn had tried using HTML5 within a container for their mobile app (to reuse web code), but this was famously abandoned in 2013 in favor of true native apps due to performance issues. The current native apps interact with LinkedIn’s backend via the same REST/GraphQL APIs that the web client uses, ensuring consistency. The mobile architecture emphasizes **modularity** – different teams own different feature modules in the app (messaging, feed, profile, etc.), which can be developed somewhat independently but integrate into a single app shell. Features are often delivered via remote configuration and feature flags (tied into LinkedIn’s experimentation platform, discussed later) so that new UI elements can be enabled or disabled without requiring app store updates.

One notable aspect of LinkedIn’s mobile architecture is how the app fetches and renders data. Instead of making many small network calls for each piece of data, the mobile app can make **batched requests** to fetch all needed data in one round trip. Historically, LinkedIn’s internal API framework (Rest.li) allowed the concept of batch queries or “multiplexed” calls. More recently, **GraphQL** has been adopted for certain use cases, especially in external-facing APIs and possibly within the mobile app, to allow clients to request exactly the data they need in one request. We’ll cover GraphQL more in a dedicated section, but from a mobile perspective, GraphQL can significantly reduce chattiness by replacing multiple REST calls with a single query.

**LinkedIn’s Use of Proxies and Edge Caching:** The frontend architecture is also supported by a layer of edge proxies and CDNs. LinkedIn uses **Apache Traffic Server** and **HAProxy** in front of its web servers ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Beyond%20the%20application%20code%2C%20we%E2%80%99ve,server%20side%20rendering%2C%20and%20more)). These proxies handle tasks like SSL termination, request routing, and static content caching. They also implement **geographic traffic routing and data center pinning** – ensuring a user’s session stays with one data center for consistency ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Beyond%20the%20application%20code%2C%20we%E2%80%99ve,server%20side%20rendering%2C%20and%20more)). Static assets (images, JS, CSS, and even pre-built templates) are served through a content delivery network. LinkedIn benefits from parent company Microsoft’s CDN infrastructure as well. By offloading static content and using aggressive caching (with cache busting on deploys), the web servers mainly handle API responses and page skeletons, which improves scalability.

**Frontend-Backend Interaction:** The contract between frontend and backend at LinkedIn has evolved into an API-driven model. Initially, the frontend servers directly invoked backend logic or databases. With SOA, they started calling mid-tier services over REST. LinkedIn created an internal REST framework called **Rest.li** to standardize these calls (more on Rest.li in the API section). Rest.li allowed the web layer to call multiple services and aggregate responses to form a page. In modern practice, the LinkedIn web client (whether server-side or as SPA) communicates with backend **API Gateways**. For example, the LinkedIn web app might call a “Feed API” endpoint that is served by a frontend service which in turn calls various feed backend services. There is also a push toward using **GraphQL** for integration with external partners and possibly internal clients because of its flexibility ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)). In October 2022, LinkedIn shared that adopting GraphQL sped up their API development by **90%** for certain integration use cases ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=Every%20customer%20looking%20to%20integrate,ROI%20use%20cases%20could%20overcome)). The GraphQL layer can aggregate data from many microservices and present it as one unified graph to the client, which aligns with the “super block” concept of grouping services behind one API.

**Performance on the Frontend:** LinkedIn leverages several strategies to keep the frontend fast and responsive:

- **CDN Caching and Asynchronous Loading:** Assets and template bundles are cached on Akamai/Azure CDN nodes globally. BigPipe ensures above-the-fold content loads first, and additional modules load async.
- **Mobile network optimizations:** On mobile, the app uses efficient binary protocols when possible (thrift over HTTP or gRPC) and retries requests quickly on failures. They likely use techniques like request prioritization (for example, load essential data first).
- **Responsive Design:** LinkedIn’s web frontend uses responsive design for various screen sizes, and the mobile app shares design elements for consistency.
- **Tracking and Measuring:** LinkedIn is known to instrument its frontend heavily. Real User Monitoring (RUM) tracks page load and interaction times in production, feeding into their monitoring systems. If a new UI feature regresses performance, it can be caught via A/B testing metrics or RUM alerts.

In conclusion, LinkedIn’s frontend architecture has evolved into a **modern, API-driven, hybrid rendering system**. Server-side rendering is used for initial loads and SEO, while client-side rendering provides interactivity and speed for subsequent actions. The mobile apps are first-class clients, calling the same backend services via optimized endpoints. Technologies like client-side templates, BigPipe, and GraphQL help minimize latency and network round trips. By the time data reaches the user’s screen (web or mobile), it has typically passed through multiple layers of caching and aggregation to ensure the view can be rendered with minimal further calls. Next, we examine the backend microservices that produce and serve this data.

## Backend Microservices Design and Orchestration

LinkedIn’s backend is composed of numerous microservices, each responsible for a specific domain or functionality. These services collectively power features like the feed, profiles, messaging, search, recommendations, etc. The design of LinkedIn’s microservices and how they are orchestrated is central to the platform’s scalability and agility.

**Domain-Oriented Microservices:** As described earlier, LinkedIn decomposed its monolith into services aligned with business domains (Profile, Search, Messaging, Feed, Recruiter, Ads, etc.) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Engineering%20started%20to%20extract%20micro,emerged%20for%20each%20functional%20area)). Each service owns the logic and data for its domain. For example:

- **Profile Service:** Manages member profile data (experience, education, etc.) and likely handles profile edits and queries.
- **Network/Graph Service:** Manages connections between members (the social graph).
- **Feed Service:** Aggregates and ranks feed updates (we’ll discuss the feed in detail later).
- **Messaging Service:** Handles LinkedIn’s messaging (InMail) functionality.
- **Search Service (Galene):** Powers search queries for people, jobs, companies, etc.
- **Recommendation Services:** Various services for “People You May Know”, job recommendations, etc.
- **Enterprise Product Services:** Such as for Sales Navigator, Recruiter, Learning – these might have their own microservice stacks separate from the consumer LinkedIn stack, but with some shared data.

These services interact via APIs and message streams, not via direct database calls to each other’s schema. This **low coupling** means teams can develop and deploy services independently as long as they honor the API contracts.

**Interface Layer (Rest.li):** LinkedIn created an internal framework called **Rest.li** to standardize how services expose APIs. Rest.li defines resource-oriented REST endpoints with a consistent modeling and client generation approach ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=When%20we%20transformed%20from%20Leo,Restful%20API%20model%20%2028)). By using JSON over HTTP and a unified data schema, Rest.li allowed LinkedIn to have a cohesive API surface across hundreds of services ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=When%20we%20transformed%20from%20Leo,Restful%20API%20model%20%2028)). One major advantage was enabling non-Java clients to easily call services (previously, some services used Java RMI which tied them to Java-only) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=this%2C%20we%20built%20out%20a,Restful%20API%20model%20%2028)). Rest.li also integrated with **D2 (Dynamic Discovery)** – each service registers with a service registry (backed by Apache ZooKeeper), and clients use D2 to lookup available instances and perform client-side load balancing ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)). This means if Service A wants to call Service B, it asks D2 for Service B’s URL, and D2 returns one of B’s instances (taking into account health and location). This automated much of the orchestration of calls and improved resilience (if a service instance goes down, clients automatically drop it on next discovery) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)).

As of mid-2010s, LinkedIn had nearly **1000 Rest.li resources** (API endpoints) and was seeing **100+ billion Rest.li calls per day** internally ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Today%2C%20LinkedIn%20has%20over%20975,day%20across%20all%20our%20datacenters)), illustrating how pervasively this architecture was used. Each microservice typically has its own Rest.li-defined interface.

**Orchestration of Microservice Calls:** When a user triggers an action (say loading the homepage), the frontend or API gateway service will orchestrate calls to many backend services. This orchestration is done through code (the service aggregator will call the necessary downstream services, possibly in parallel or sequence as needed). LinkedIn uses both **synchronous REST calls** for request/response and **asynchronous messaging** (via Kafka) for event-driven updates (discussed in the next section).

To manage complex call graphs, LinkedIn relies on both the previously mentioned **Super Block** concept (aggregating some calls behind one service) and careful system design. There isn’t an external orchestrator (like a workflow engine) for online requests; instead, each service’s code orchestrates its downstream interactions. However, for long-running processes (like offline workflows) LinkedIn uses schedulers (e.g. Azkaban or Gobblin for data flows).

**Example:** A “View Profile” request from a client might hit a Profile API service. The Profile service code will then call the Connections service (to get mutual connections), the Recommendations service (to get “People also viewed”), the Content service (to get the member’s articles), etc. Each of those may in turn call further services or databases. The results are aggregated and sent back. This fan-out is handled programmatically. To ensure efficiency, some calls are made in parallel and the response times are closely monitored. LinkedIn built tooling to trace these distributed call graphs – e.g., an internal tool called **InTempo/inCapacity** was mentioned for tracing and profiling request flows in real time ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=A%20near,incoming%20ProfileView%20events)), helping engineers identify slow bottlenecks in the call chain.

**Service Communication:** Nearly all inter-service communication at LinkedIn uses REST over HTTP (with JSON or Avro as the payload format) via the Rest.li framework ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=When%20we%20transformed%20from%20Leo,Restful%20API%20model%20%2028)). Rest.li can also support batching multiple requests into one HTTP call to reduce overhead. In some cases, services might use **Apache Kafka** topics to broadcast events instead of direct calls (for example, when a member updates their profile, a Profile service might write an event to Kafka so that Search service and others can consume it asynchronously, rather than Profile calling each service synchronously). This mix of request/response for read operations and publish/subscribe for background propagation is a common pattern in LinkedIn’s architecture.

**Service Orchestration and Scaling:** LinkedIn deploys its microservices on a standardized infrastructure. Historically, services ran on LinkedIn’s own servers or VMs in their data centers. Over time, LinkedIn has moved toward containerization and cloud-like scheduling. While LinkedIn hasn’t publicly detailed a complete migration to Kubernetes, they have discussed using Apache YARN (traditionally for Hadoop/Samza jobs) for scaling some services and caches ([Auto-Scaling with Apache Helix and Apache YARN](https://engineering.linkedin.com/cluster-management/auto-scaling-apache-helix-and-apache-yarn#:~:text=Auto,value%20store)) ([Auto-Scaling with Apache Helix and Apache YARN](https://engineering.linkedin.com/cluster-management/auto-scaling-apache-helix-and-apache-yarn#:~:text=We%20will%20show%20how%20to,value%20store)), and Apache Helix (another LinkedIn open source project) for cluster management in certain systems. It’s likely that many stateless services are now run in a containerized environment for ease of deployment.

Continuous deployment (discussed later) implies an automation layer that can orchestrate rolling out new versions of these services across thousands of machines. LinkedIn’s internal PaaS might schedule services onto machines based on resource needs. A hint comes from LinkedIn’s data center networking article, which mentions the need to support “100,000 to 200,000 bare metal servers” in the data centers ([Project Altair: The Evolution of LinkedIn's Data Center Network](https://www.linkedin.com/blog/engineering/data-management/project-altair-the-evolution-of-linkedins-data-center-network#:~:text=Network%20www,servers%20without%20adding%20an)) – at that scale, automation is key.

**Configuration and Service Discovery:** All these microservices need to find each other and have configurations for different environments. LinkedIn likely uses ZooKeeper not just for D2 service discovery but also for configuration management (many companies use ZooKeeper or etcd for storing service configs). LinkedIn’s Nuage platform (mentioned in context of Kafka) provides self-service provisioning for data resources ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=Kafka%20REST%20is%20a%20HTTP,for%20administrative%20operations%20on%20topics)), and perhaps similar tooling exists for provisioning new services. They also have internal frameworks for feature flags and dynamic configuration (which tie into the experimentation platform, LiX).

**Resilience Patterns:** Given the number of service calls that can be involved in fulfilling a request, resilience techniques are critical. LinkedIn employs:

- **Timeouts and Retries:** Every service call has a sensible timeout to avoid hanging because one downstream is slow. If a call fails or times out, the calling service may retry (possibly to a different instance via D2) or degrade gracefully.
- **Bulkheads and Pooling:** Services likely use connection pools or limits per downstream to avoid one slow service consuming all threads.
- **Fallbacks:** In some cases, if a dependent service is unavailable, a service might have a cached copy or a default behavior. For instance, if the Recommendations service fails when loading a profile, the Profile service might still return the profile data but omit the recommendations (ensuring the page still loads, albeit with reduced content).
- **Circuit Breakers:** It’s common to implement circuit breakers (à la Hystrix) that trip if a downstream is failing repeatedly, to prevent hammering it and to fail fast.

LinkedIn’s monitoring (to be discussed) also gives rapid insight into service latencies and failures so issues in the microservice mesh can be resolved quickly.

**Security between Services:** Each microservice call at LinkedIn likely carries authentication information – LinkedIn must ensure that internal calls are authorized and that user data isn’t accessed improperly. While details aren’t public, LinkedIn could be using an internal token system or mutual TLS for service-to-service auth. Given compliance needs, every request likely has context of the user and application making the call, and services enforce permissions (e.g., you can’t fetch someone’s private data via an internal API without proper auth). We’ll discuss security more later.

In summary, LinkedIn’s backend is a **web of microservices** orchestrated through a combination of RESTful calls and event streams. The design emphasizes clear domain boundaries, stateless services for ease of scaling, automated discovery/load-balancing, and resilience in handling the complex call topology. By adopting frameworks like Rest.li and D2, LinkedIn created a uniform way for services to interact at scale ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)). This microservice foundation allows LinkedIn to innovate rapidly (deploying services independently) while handling the massive workload distributed across many machines.

## Data Storage: Distributed Databases, Sharding, and Replication

Data is the core of LinkedIn’s platform – profiles, connections, posts, messages, searches, etc. To support fast access and updates at scale, LinkedIn uses a variety of **specialized data storage systems**, each tuned for different use cases. The data layer includes traditional databases, distributed NoSQL stores, graph stores, and big data systems. Key themes are **partitioning (sharding)** data to scale writes, **replicating** data across nodes and data centers for availability, and using the right type of storage for the right workload (relational vs key-value vs search index, etc.).

Major categories of LinkedIn’s data stores include:

- **Relational Databases (RDBMS):** In early days, LinkedIn used relational databases (like Oracle or MySQL) for storing core member data. Even today, some data likely resides in relational form where strong consistency and complex querying are needed (e.g. transactional data or account info). However, many relational datasets were moved into or supplemented by distributed systems (Espresso, described below). LinkedIn’s approach to scaling relational data was to introduce read replicas and then sharded clusters ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=An%20easy%20fix%20we%20did,versus%20the%20main%20master%20DB)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=%2A%20While%20the%20master,since%20moved%20to%20partitioned%20DBs)). For example, member profiles might be spread across multiple databases by member ID range. Each shard handles a subset of users, allowing concurrent writes. Relational databases provide ACID guarantees which are useful for critical data, but sharding introduces complexity in queries that span shards (LinkedIn mitigates this by keeping queries shard-local when possible or using application logic to aggregate across shards).

- **Espresso (Distributed Document Store):** **Espresso** is LinkedIn’s primary online distributed database for many datasets. It was built in-house to be a multi-tenant, scalable **NoSQL document store** ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Many%20of%20our%20databases%20run,much%20of%20the%20difficult%20replication)). Espresso presents a schema (similar to a table) but distributes storage across many servers. Key features of Espresso:

  - **Sharding:** Data is partitioned by a primary key. Each shard is managed by a separate storage node. This allows horizontal scaling of writes/reads.
  - **Replication and Multi-Datacenter Master/Master:** Espresso supports active-active replication, meaning data can be updated in multiple data centers and synced ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Many%20of%20our%20databases%20run,much%20of%20the%20difficult%20replication)). This is critical for LinkedIn’s geo-distribution; it handles the tricky replication logic, including conflict resolution, under the hood.
  - **Consistency:** Espresso aims for a balance of consistency and availability. It likely uses quorum writes/reads or other strategies to keep data in sync. As a system handling user updates (profiles, connections, etc.), it must ensure a reasonable level of consistency.
  - **Secondary Indexes:** As a document store, Espresso probably provides some indexing beyond primary key to allow efficient lookups by certain fields (though for more complex queries, LinkedIn might offload to search systems or analytic systems).

  Espresso is mentioned as powering many of LinkedIn’s databases ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Many%20of%20our%20databases%20run,much%20of%20the%20difficult%20replication)). For instance, member profile data, group posts, or company pages could all be stored in Espresso as documents keyed by an ID. By open sourcing Espresso, LinkedIn contributed this system to the community. Espresso essentially replaced many direct MySQL/Oracle usages, giving LinkedIn more control and scalability in a cloud-native way.

- **Voldemort (Key-Value Store):** **Project Voldemort** was LinkedIn’s early open-source distributed key-value store (inspired by Amazon Dynamo). It was used to cache and serve **precomputed derived data** and other heavily accessed items ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=reduce%20the%20load%20altogether%20by,with%20precomputed%20results%20when%20appropriate)). For example, the People You May Know recommendations, which are computed offline, could be stored in Voldemort keyed by member ID for fast retrieval. Voldemort provides eventual consistency, partitioning, and replication. It does not support complex queries – it’s a simple get/put store. LinkedIn once relied on Voldemort to reduce database load by serving data that doesn’t need to be transactionally up-to-date ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=reduce%20the%20load%20altogether%20by,with%20precomputed%20results%20when%20appropriate)). Over time, however, Voldemort’s role has been largely supplanted by **Venice**, LinkedIn’s next-gen derived data store (discussed below), and by Espresso for cases requiring more functionality. In fact, Venice’s development led to gradually **replacing many Voldemort use cases** within LinkedIn ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=Venice%20entered%20production%20at%20the,Interested%20In%2C%20and%20many%20more)). Still, Voldemort was an important part of LinkedIn’s mid-2010s architecture, backing things like search index storage and read-heavy datasets- **Venice (Derived Data Store):** **Venice** is LinkedIn’s newer **derived data storage platform**, designed specifically for serving read-heavy, precomputed data at scale. Open-sourced in 2022, Venice now powers more than **1,800 datasets** at LinkedIn and is used by over **300 applications** ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=We%20are%20proud%20to%20announce,batch%20and%20stream%20processing%20jobs)). It was introduced around 2016 as a replacement for many Voldemort use cases ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=Venice%20entered%20production%20at%20the,Interested%20In%2C%20and%20many%20more)). Venice provides a distributed key-value store with **high-throughput asynchronous ingestion** from both batch and streaming sources ([Venice, Derived Data Platform for Planet-Scale Workloads. - GitHub](https://github.com/linkedin/venice#:~:text=GitHub%20github,batch%20and%20streaming%20sources)). Key features of Venice include:

  - **Batch and Stream Ingestion:** Venice can ingest data from Hadoop batch jobs or Samza/Flink streaming jobs, making it ideal for machine learning features or recommendation results that are computed offline and need to be served online.
  - **Optimized for Read (RO) Workloads:** It’s eventually consistent and optimized to serve data with low latency and high fan-out (e.g. many keys per query). For example, People You May Know suggestions, job recommendations, and other AI-driven results are produced in big data pipelines and stored in Venice for quick retrieval during page loads ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=Venice%20entered%20production%20at%20the,Interested%20In%2C%20and%20many%20more)).
  - **Active-Active and Multi-tenant:** Like Espresso, Venice supports multi-datacenter active-active deployment (important for global availability). It’s multi-tenant, meaning many teams can use Venice as a service for different datasets without standing up separate clusters ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=We%20are%20proud%20to%20announce,batch%20and%20stream%20processing%20jobs)).
    Venice’s impact: by 2018, LinkedIn had migrated roughly **500 Voldemort read-only use cases fully to Venice** ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=By%202018%2C%20the%20roughly%20500%C2%A0Voldemort,in%20replacement)), using Venice’s drop-in compatibility for bulk data pushes. This underscores how Venice became the de facto solution for **derived data** (i.e. data that originates from processing other data, such as ML features, aggregations, or denormalized views). The majority of LinkedIn’s AI use cases (e.g. recommendation candidate lists, model outputs) rely on Venice ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=Venice%20entered%20production%20at%20the,Interested%20In%2C%20and%20many%20more)). Venice emphasizes **throughput and scale over per-record transactionality** – it’s acceptable for its data to be slightly stale or eventually consistent, as it’s derived information, not primary user edits.

- **Search Index (Galene) and Analytics Stores (Pinot):** For powering search features and certain feed/content queries, LinkedIn uses specialized stores:

  - **Galene** is the codename for LinkedIn’s search infrastructure (successor to early systems like Lucene indices and Sensei). Galene is not a single database but a system that builds inverted indices of profiles, jobs, etc., and can answer search queries at scale. It likely uses offline map-reduce jobs to build index shards and online services to query them, similar to ElasticSearch or SolrCloud in nature. Galene was mentioned as replacing the older **Sensei** search infrastructure ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,the%20engineering%20organization%2C%20it%20was)). Under Galene, when a member updates their profile or a job is posted, events (via Kafka) update the search index so that it becomes searchable quickly. We can consider the search index as a denormalized store optimized for text queries and relevance ranking.
  - **Apache Pinot** is an open-source distributed **OLAP datastore** created at LinkedIn for real-time analytics queries. Pinot is designed for **low-latency aggregation queries** on large data streams. LinkedIn uses Pinot to power things like “Who Viewed Your Profile” analytics, ad analytics, and **site-wide monitoring metrics** ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)). Pinot ingests events (e.g. profile view events, ad impressions, or metric logs) in real-time (from Kafka) and makes them queryable immediately. It pre-aggregates and indexes data to answer queries with sub-second latency. For example, Who Viewed Your Profile might query Pinot for the count of profile views by company or role in the past week for a given user. Pinot can quickly slice-and-dice multi-dimensional data (e.g. by day, viewer’s industry, etc.) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)). It’s used extensively in LinkedIn’s internal analytics and dashboarding tools as well. ThirdEye (LinkedIn’s monitoring platform) builds on Pinot to allow interactive exploration of metrics and anomalies ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)).
  - **Sensei / Feed Index:** Historically, LinkedIn’s feed was powered by a search-like system called Sensei (with components Bobo, Norbert, Zoie) which indexed feed updates and allowed queries with ranking logic ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Sensei%2C%20the%20generic%20search%20system,infrastructure%2C%20provided%20the%20following%20features)) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=logic,A%2FB%20test%20different%20relevance%20algorithms)). Sensei was eventually replaced for feed generation (we’ll discuss the new FollowFeed system in the feed section), but it’s worth noting as a storage system, Sensei was essentially a real-time index of feed content built on Lucene. It stored every feed update as a document and allowed retrieval by member with filtering and scoring. The reason Sensei needed replacement was operational scaling – keeping the entire feed index in memory became costly as content grew ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,of%20smaller%20components%E2%80%94Bobo%2C%20Norbert%20and)). Its relevance was also coded in query language which became hard to maintain ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,the%20same%2C%20while%20leverage%20decreased)). The replacement (FollowFeed with RocksDB storage) is detailed later, but Sensei is an example of a specialized data store to meet a specific use case (feed updates).

- **Graph Storage:** LinkedIn’s **social graph** (the “connections” network and other entity-entity relationships) is massive – as of 2023, LinkedIn reported the graph contains **270+ billion edges** (connections between members and other entities) ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=By%20hosting%20LinkedIn%E2%80%99s%20Economic%20Graph%2C,people%20join%20and%20use%20LinkedIn)). Early on, LinkedIn stored the graph in memory in a service (“Cloud”) for fast traversal ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Member%20Graph)). This was essentially a graph cache. Over time, they needed a more robust graph database. LinkedIn built **LIquid**, an in-house graph database designed for **fast graph queries at scale**. LIquid implements a relational-style graph model with a declarative **Datalog query language**, but under the hood it uses specialized **index data structures in shared memory** for constant-time edge traversal ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Editor%E2%80%99s%20note%3A%20In%20this%20two,free%20shared)). LIquid can answer complex graph queries (e.g., “find path within 3 degrees between Member A and Member B” or “who are the mutual connections of A and C”) extremely fast. It replaced many hand-coded graph traversal implementations that various teams had built, providing a single service for graph data ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Introducing%20LIquid%2C%20LinkedIn%E2%80%99s%20in,database)).

  - **Scale of LIquid:** LIquid serves the **Economic Graph** – which includes members, companies, schools, jobs, skills, etc., and all the connections among them ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=By%20hosting%20LinkedIn%E2%80%99s%20Economic%20Graph%2C,people%20join%20and%20use%20LinkedIn)). It hosts ~15 TB of graph data in memory and handles about **2 million queries per second** at peak ([QCon San Francisco 2023 | LIquid: A Large-Scale Relational Graph Database](https://qconsf.com/presentation/oct2023/liquid-large-scale-relational-graph-database#:~:text=We%20describe%20LIquid,this%20in%20memory%20at%20scale)) ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=By%20hosting%20LinkedIn%E2%80%99s%20Economic%20Graph%2C,people%20join%20and%20use%20LinkedIn)). LinkedIn expects this to double in the near future as data grows ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=positions%2C%20jobs%2C%20events%2C%20groups%2C%20etc,people%20join%20and%20use%20LinkedIn)). LIquid maintains **270B+ edges**, enabling features like People You May Know suggestions, which require real-time graph computation (common connections, “triangle closing” in graph terms) ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=By%20hosting%20LinkedIn%E2%80%99s%20Economic%20Graph%2C,people%20join%20and%20use%20LinkedIn)). By using LIquid’s query language, developers can retrieve graph insights without having to manually traverse lower-level stores.
  - **Storage approach:** Underlying LIquid, data is stored in a distributed way. Part 2 of LinkedIn’s LIquid blog indicates they store graph data in a relational model across multiple servers, but with heavy use of indexes in memory for speed. They also mention **“wait-free shared-memory indexes”** which suggests they use concurrency-friendly structures for multi-threaded access ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=the%C2%A0economic%20graph,memory%20index%20structures)). LIquid likely uses persistent storage (possibly a RocksDB or Espresso table) to back the in-memory index for durability, but serves most queries out of memory.
  - **Graph traversal outside LIquid:** Some graph queries are also handled via offline computations; for example, LinkedIn might run graph analytics jobs (using frameworks like Spark or GraphX) on the entire network to compute global metrics or heavy precomputations (like connected components, influence scores, etc.). Those results might then be fed back into online systems via Venice or similar. But for real-time needs (like querying a member’s second-degree connections for PYMK), LIquid is the go-to solution.

- **Messaging and Social Feed Data:** LinkedIn’s private messages and feed content are also stored in specialized ways:

  - **Messaging:** Likely stored in a distributed NoSQL store (possibly Espresso or a separate system) with appropriate indexing for threads. Given the need for reliability (nobody wants lost messages), messaging data might also be backed up or stored with strong consistency. There was an older LinkedIn project called **Twissandra** (combining Twitter’s model on Cassandra) for a social messaging timeline; LinkedIn may have drawn inspiration from that or simply used a document store. It’s possible they use Espresso or even relational DBs for messaging, since message volume per user isn’t extremely high compared to, say, feed events.
  - **Feed and Activity Streams:** Every action on LinkedIn (a post, like, comment, share) generates an **activity event**. These events are stored in an **activity store** (to later render your own activity feed) and also fed into feed generation systems. The persistent storage of raw activities could be in Espresso or a dedicated activity database. Byron Ma’s 2014 blog on publishing mentions an **“Activities index” on Espresso** that stores published articles as updates, which Feed-Mixer then fetches ([Maximizing Our Publishing Platform Reach with Network Distribution](https://www.linkedin.com/blog/engineering/archive/maximizing-our-publishing-platform-reach-network-distribution#:~:text=Data%20flow%20from%20Publishing%20Service,Mixer)). So Espresso was used to store feed update records (with each update as a document). The current FollowFeed system stores per-user timelines in RocksDB (more on that soon). Additionally, LinkedIn keeps **social gestures** (likes, comments) likely in stores that can be quickly aggregated (these could be in Venice/Pinot for count aggregation or Espresso for transactional consistency on counts).

- **Big Data Storage (HDFS):** LinkedIn operates huge Hadoop clusters for offline data. Data from the site (logs, tracking events, database snapshots) is continuously ingested into **HDFS (Hadoop Distributed File System)** for batch processing ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)). They have a data pipeline (Gobblin, see below) to pull data from various online stores into HDFS. On HDFS, LinkedIn keeps historical data used for building machine learning models, computing recommendations, generating aggregate analytics (e.g. number of connections per region), etc. This offline data store is the source for many derived datasets that then get pushed to serving stores like Venice or Pinot. Hadoop (with MapReduce or more modern Spark jobs) crunches through massive datasets to produce results like “People You May Know candidates” or “Recommended jobs for user X” which are then loaded into online stores for quick serving ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)). HDFS also likely stores backup snapshots of databases for disaster recovery.

To facilitate moving data into these storage systems, LinkedIn developed **Gobblin**, an open-source data ingestion framework. Gobblin is used to “gobble in” data from many sources (databases, REST APIs, Kafka topics, etc.) into HDFS or other stores ([Gobblin' Big Data With Ease | LinkedIn Engineering](https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease#:~:text=Gobblin%27%20Big%20Data%20With%20Ease,datasets%20through%20a%20single%20framework)). It automates and scales the ETL (extract-transform-load) process, which is crucial for feeding the data warehouse and analytics systems.

**Data Governance and Compliance:** With so many data stores, LinkedIn has a comprehensive data management layer to enforce privacy and compliance. LinkedIn’s **Data Management Platform** monitors data across online and offline stores and attaches metadata about ownership, PII, retention policies, etc. ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=how%20we%20protect%20member%20data,that%20we%20store%20and%20analyze)) ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=with%20security%2C%20legal%2C%20and%20executives,our%20commitment%20to%20our%20members)). Their internal tool **CMON (Compliance Monitoring)** scans systems for policy violations and tracks where user data is stored and how it’s used ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=LinkedIn%20uses%20Kafka%2C%20Espresso%2C%20Hadoop%2C,our%20commitment%20to%20our%20members)). This is important for GDPR compliance – e.g., if a user deletes their account, LinkedIn must scrub their data from all systems (online and offline). The data architecture includes hooks so that a deletion request triggers removal in Espresso, Venice, LIquid, activity stores, etc., or at least masking of personal info. The metadata aggregation also helps answer the question “where is this piece of data stored and who uses it,” which is critical in such a complex environment ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=with%20security%2C%20legal%2C%20and%20executives,our%20commitment%20to%20our%20members)).

In summary, LinkedIn’s data storage layer is a **polyglot persistence environment** – different storage technologies for different needs:

- **Espresso** for primary online data with strong needs for multi-master and moderate query complexity (profiles, posts, etc.) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Many%20of%20our%20databases%20run,much%20of%20the%20difficult%20replication)).
- **Voldemort (historically) and Venice** for derived, read-optimized data for AI and recommendations ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=Venice%20entered%20production%20at%20the,Interested%20In%2C%20and%20many%20more)).
- **Kafka** (which we’ll talk about next) as a persistent commit log feeding many stores.
- **Pinot** for real-time analytics and OLAP queries (metrics, insights) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)).
- **LIquid** for graph queries at scale (connections, network) ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=By%20hosting%20LinkedIn%E2%80%99s%20Economic%20Graph%2C,people%20join%20and%20use%20LinkedIn)).
- **Hadoop/HDFS** for the data lake and batch computations ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)).
- **Search indexes** (Galene) for information retrieval.
- **Caches** (memcached or Redis, likely) for ephemeral caching at various points.
- Traditional **RDBMS** for specific use cases requiring SQL or strict transactions (possibly financial data, or some internal tools).

All these are tied together with pipelines (Gobblin, Databus/Brooklin, Kafka) so that data flows where it needs to. Next, we’ll look more closely at that data flow mechanism – LinkedIn’s messaging and event streaming architecture, which connects these storage systems and services in real-time.

## Messaging and Event Streaming (Kafka and Beyond)

LinkedIn is a highly event-driven system. When something happens in one part of the site – a member updates their profile, a new post is created, someone views a profile – dozens of other systems may need to react (update search indexes, send notifications, recompute recommendations, update analytics counters, etc.). To enable this, LinkedIn relies on a robust **messaging and event streaming infrastructure**, centered on **Apache Kafka**. In fact, Kafka was originally developed at LinkedIn to address exactly these needs and has since become a cornerstone of their architecture ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=To%20collect%20its%20growing%20amount,whenever%20someone%20updated%20their%20profile)) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=result%20was%20the%20development%20of,handles%20well%20over%20%2025)).

**Apache Kafka as the Central Data Bus:** By around 2011, LinkedIn had a proliferation of custom data pipelines – separate queuing systems for different features (one for tracking events, one for messaging, one for search index updates, etc.) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=To%20collect%20its%20growing%20amount,whenever%20someone%20updated%20their%20profile)). This was hard to scale and maintain, so LinkedIn built Kafka as a **unified publish-subscribe log** to handle all streaming data ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=As%20the%20site%20grew%2C%20more,over%20%2025%20500%20billion)). Today, Kafka acts as the **central event bus** for LinkedIn:

- User activity events (page views, clicks, likes) are published to Kafka tracking topics.
- Database change events (for example, a new connection established could be published via Databus into Kafka for consumers).
- Log data and metrics are streamed through Kafka into monitoring systems or Hadoop.
- Asynchronous processing: for instance, when you send a message on LinkedIn, the send event might go through Kafka to eventually deliver a notification to the recipient if they are offline, etc.
- Feeding indexes: profile updates flow via Kafka to search index updaters, ensuring near-real-time search.
- **Site monitoring and alerting:** Kafka pipes system metrics and logs into monitoring tools ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=result%20was%20the%20development%20of,handles%20well%20over%20%2025)).
- **Analytics pipelines:** Kafka feeds Hadoop and Samza jobs for both real-time and batch analytics ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=concept%20of%20a%20commit%20log,handles%20well%20over%20%2025)).

LinkedIn runs Kafka at extreme scale. By 2019, their Kafka deployment exceeded **7 trillion messages per day** ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=LinkedIn%E2%80%99s%20scale,surpassed%207%20trillion%20per%20day)). They maintained over **100 Kafka clusters with 7 million partitions** ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=LinkedIn%E2%80%99s%20scale,surpassed%207%20trillion%20per%20day)), an astonishing number. At peak, Kafka at LinkedIn has handled millions of messages per second published (for example, earlier reports noted 4.5 million msgs/sec peaks) ([Kafka Tops 1 Trillion Messages Per Day at LinkedIn - Datanami](https://www.bigdatawire.com/2015/09/02/kafka-tops-1-trillion-messages-per-day-at-linkedin/#:~:text=Datanami%20www,34%20PB%20of)). This scale is achieved by partitioning topics and spreading load across thousands of brokers. Kafka’s design (sequential disk writes, zero-copy reads) is well-suited for this high throughput.

To manage Kafka at this scale, LinkedIn has built an **ecosystem of tools** around it ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=Kafka%20ecosystem%20at%20LinkedIn)):

- **Brooklin:** An open-source data streaming utility used by LinkedIn to **mirror Kafka data across clusters and data centers** ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=,usage%20monitor%20called%20%E2%80%9CBean%20Counter%E2%80%9D)). Brooklin (successor to LinkedIn’s Databus 2.0) can consume from a source (Kafka or other) and produce to a destination (Kafka cluster, or other systems). LinkedIn uses it to keep Kafka clusters in sync across colos (replacing the older Kafka MirrorMaker) and to stream database change events into Kafka from Espresso etc.
- **Kafka REST Proxy:** LinkedIn runs a REST proxy for Kafka to serve non-Java clients ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=,usage%20monitor%20called%20%E2%80%9CBean%20Counter%E2%80%9D)). Many LinkedIn systems are Java-based and use the Kafka Java client, but for those that aren’t (or for convenience in scripting), the REST proxy provides HTTP endpoints to publish/consume messages. This also is used for administrative actions via HTTP.
- **Schema Registry:** LinkedIn standardizes message schemas using Apache **Avro**. Producers register schemas and embed schema IDs in messages; consumers retrieve schemas from the **schema registry** to decode data ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=LinkedIn)). This ensures all services agree on data formats. The schema registry at LinkedIn is backed by a replicated store so that it’s highly available across data centers ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=We%20have%20standardized%20on%20Avro,database%20that%20contains%20the%20schemas)).
- **Monitoring and Audit (Kafka Monitor, Cruise Control, etc.):** With so many clusters, LinkedIn developed tools like **Kafka Monitor** and **Cruise Control**. Cruise Control (open sourced by LinkedIn) automatically balances partition load across brokers and can self-heal by moving data if a broker fails ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=,usage%20monitor%20called%20%E2%80%9CBean%20Counter%E2%80%9D)). **Kafka Monitor (or Kafka Consumer Auditor)** checks the health of the pipelines, ensuring no data loss (e.g., monitoring that consumers are caught up, performing end-to-end latency tracking). LinkedIn’s internal “Bean Counter” tracks usage and completeness of data flows ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=,usage%20monitor%20called%20%E2%80%9CBean%20Counter%E2%80%9D)), and a “Cluster Auditor” likely ensures data is replicated and consumed as expected.
- **LiKafka library:** In LinkedIn’s diagrams, components called “LiKafkaProducer” and “LiKafkaConsumer” are shown. These likely refer to LinkedIn’s internal Kafka client library that wraps the standard client with LinkedIn-specific enhancements (maybe schema handling, better integration with monitoring, etc.).
- **Nuage:** Mentioned in Kafka context as a self-service portal ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=Kafka%20REST%20is%20a%20HTTP,for%20administrative%20operations%20on%20topics)), Nuage allows engineers to create topics, check their throughput, and configure producers/consumers without directly involving ops. It provides visibility (e.g. schema history, data lineage) and governance for Kafka topics ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=Kafka%20is%20self,access)), and also interfaces with security (for ACLs when those are enabled on Kafka).

**Use Cases:** Virtually every engineering team at LinkedIn uses Kafka. Some examples:

- When a member updates their profile, the Profile service writes an event to a Kafka topic “profile_updates”. Consumers like the Search Indexer and Feed service subscribe to that topic to update their stores so that the change is reflected in search results and maybe generates a feed update (“X updated their profile”).
- The **feed fan-out** might be handled via Kafka – when someone posts, that event goes to a “feed_updates” topic which the Feed services consume to update timelines.
- **Email/Notification** pipeline: actions trigger events that are consumed by a Notification service which then sends emails or mobile push notifications.
- **Metrics**: Every page impression or click generates a Kafka event to track feature usage. These go into Pinot for dashboards and also into Hadoop for long-term analysis.
- **Messaging**: Possibly, Kafka is used internally for chat message delivery (though typically a separate system or direct push might handle that for latency; Kafka could be used for logging messages or for async fan-out like sending a copy to email).
- **Stream Processing**: LinkedIn uses **Apache Samza**, a stream processing framework they built (with similar goals to Apache Flink or Storm), to consume Kafka topics and do transformations or joins in real-time ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Apache%20Samza%20was%20initially%20developed,and%20Zhu%20explain%20their%20choice)). For example, a Samza job might join click events with member info to produce a real-time “engagement” metric per segment, writing results to a dashboard or to Venice for use in recommendations. Samza jobs run on YARN and are managed by LinkedIn’s Samza team, providing “streams processing as a service” within the company ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=match%20at%20L313%20Secondly%2C%20deploying,other%20LinkedIn%20tooling%20and%20environments)). We’ll discuss Samza more in the next (real-time processing) section, but it’s deeply tied to Kafka.

**Scaling Kafka:** Running Kafka at LinkedIn’s scale requires careful attention to **operability**. LinkedIn maintains their own Kafka fork (with “-li” suffix) with patches tuned for their needs ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=Running%20Kafka%20at%20such%20a,running%20in%20LinkedIn%E2%80%99s%20production%20environment)). Some focuses have been:

- Improving **throughput and latency** (they likely tuned network and disk settings, and partition allocation).
- Adding **security features** (encryption, authentication). Kafka didn’t initially have these, but now has ACLs – LinkedIn has to manage multi-tenant security so that teams cannot accidentally consume others’ sensitive data.
- **Quota management:** to prevent any one producer or consumer from overrunning a cluster.
- **Monitoring hooks:** exposing more metrics, and integration with LinkedIn’s internal systems (e.g., their centralized trace IDs might propagate through Kafka messages for debugging end-to-end flows).
- **Upgrades and compatibility:** They test new Kafka versions and contribute fixes upstream.

LinkedIn is also known to have early adopted features like tiered storage in Kafka (to handle ever-growing log sizes by offloading older segments to cheaper storage, possibly). Their contributions via open source (such as Cruise Control and Brooklin) show their commitment to scale out Kafka reliably.

**Brooklin for Heterogeneous Streaming:** While Kafka covers many use cases, sometimes data needs to be streamed from other sources or to other sinks. **Brooklin**, developed at LinkedIn, is used for:

- **Change Data Capture (CDC):** Brooklin can tail Espresso’s change logs or Oracle binlogs and publish to Kafka (this was essentially **Databus**’s job historically). In fact, LinkedIn mentions Brooklin as the successor to Databus (next-gen CDC under development around 2016) ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=plays%20a%20critical%20role%20as,which%20is%20under%20development)).
- **Multi-Cluster Mirroring:** Kafka MirrorMaker was used to replicate topics to remote clusters. LinkedIn replaced it with Brooklin for more flexibility and reliability ([Kafka Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin#:~:text=We%20run%20several%20clusters%20of,release%20every%20quarter%20or%20so)). For example, messages written in data center A can be mirrored to data center B’s Kafka via Brooklin so that each DC has all data (if needed). Brooklin ensures ordering per partition is kept and can handle back-pressure.
- **Integrating Legacy Systems:** If some data is on a legacy queue or DB, Brooklin can produce it into Kafka, making it accessible to the rest of the pipeline.

**Streaming Example - Metric Collection:** LinkedIn’s metrics pipeline shows how these pieces fit: Each service logs metrics (counters, latencies) to a local log file; an agent tails it and sends to Kafka (or directly logs to Kafka). Those metrics go through Kafka and are consumed by Samza jobs that might do rolling aggregations, then stored in Pinot for dashboards and also fed to ThirdEye for anomaly detection ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)). The pipeline has to be robust because any drop in metrics could blind LinkedIn’s SREs. Kafka’s strong durability (persisting to disk in order) and scalability made it a reliable backbone for this.

In summary, **Kafka and its ecosystem provide LinkedIn with a “nervous system” for the site** ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=concept%20of%20a%20commit%20log,handles%20well%20over%20%2025)). It allows decoupling – producers don’t need to know who consumes their events, and consumers can independently process events at their own pace. This decoupling is crucial for a large organization; teams can innovate (adding new event consumers for new features) without impacting the event producers.

LinkedIn’s design choice to invest in Kafka has paid off immensely: it enabled near-real-time **data flow between microservices, data stores, and analytics systems**, turning LinkedIn into a real-time enterprise where information propagates almost instantly. Next, we will see how LinkedIn leverages this streaming infrastructure along with batch processing to perform computations – i.e., the “pipeline” that transforms raw events into useful data, both in real-time (stream processing) and offline (batch processing).

## Real-Time and Batch Data Processing

To derive value from the vast data streams and datasets, LinkedIn employs both real-time stream processing and offline batch processing. These complement each other in what was historically called a “Lambda architecture” (combining batch + speed layer) – though LinkedIn has evolved beyond the classic Lambda approach in some cases ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=One%20of%20LinkedIn%27s%20premium%20features%2C,in%20significant%20development%20velocity%20improvements)) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Since%20maintaining%20near%20real,below%20shows%20the%20new%20architecture)). The goal is to provide fresh insights (via streaming) without sacrificing accuracy or depth of analysis (via batch).

**Streaming Data Processing (Apache Samza and Real-Time Systems):** For real-time processing of events, LinkedIn uses **Apache Samza**, a stream processing framework it built and open-sourced. Samza consumes data from Kafka topics (and can output to Kafka or other stores), performing transformations, aggregations, joins, etc., on the fly. At LinkedIn, Samza is effectively the “default” distributed stream processing service ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=architecture%20engineering)). It’s tightly integrated with their environment:

- Samza jobs run on YARN clusters maintained by LinkedIn. Engineers submit Samza jobs which then run across a cluster for scalability ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=match%20at%20L313%20Secondly%2C%20deploying,other%20LinkedIn%20tooling%20and%20environments)).
- Samza provides a high-level API (similar to Apache Beam API) which LinkedIn adopted. In fact, Samza implements the Beam API, allowing devs to define pipelines in a unified way ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Apache%20Samza%20was%20initially%20developed,and%20Zhu%20explain%20their%20choice)). This made it easier to express complex operations (filters, joins, windowed aggregations) in a Samza job ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=,done%20in%20the%20old%20processor)).
- **Examples of Samza use cases at LinkedIn:**
  - **Real-Time Analytics:** Many metrics are computed in real-time using Samza. E.g., counting the number of profile views per member per hour could be done in a streaming way rather than waiting for a batch job.
  - **Derived Features:** If an ML feature needs to be updated live (e.g., a rolling count of a user’s recent activity for a model), a Samza job could maintain that and publish to Venice.
  - **Feed and Notification Aggregation:** Some feed ranking features might be updated continuously. Or grouping notifications (e.g., “You have 5 new likes”) could be handled by a streaming job that aggregates events.
  - **Abuse and Spam Detection:** Samza can be used to scan events in real-time for suspicious patterns to trigger automated defenses.
  - **Removing Lambda complexity:** A notable case study is LinkedIn’s **“Who Viewed Your Profile” (WVYP)** system. Originally, WVYP used a Lambda architecture: a Samza (speed layer) job updated recent views for quick feedback, and a Hadoop batch job computed accurate historical counts, with the results merged in the UI ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=The%20following%20diagram%20describes%20a,batch%20and%20stream%20processing%20methods)) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=A%20near,for%20an%20API%20to%20retrieve)). This meant dual code paths (complex, and double compute cost). LinkedIn engineers recently **migrated WVYP to a Lambda-less architecture** by using Samza for both real-time and nearline aggregation, eliminating the batch layer entirely ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=One%20of%20LinkedIn%27s%20premium%20features%2C,in%20significant%20development%20velocity%20improvements)) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Since%20maintaining%20near%20real,below%20shows%20the%20new%20architecture)). They enhanced the Samza pipeline to handle what the batch did, thus simplifying the system and speeding up development (no need to maintain two different codebases) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=reasoning%20behind%20this%20transition%3A)) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Apache%20Samza%20was%20initially%20developed,and%20Zhu%20explain%20their%20choice)). This demonstrates confidence in Samza’s capability to handle fairly large-scale stateful stream processing with accuracy. The new WVYP design uses Samza to join profile view events with some reference data to compute insights in near real-time, writing results to an OLAP store (Pinot or similar) for serving ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=A%20near,for%20an%20API%20to%20retrieve)) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Since%20maintaining%20near%20real,below%20shows%20the%20new%20architecture)).
- **Samza’s operation:** LinkedIn’s Samza jobs are managed as first-class citizens: the Samza team provides a platform where deploying a job is straightforward, and monitoring (through metrics) is in place ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=match%20at%20L313%20Secondly%2C%20deploying,other%20LinkedIn%20tooling%20and%20environments)). Samza jobs can maintain state via RocksDB (embedded key-value store) for aggregations and use checkpoints for fault tolerance.

Aside from Samza, LinkedIn also leverages other streaming tools:

- **Fired off computations in services:** Not every simple streaming need requires a Samza job. Sometimes a service will subscribe to a Kafka topic directly and handle events inline. For example, the Feed service might consume certain events to update an in-memory cache. But for scalability and reusability, dedicated Samza jobs are preferred for heavy lifting.
- **Real-time indexing** as discussed, uses Kafka + Samza or custom consumers (for search and feed index updates).
- **Complex Event Processing:** It’s possible LinkedIn uses something like Apache Flink or Pinot’s user-defined functions for certain tasks, but Samza has been their main workhorse.

**Batch Data Processing (Hadoop, Spark, etc.):** Batch processing is used for deeper analysis, rebuilding large indexes, and any processing that can tolerate minutes or hours of latency but might need to crunch huge volumes of historical data. LinkedIn has a massive Hadoop ecosystem:

- **Hadoop MapReduce:** Historically used for things like building People You May Know suggestions via collaborative filtering on the entire graph, generating “Notable Alumni” for universities, or computing distribution statistics for the feed ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)). LinkedIn’s “Browse Map” (who viewed profiles) or other products were initially computed by MapReduce jobs offline ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)).
- **Apache Spark:** Spark is likely extensively used at LinkedIn now for machine learning, graph analytics, and faster batch jobs. In fact, LinkedIn open-sourced **Photon ML**, a machine learning library on Spark, used for training models like job recommendations or relevance models ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=%C2%B7%C2%A0TonY%3A%C2%A0TensorFlow%20on%20YARN%20,training%20as%20a%20Hadoop%20application)). Photon ML supports algorithms at LinkedIn scale (e.g. large-scale linear models).
- **Presto or Hive:** For ad-hoc querying of the data lake, LinkedIn likely uses query engines like Presto or Hive. Their data scientists and analysts query data for insights and experiments through such tools.
- **Azkaban:** LinkedIn created **Azkaban**, a batch workflow scheduler, to manage Hadoop job pipelines (similar to Airflow or Oozie). Azkaban would schedule daily jobs, manage dependencies (e.g., first run the data import, then run the training job, then publish the results to HDFS/Venice).
- **Gobblin:** As mentioned, Gobblin moves data into HDFS, but also can move between other stores. For example, a Gobblin job might dump a day’s worth of Kafka events to HDFS for archive and later backfill.

**Combined Architecture – When to Use What:** LinkedIn often pairs streaming and batch:

- For things that require **instant reaction** (seconds), they use streaming (Samza).
- For things that can be **computed periodically** with more completeness (hours or daily), they use batch (Hadoop/Spark).
- Some systems use both: e.g. the feed ranking might use streaming to update counts and batch to recompute model weights daily.
- Over time, as streaming tech matured, LinkedIn has tried to unify them. The WVYP case is one where they dropped the batch part altogether in favor of a continuous pipeline ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=One%20of%20LinkedIn%27s%20premium%20features%2C,in%20significant%20development%20velocity%20improvements)) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=,away%20from%20the%20Lambda%20architecture)). They found maintaining both batch & stream caused code duplication and potential inconsistencies ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=reasoning%20behind%20this%20transition%3A)).

Another example: **Experimentation metrics.** When LinkedIn runs A/B tests, they need dashboards of results. Likely a streaming job computes near-real-time metrics for an experiment (so PMs can see preliminary results), but the final analysis might be done with a batch job (scanning full data at end of experiment for statistical significance).

**Data Lake and ML Feature Pipelines:** LinkedIn’s ML platform often starts with batch processing:

- They aggregate training data sets on Hadoop (join various sources like member data, activity labels, etc.).
- Train models (perhaps using Spark or TensorFlow on YARN with their **TonY** framework ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=setup%20across%20multiple%20datacenters%20and,provides%20very%20cheap%20storage))).
- Output model artifacts or large feature sets that get loaded into online systems (e.g. a trained model might be exported and served by a model-serving system, or user feature embeddings might be computed in batch and loaded to a key-value store).
- **Feature Engineering:** LinkedIn’s **Feathr feature store** ties into both batch and streaming. Some features are computed in batch (like aggregations over months of data), while others are updated via streaming (like count in last hour). Feathr provides an abstraction so that an ML engineer can define a feature once and have it computed in the appropriate way (batch or stream or hybrid) ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=We%20are%20open%20sourcing%20Feathr,specific%20feature%20pipeline%20solutions)) ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=At%20LinkedIn%2C%20we%20have%20hundreds,our%20ML%20applications%20at%20scale)). Feathr then materializes those features either offline for training or online for serving, using the appropriate pipeline (Spark for batch features, Samza for streaming features, etc.) ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=We%20are%20open%20sourcing%20Feathr,specific%20feature%20pipeline%20solutions)) ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=At%20LinkedIn%2C%20we%20have%20hundreds,our%20ML%20applications%20at%20scale)).

**Eliminating the Lambda Architecture:** The “Lambda architecture” pattern (batch + speed layer producing outputs to be merged) was a popular approach to ensure both correctness and real-time results. LinkedIn used it in places (like WVYP). However, maintaining two pipelines is effort-intensive. With improvements in stream processing (stateful, exactly-once processing in Kafka/Samza, etc.), LinkedIn has been able to trust stream computations more. They reported significantly improved development velocity after removing the batch layer in WVYP ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=reasoning%20behind%20this%20transition%3A)). That said, some systems still likely use a Lambda style (especially those developed before these improvements or where batch is still much simpler).

One compromise is the **Kappa architecture** – rely on stream processing only, and if you need to recompute, just replay the entire log through your stream job. Kafka’s retention of events makes this feasible to an extent (if you keep a year of events, you could recompute a year’s state by replaying). Tools like Apache Beam also aim to unify batch and streaming into one model. LinkedIn’s use of Beam API on Samza suggests they are moving toward such unification ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Apache%20Samza%20was%20initially%20developed,and%20Zhu%20explain%20their%20choice)).

**Case Study – Real-time Profile Ranking:** As a hypothetical, consider LinkedIn’s search ranking for people. They might train a model offline (batch) but use features that are updated in real time (like whether a profile was recently active). To serve search queries:

- A batch job trains a model (e.g. learning to rank) using historical data.
- The model is deployed to an online scoring service.
- Real-time events (profile updates, user activity) are fed to this scoring service or its feature store (via streaming) so that when a search happens, the model has up-to-date info (e.g. boost recently active profiles).
- The actual search query fan-out and scoring might be done in the search service (Galene) which merges both static batch-computed data and dynamic streaming-fed data.

All these illustrate how LinkedIn mixes batch and streaming to get both **depth of analysis and freshness of data**.

In summary, LinkedIn’s data processing pipeline is **end-to-end and continuous**:

- **Ingestion**: from site to Kafka (real-time) and Hadoop (batch).
- **Processing**: Samza for streaming transformations and Spark/Hadoop for batch crunching.
- **Storage**: Results to Venice, Pinot, LIquid, or Espresso depending on usage.
- **Serving**: via microservices, search, feed, etc., to end users.

This resilient pipeline ensures LinkedIn can derive insights from data quickly (e.g., updating a recommendation minutes after a user’s action) while also leveraging the full power of big data for more complex analysis (e.g., training an AI model on months of data). With this data pipeline foundation, LinkedIn then builds specific product features like the Feed and recommendations, which we will examine next, along with the machine learning infrastructure that underpins personalization on the platform.

## Machine Learning Infrastructure and Recommendation Systems

Personalization and recommendation are at the heart of LinkedIn’s member experience – from feed ranking to “People You May Know” to job recommendations and beyond. LinkedIn has invested heavily in an internal **machine learning infrastructure (Pro-ML)** to support these use cases end-to-end, from data preparation and model training to online inference and feature storage ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=The%20core%20of%20LinkedIn%E2%80%99s%20machine,like%20Spark%20or%20Hadoop%20YARN)). In this section, we cover how LinkedIn’s architecture supports machine learning at scale and highlight key recommendation systems.

**Pro-ML Platform:** LinkedIn’s core machine learning platform is often referred to as **Pro-ML** (Productive Machine Learning) ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=The%20core%20of%20LinkedIn%E2%80%99s%20machine,like%20Spark%20or%20Hadoop%20YARN)). Pro-ML is a **lifecycle management system for ML models** – it handles training, validation, deployment, and monitoring of models in production ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=The%20core%20of%20LinkedIn%E2%80%99s%20machine,like%20Spark%20or%20Hadoop%20YARN)). Conceptually, Pro-ML provides:

- **Training Pipeline Orchestration:** It likely integrates with Hadoop/Spark to run training jobs (e.g., schedule a Spark job to train a new model version daily or when data is updated).
- **Feature Management:** Integration with feature stores (Feathr) to ensure models use consistent features in training and serving.
- **Model Serving and Online Inference:** Once a model is trained, Pro-ML helps deploy it to an online environment for inference. This might be an internal prediction service or embedded model in the application.
- **Experimentation Hooks:** Pro-ML connects with LinkedIn’s A/B testing platform (XLNT) so new models can be rolled out as experiments (serve model A to a control group and model B to experiment group).
- **Monitoring and Feedback:** It monitors model performance metrics (accuracy, etc.) and data quality. If data drift or anomalies occur, Pro-ML can alert engineers or even trigger retraining ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=The%20core%20of%20LinkedIn%E2%80%99s%20machine,like%20Spark%20or%20Hadoop%20YARN)).
- **Automation:** The aim is to automate as much as possible, doubling the effectiveness of ML engineers ([Pro-ML is the Architecture Powering Machine Learning at LinkedIn](https://thesequence.substack.com/p/edge46#:~:text=LinkedIn%20thesequence.substack.com%20%20Conceptually%2C%20Pro,ML)) ([Scaling Machine Learning Productivity at LinkedIn](https://www.linkedin.com/blog/engineering/machine-learning/scaling-machine-learning-productivity-at-linkedin#:~:text=Scaling%20Machine%20Learning%20Productivity%20at,AI%20and%20modeling%20to)). This includes automating retraining, hyperparameter tuning, etc.

LinkedIn’s Pro-ML leverages open-source and LinkedIn-built tools:

- **Training at Scale:** They use Apache Spark for large-scale ML (with Photon ML library for algorithms) ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=%C2%B7%C2%A0TonY%3A%C2%A0TensorFlow%20on%20YARN%20,training%20as%20a%20Hadoop%20application)). For deep learning, LinkedIn developed **TonY** (TensorFlow on YARN) to train neural networks on Hadoop clusters ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=setup%20across%20multiple%20datacenters%20and,provides%20very%20cheap%20storage)). TonY now supports TensorFlow, PyTorch, etc., enabling distributed training jobs on LinkedIn’s infrastructure.
- **Feature Store (Feathr):** As introduced, **Feathr** is LinkedIn’s feature store for ML, now open-sourced ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=We%20are%20open%20sourcing%20Feathr,specific%20feature%20pipeline%20solutions)). Feathr provides a unified way to define features, compute them (batch or streaming), and serve them online. It addresses the challenge of “feature parity” – ensuring the feature values during model training (which might come from offline data) match those during online inference ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=At%20LinkedIn%2C%20we%20have%20hundreds,our%20ML%20applications%20at%20scale)) ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=Feature%20preparation%20pipelines%2C%20the%20systems,serving%20skew)). Feathr allows teams to share common features across models and avoid redundant pipeline work ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=We%20are%20open%20sourcing%20Feathr,specific%20feature%20pipeline%20solutions)) ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=A%20few%20years%20ago%2C%20we,need%20to%20ensure%20features%20are)). Dozens of LinkedIn applications use Feathr to manage thousands of features in production ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=We%20are%20open%20sourcing%20Feathr,specific%20feature%20pipeline%20solutions)). For example, a feature like “number of profile views in last 7 days” can be defined once in Feathr; offline it will compute it from logs, and online it might be updated by a streaming job, but any model using it just calls Feathr’s API to get the value, abstracting the complexity.
- **Model Storage/Serving:** Trained models (especially large ones like gradient boosted trees or neural nets) might be stored in a model repository (perhaps as PMML or ONNX or custom format) and served via a service. LinkedIn has not open-sourced a specific model serving tool, but likely they have something analogous to Google’s TF Serving or Uber’s Michelangelo. They might also leverage **Venice** or other stores for model parameters if a model is simple enough to be stored as data (e.g., coefficients for a logistic regression could be stored in a key-value store and applied in code).

**Recommendation Systems:** Several flagship LinkedIn recommendation products and how the architecture supports them:

- **People You May Know (PYMK):** PYMK suggests new connections. This system operates on LinkedIn’s graph. Historically, LinkedIn used **graph and co-occurrence algorithms offline** to generate candidates ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20developed%20a%20massive%20offline,Alumni%2C%20and%20profile%20browse%20maps)) (like computing common connections, similar profiles, etc., via Hadoop jobs) and then used models to rank those candidates. The results were stored (previously in Voldemort, now likely Venice) for each user ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)). On the online side, when you visit the My Network page, the service pulls your precomputed PYMK list from Venice (fast key lookup) and possibly refines it (filtering out people you’ve seen or connected with since computation, etc.). The push toward real-time could mean that some aspects of PYMK update live (for example, if you and I share a brand new mutual connection, the system might update recommendations sooner via a streaming update). LinkedIn mentioned using Hadoop and Voldemort to precompute PYMK and similar features ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=automatically%20www)), highlighting the batch component. Now with LIquid, finding second-degree connections and such could be done on the fly for incremental updates.
- **Feed Ranking:** The LinkedIn feed uses machine learning to personalize the order of updates. The feed ranking model is trained on engagement data (what posts you click, like, etc.). LinkedIn has published about using **multi-objective learning** (optimizing for various engagement types) and incorporating **deep learning** for feed ranking. The architecture for feed ML:
  - **Feature Ingestion:** Features include user’s profile, item content (text, author, etc.), social graph features (are you connected to author?), temporal features, etc. These are gathered via Feathr from various stores (LIquid for graph features, Pinot for aggregated stats, etc.).
  - **Model:** Likely a neural network or ensemble that outputs a relevance score.
  - **Serving:** When your feed is requested, the Feed service fetches a pool of candidate updates (say 100 updates from people/companies you follow, retrieved via FollowFeed timeline store, covered in next section) and then calls the ML model to score them. The model might be served in a service with CPU/GPU acceleration if needed, given the volume of scoring. The top N are returned to display. Latency is crucial; this scoring must happen in tens of milliseconds. That is why a lot of feature pre-processing is done – e.g., heavy NLP on content (like classifying the topic of a post) is done offline so that only a numeric feature is used at serve time, rather than doing NLP in real-time.
  - **Online A/B testing:** The feed ranking is continuously A/B tested – new models or feature tweaks are rolled out to small percentages to measure impact on metrics (e.g., viral actions, session length).
  - **Real-time adaptation:** LinkedIn’s feed also incorporates immediate feedback loops. For instance, if you hide a certain type of post or mark “I’m not interested in this content”, the system updates your preferences quickly. Such signals likely go through Kafka to the feed preference store and may directly lower similar content’s ranking for that user in subsequent requests.
- **Job Recommendations (Jobs You May Be Interested In):** This uses profile, resume, and activity data to match members with job postings. LinkedIn likely uses a combination of **content-based filtering** (matching skills to job requirements) and **collaborative filtering** (people like you applied to these jobs) done offline. The architecture might involve:
  - An offline indexing of jobs and members in some vector space, and computing top matches via matrix factorization or ANN (approx nearest neighbor) algorithms.
  - Results stored in Venice keyed by member.
  - An online service to filter out expired jobs or ones the user already saw/applied, then rank the rest. The ranking could be an ML model that accounts for user recent activity (if a user is highly active in job search, maybe show more fresh jobs).
  - Real-time signals like a member clicking on certain jobs can trigger updates to their recommendations sooner via streaming (e.g., emphasize similar jobs).
- **Search and Recommendation Convergence:** LinkedIn treats search and recommendation somewhat distinctly (search is user-initiated queries, recommendations are system-initiated suggestions), but the infrastructure overlaps. For example, **Galene search** might incorporate ML in ranking (like learning to rank for search results – indeed LinkedIn had an ML system called **Galene Learning**). Those models are part of search’s serving stack. Meanwhile, recommendations like PYMK use graph algorithms (LIquid) and ML models to pick who to suggest (taking into account the likelihood you know that person and will connect).
- **Ads and Pricing Models:** LinkedIn also has advertising (sponsored posts, sponsored job listings). Their ML infra is used for ad relevance and bid optimization. They likely have real-time bidding features computed (like a user’s probability to click an ad given context) using the same Kafka/Samza pipeline. Pinot might store aggregated ad performance metrics for use in pacing and budget management in near-real-time.

**Infrastructure for Model Serving:** Ensuring low-latency inference for many models is a challenge. LinkedIn could use:

- **Dedicated prediction services**: A cluster where models are loaded into memory and queries are sent via RPC. Some big tech companies use one service cluster per major model (for instance, feed model service, jobs model service, etc.). Others use a generic system where any model can be deployed and addressed.
- **On-the-fly in the application**: For simpler models, the logic might be embedded in the service code (like computing a linear regression is trivial to do in Java code if you have the coefficients). For example, LinkedIn’s anti-abuse scoring might just be a linear formula applied in request flow.
- **Hardware acceleration**: If deep models are used (like BERT-based NLP for content understanding or image analysis for profile pictures), LinkedIn might leverage GPUs. They could use data center GPU clusters where expensive tasks are offloaded asynchronously.

**Monitoring ML Systems:** LinkedIn’s observability extends to ML:

- They track feature distributions (to detect if an input feature suddenly has values outside the expected range, which might indicate a bug upstream – e.g., a parsing issue).
- Track model outputs and key metrics (like CTR, message acceptance rate for connection recommendations, etc.) and compare across model versions or against baselines.
- They have an internal **Model Health Assurance platform** (one of their blog posts implies such a tool with inGraph integration for model metrics) ([Model health assurance platform at LinkedIn](https://www.linkedin.com/blog/engineering/analytics/model-health-assurance-at-linkedin#:~:text=Model%20health%20assurance%20platform%20at,monitors%20drift%20in%20prediction)).

**Human in the Loop:** LinkedIn also employs human domain experts (e.g., editorial or taxonomists) in some ML processes. A blog noted how they use human-curated relationships (like knowing “Sr. Software Engineer” ~ “Senior Software Engineer” job titles) to feed into ML models ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=Machine%20Learning%20with%20Humans%20in,the%20Loop)). This hints at knowledge graph usage (they have an internal ontology of skills, titles, etc.). The **Knowledge Graph** of LinkedIn (part of Economic Graph) supplies features to ML models. For example, a job recommendation model might use a feature like “distance between member’s skills and job’s required skills in the skills graph”.

In summary, LinkedIn’s ML infrastructure is a **comprehensive ecosystem**:

- Data pipelines (Kafka, Samza, Hadoop) feed data in.
- Feature store (Feathr) simplifies feature engineering and ensures consistency ([Open sourcing Feathr – LinkedIn’s feature store for productive machine learning](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=We%20are%20open%20sourcing%20Feathr,specific%20feature%20pipeline%20solutions)).
- Training infrastructure (Spark, TonY) scales model development.
- Experimentation platform (XLNT/T-Rex) enables scientific evaluation of models at runtime ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=At%20any%20given%20time%2C%20LinkedIn%E2%80%99s,a%20notification%20system%2C%20and%20a)).
- Serving infrastructure (online features, model inference) delivers the results in production.
- Monitoring and iteration close the loop, so models are retrained and improved regularly.

This empowers LinkedIn to deploy a variety of recommendation engines and personalization features that drive member engagement. Next, we will dive into one such feature in detail – the LinkedIn Feed – which ties together many of these components (streaming, data storage, ML ranking, etc.) into the end-user content distribution system.

## Feed and Content Distribution Models

The LinkedIn homepage feed is a central product experience, showcasing updates from a member’s network and interests. Architecturally, generating and delivering the feed is a complex challenge, as it involves **gathering content from many sources, ranking that content in real-time for each user, and doing so at scale for hundreds of millions of daily feed visits**. LinkedIn’s feed system has undergone major redesigns. Let’s explore how it works now, and how it evolved from earlier models.

**Original Feed (Sensei and Feed-Mixer):** In the earlier days (circa early 2010s), LinkedIn’s feed was powered by a system called **Feed-Mixer**, which took care of blending various content types (posts, shares, ads, etc.) ([Maximizing Our Publishing Platform Reach with Network Distribution](https://www.linkedin.com/blog/engineering/archive/maximizing-our-publishing-platform-reach-network-distribution#:~:text=At%20LinkedIn%2C%20the%20Feed,relevant%20content%20for%20the%20viewer)). Content was stored and retrieved via a search-like engine called **Sensei** ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Image%3A%20Sensei%3A%20Previous%20Feed%20Infrastructure)). Sensei was a distributed real-time indexing system built on Lucene:

- Members’ feed updates (from connections, followed entities) were indexed into Sensei’s searcher nodes via Kafka events ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Content%20records%20were%20ingested%20into,sorted%20according%20to%20their%20relevance)). Each update became a document in an inverted index.
- Feed-Mixer would, on a feed request, query Sensei for that member’s content (essentially retrieving recent posts relevant to the user) using a query language called BQL that allowed filtering and scoring on the fly ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Kafka,A%2FB%20test%20different%20relevance%20algorithms)).
- The relevance scoring (ranking algorithm) was embedded in the Sensei query itself as a script, enabling quick experimentation (they could A/B test different scoring functions by changing the query parameters) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=logic,A%2FB%20test%20different%20relevance%20algorithms)) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=scores,A%2FB%20test%20different%20relevance%20algorithms)).
- The results from multiple shards were merged by a broker and returned the top N to show ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=logic,A%2FB%20test%20different%20relevance%20algorithms)).

This approach scaled horizontally (they could add searcher nodes) and allowed fairly complex ranking. However, it had downsides: **operational complexity and high memory usage** (keeping full index in RAM) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,of%20smaller%20components%E2%80%94Bobo%2C%20Norbert%20and)), **code complexity with many components** ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=increased%20the%20cost%20to%20operate,the%20same%2C%20while%20leverage%20decreased)), and diminishing returns since Sensei was being phased out in favor of LinkedIn’s new search (Galene) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,the%20engineering%20organization%2C%20it%20was)). Also, as LinkedIn’s network grew denser and content volume per user rose, Sensei struggled. Feed-Mixer needed a refresh.

**FollowFeed – Modern Feed Generation:** Around 2016-2017, LinkedIn developed a new feed system called **FollowFeed** to make the feed faster and smarter ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Twenty%20months%20ago%2C%20we%20set,and)). FollowFeed took a different approach: instead of treating the feed as a search problem on a global index, it pre-computes and stores a **timeline for each user** of content from the entities they follow ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=FollowFeed%20generates%20the%20feed%20for,by%20an%20entity%20with%20the)) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L332%20In%20FollowFeed%2C,partitioned%20across%20a%20cluster%20of)). Key aspects of FollowFeed’s design:

- Each member (and each entity like a company or hashtag) has a **timeline** of content they produced or that is associated with them, stored as a list (most recent first). For example, for user A, there’s a timeline of all posts by A, for Company X, a timeline of posts by X, etc. ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L332%20In%20FollowFeed%2C,partitioned%20across%20a%20cluster%20of)).
- The feed for member M is generated by **aggregating the timelines** of all entities M follows (connections, followed influencers, groups, hashtags, etc.) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=FollowFeed%20generates%20the%20feed%20for,by%20an%20entity%20with%20the)) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=In%20FollowFeed%2C%20timelines%20are%20logically,partitioned%20across%20a%20cluster%20of)). Essentially, at request time, the system needs to merge N sorted lists (where N is number of sources user follows) to produce a consolidated sorted list of recent updates.
- **Storage:** FollowFeed stores these timelines in a key-value store. LinkedIn chose to use **RocksDB** (an embedded storage engine) at the heart of FollowFeed’s index nodes ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L352%20For%20persistent,a%20JNI%20binding%20to%20enable)). The feed index is partitioned across a cluster of machines, each holding many members’ timelines. The data structure can be thought of as: key = (EntityID, ContentType) and value = list of content IDs in reverse chrono order ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=In%20FollowFeed%2C%20timelines%20are%20logically,partitioned%20across%20a%20cluster%20of)). RocksDB provides persistence on SSD and fast access.
- **Bringing computation to data:** Instead of pulling all those lists to one server to merge, FollowFeed executes the fan-in merge in a distributed way. The index nodes that store the timelines also do partial relevance computation. Specifically, FollowFeed servers (index nodes) will fetch the latest items from each timeline in their partition and apply filters and preliminary scoring then send top results to a central aggregator which finalizes the merge ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L479%20A%20key,and%20caches%20relevance%20features%20of)) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=A%20key%20feature%20of%20FollowFeed,and%20caches%20relevance%20features%20of)). This parallelizes feed assembly.
- **In-memory caching:** FollowFeed uses a Guava cache on each index node to cache recently accessed timeline data, since many members check frequently so their data is hot ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L420%20Utilizing%20this,This%20translates%20to%20two%20requirements)). Efficient caching is “mission-critical” as described ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L420%20Utilizing%20this,This%20translates%20to%20two%20requirements)).
- **Filtering and Policy:** FollowFeed supports a custom filtering grammar allowing clients to specify criteria (e.g., only certain content types, or “unseen” content) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L467%20services,not%20actually%20use%20SQL%20and)). This is used for things like “show only job posts” feeds, etc. The filtering happens at the index node as it scans timeline entries ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=FollowFeed%E2%80%99s%20filtering%20logic%2C%20here%20is,not%20actually%20use%20SQL%20and)).
- **Relevance Ranking:** A key feature is to integrate ML-based ranking. FollowFeed doesn’t just merge by time; it ranks by a score. To enable this, it **pre-computes some relevance features for each content item and stores them with the item in the timeline** ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L479%20A%20key,and%20caches%20relevance%20features%20of)). E.g., for a post, it might store features like “number of likes so far” or “author’s connection strength”. These features are updated either in real-time or at write time, so when assembling the feed, the system can compute a relevance score quickly. FollowFeed can perform the heavy computation of scoring in parallel on index nodes, each scoring the candidates it will potentially output ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L479%20A%20key,and%20caches%20relevance%20features%20of)). The final aggregator combines the top lists from each node and possibly reranks globally (ensuring diversity, freshness).
- **Performance:** FollowFeed achieved a **5x reduction in 99th percentile feed latency** compared to the old system ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Twenty%20months%20ago%2C%20we%20set,and)). This is huge for user experience. It also allowed **wider distribution** (members had more connections and content but system kept up) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,distributed%20to%20a%20wider%20audience)) and **improved relevance** (could handle more complex scoring) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=,time%20performance)).
- **Data retention:** They increased how much content they keep in the feed index (as noted, 20x more retention with half the hardware vs old system) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=data%20retention%20,servers%20was%20reduced%20by%20half)), likely due to RocksDB efficiency.

**Real-time Feed Updates:** When someone posts or engages:

- The new post event goes through Kafka. The FollowFeed system consumes this event and **appends the post ID to the timelines** of relevant followers. How? Possibly via **multicast** – e.g., if user A with 500 connections posts, the feed service will append to A’s own timeline and also to an “aggregator” timeline for each follower. However, doing 500 writes per post might be heavy (this is the classic fan-out write model). It might instead push into a Kafka topic that each timeline partition consumes and updates its store accordingly. Given LinkedIn’s scale, they likely do near-real-time fan-out for first-degree connections. For second-degree, they rely on graph traversal on read (i.e., they might not push content that far out unless it goes viral).
- For viral content (likes, comments making content appear in second-degree feeds), the system could treat that as another event: e.g., if your connection likes someone’s post, that event might cause that post to be eligible in your feed (with a context “X liked this”). FollowFeed likely handles this by having separate timelines for “activities” like likes. They could append “A liked Post P” to all of A’s connections’ “activity timeline”. Then on read, merging content timeline and activity timeline yields the full feed.

This is complex, but LinkedIn’s design likely balances between **fan-out on write** (pushing new content to followers’ lists for efficiency at read) and **fan-in on read** (for when it can’t precompute every combination, it computes on demand). FollowFeed leans more on fan-out (each follow relationship results in timeline entries), which is feasible because LinkedIn’s average connection degree is not too high relative to, say, Twitter (Twitter with millions of followers per user can’t push to all followers easily, LinkedIn’s connections are mutual and usually in the hundreds at most). The blog mentions network graph getting denser, but still manageable for first-degree.

**Unified Social Content Platform (USCP):** In Byron Ma’s 2014 blog ([Maximizing Our Publishing Platform Reach with Network Distribution](https://www.linkedin.com/blog/engineering/archive/maximizing-our-publishing-platform-reach-network-distribution#:~:text=When%20an%20author%20publishes%20an,and%20serves%20it%20to%20clients)), he references a **Unified Social Content Platform** that Feed-Mixer fetches activities from. USCP likely was an abstraction over various content sources (status updates, shares, original posts, etc.). In FollowFeed era, USCP might be replaced by or integrated with FollowFeed’s storage. It indicates the feed aggregator needed to talk to a central content store plus various personalized recommenders (“first pass rankers”). Now, FollowFeed’s approach is the content store and first-pass ranking combined at the timeline index nodes.

**Diversity and Mixing:** Feed-Mixer was designed to mix content types (user posts, ads, recommended follows, etc.) ([Maximizing Our Publishing Platform Reach with Network Distribution](https://www.linkedin.com/blog/engineering/archive/maximizing-our-publishing-platform-reach-network-distribution#:~:text=At%20LinkedIn%2C%20the%20Feed,relevant%20content%20for%20the%20viewer)). FollowFeed presumably continues this, but possibly with a different mechanism. It likely still calls out to other services for content to mix in:

- **Ads**: an Ads service provides sponsored content which needs to be slotted into the feed.
- **Recommendations**: LinkedIn might insert “People You May Know” or “Jobs” suggestions as feed items. Those come from their respective services.
- **Trending News**: sometimes LinkedIn shows news highlights or prompts; those come from an editorial service.

Feed assembly has to integrate these with the organic content. Possibly the **aggregator layer (Feed API)** after getting organic posts from FollowFeed will call these other sources to insert items in the feed, then do a final rank to blend them in a sensible way (ensuring, say, not too many ads, proper spacing, etc.).

**Push Notifications and Real-Time Delivery:** LinkedIn also delivers some content proactively. For example, if someone in your network posts and LinkedIn thinks it’s highly relevant to you, they might send a push notification. That involves a separate pipeline: the Feed service or an offline job might identify such events and trigger a notification (via Kafka to the notification service). That’s more on the notification architecture, but it intersects feed relevance logic.

**Evolution Over Time:** The feed system is continuously evolving:

- **Harnessing ML Embeddings:** A 2023 LinkedIn blog describes upgrading the feed relevance model to use massive embedding features (hundreds of millions of parameters) to capture member-content interaction patterns ([Enhancing homepage feed relevance by harnessing the power of large corpus sparse ID embeddings](https://www.linkedin.com/blog/engineering/feed/enhancing-homepage-feed-relevance-by-harnessing-the-power-of-lar#:~:text=Our%20Homepage%20Feed%20produces%20billion,algorithms%20which%20power%20these%20products)) ([Enhancing homepage feed relevance by harnessing the power of large corpus sparse ID embeddings](https://www.linkedin.com/blog/engineering/feed/enhancing-homepage-feed-relevance-by-harnessing-the-power-of-lar#:~:text=Our%20focus%20is%20on%20transforming,relevant%20or%20aligns%20with%20member)). This suggests LinkedIn moved to deep learning for feed. The infrastructure had to incorporate these – possibly via a recommendation model that uses embeddings for members and content. Training those required big data and the serving required efficient similarity lookup or an expanded feature vector to the model. They mention modernizing architecture to handle large models and ensure swift training and serving ([Enhancing homepage feed relevance by harnessing the power of large corpus sparse ID embeddings](https://www.linkedin.com/blog/engineering/feed/enhancing-homepage-feed-relevance-by-harnessing-the-power-of-lar#:~:text=feed%20to%20our%20members%20that,the%20goals%20of%20our%20vision)). This means more GPU use in training and perhaps even in inference, or at least heavily optimized CPU inference with vectorized math.
- **Multi-Objective Optimization:** The feed doesn’t just maximize one metric; LinkedIn cares about different aspects (e.g., user engagement but also seeing diverse content, etc.). Their ML training (multi-task learning) is likely supported by the infrastructure (with TensorFlow & TonY for training multi-objective models as referenced in a 2021 blog on multi-task for feed).
- **Real-time Interaction Graphs:** A recent trend is using graph neural networks (GNNs) or real-time graph features in feed ranking (to capture community trends). LinkedIn’s graph systems team published about using GraphSAGE on the social graph for recommendations ([Graph Systems - LinkedIn](https://www.linkedin.com/blog/engineering/graph-systems#:~:text=Graph%20Systems%20,new%20graph%20database%2C%20Part%202)). Integrating that requires combining graph queries (with LIquid) into feed ranking features.

**Outcome:** LinkedIn’s current feed architecture (FollowFeed + ML ranking) achieves:

- Much faster response: p99 latency was cut by 80% or more ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Twenty%20months%20ago%2C%20we%20set,and)), meaning even users with lots of content get their feed quickly.
- More personalized ranking due to ML and more features (the system can rank tens of thousands of candidate updates per request in parallel) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L479%20A%20key,and%20caches%20relevance%20features%20of)).
- Ability to handle **wide fan-out** (if a user follows many people, the system still merges efficiently) and **wide fan-in** (if a post is very viral, it can appear in many feeds).
- Simpler scaling of storage by partitioning timelines (horizontal scale by adding timeline servers).
- Cleaner separation: content ingestion (writing to timelines) vs content serving (reading timelines with relevance) is well-defined.

The feed illustrates a broader principle in content distribution at LinkedIn: they maintain **per-user or per-entity state to avoid expensive computations on the fly**. This is also seen in other features: e.g., each member has an **update digest** for notifications, each group has a list of recent posts, etc. Precomputing or incrementally maintaining these lists can drastically reduce query load.

Now that we have seen the feed – which combines distributed storage, streaming updates, and ML ranking – we will move to the underlying graph and network that powers much of LinkedIn’s products, followed by operational aspects like deployment and monitoring.

## Social Graph Storage and Traversal

LinkedIn’s core is the **professional graph** – a network of members connected by relationships (connections, colleagues, etc.) and enriched with entities like companies, schools, skills, groups. Storing and querying this graph efficiently is critical for features such as connection suggestions, mutual connection display, introductions, and graph-based recommendations. We touched on LinkedIn’s graph infrastructure (the LIquid graph database) earlier; here we dive deeper into how the social graph is stored, traversed, and utilized in the architecture.

**Graph Data Model:** LinkedIn’s social graph can be considered a **property graph**: nodes represent entities (members, organizations, skills, etc.) and edges represent relationships (Member -> Member connection, Member -> Company employment, Member -> Skill endorsement, etc.). Each node/edge may have attributes (e.g., an edge might have “connection strength” or timestamps). LinkedIn’s public notion of a “connection” is a mutual relationship (bidirectional edge) akin to an undirected edge in the graph.

**Early Implementation (In-Memory Graph):** The first graph service, called **Cloud**, kept the member-member connection graph in memory for fast traversal ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Member%20Graph)). This allowed operations like “shortest path” (degree of connection) queries to quickly determine how two people are connected (e.g., 2nd or 3rd degree). Cloud was a separate service that the app would call for graph queries (like listing mutual connections between two users). It used Java’s memory and possibly some custom algorithms. However, an in-memory approach has limitations: memory size vs graph size, and difficulty in updating (if it’s not persistent, needs reload or external store). As LinkedIn grew, a more robust system was needed.

**LIquid Graph Database:** As discussed, LinkedIn spent several years building **LIquid**, which is now the backbone of their graph queries ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Editor%E2%80%99s%20note%3A%20In%20this%20two,free%20shared)) ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Introducing%20LIquid%2C%20LinkedIn%E2%80%99s%20in,database)). Key highlights:

- **Relational Graph Model:** LIquid retains the relational algebra approach – meaning it treats the graph edges kind of like relational rows (subject, predicate, object) but optimized for graph operations ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=the%C2%A0economic%20graph,memory%20index%20structures)). This gives it a formal underpinning, making complex queries easier to express.
- **Datalog Query Language:** Instead of writing imperative traversals, LinkedIn engineers can query the graph with a declarative language (Datalog), specifying patterns and letting the engine figure out the optimal traversal ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=traversal%20of%20graph%20edges%20with,memory%20index%20structures)).
- **High Performance Traversal:** LIquid’s claim to fame is **constant-time traversal** of edges ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Editor%E2%80%99s%20note%3A%20In%20this%20two,free%20shared)). Likely, it uses adjacency lists and hash indexes in memory, so retrieving the neighbors of a node is O(1) or O(k) where k is degree. A series of traversals (like finding 2nd degree connections) can be done quickly by intersecting neighbor sets.
- **Wait-Free Shared Memory Structures:** This suggests LIquid’s internal data structures allow concurrent reads (and maybe writes) without locking, enabling extremely high QPS in multi-threaded environment ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=complex%20n,memory%20index%20structures)). Possibly using copy-on-write or lock-free algorithms for updating indices.
- **Scaling the Graph:** The LinkedIn graph (with 270 billion edges) obviously doesn’t fit on one machine. LIquid is a distributed service. The graph could be partitioned by member ID ranges or by some natural graph partition (maybe by clusters of network). Each server might host a partition of nodes and all their edges. LIquid’s query planner figures out how to execute multi-step queries across partitions.
- **Active-Active across DCs:** They indicated active-active support for multi-datacenter in Venice; likely LIquid also has a replication story. Possibly each data center runs an instance of LIquid on the full graph (since the graph is heavily interconnected globally, partitioning by geography might not be ideal). They might replicate graph updates (new connections, etc.) via Kafka to all LIquid instances.
- **Updates:** When a user connects with another, that’s an edge insertion in LIquid. That must be done in such a way that subsequent queries see the new connection quickly. Possibly, there’s a small delay (the edge might be written to a persistent store, and LIquid incrementally updates indices). They might batch updates or apply them streaming. Given the QPS emphasis, reads are optimized, and writes likely occur at a lower rate (connections don’t happen as frequently as reads). A specialized pipeline (maybe via Brooklin) feeds LIquid with new edges and attribute updates from the source-of-truth (which could be Espresso or simply LIquid itself if it’s the primary store).

**Use Cases of LIquid:**

- **Degree of Connection:** When you view a profile, LinkedIn shows whether the person is a 2nd or 3rd degree connection and any mutual contacts. LIquid can compute that quickly: find shortest path length between you and them, and fetch mutuals for display. Previously, Cloud did this; now LIquid likely does it more systematically with one query for “friends-of-friends intersection.”
- **People You May Know:** Graph algorithms identify 2nd degree connections with high interaction or shared attributes as candidates. LIquid can very fast enumerate 2nd degree for each user (that’s basically neighbors-of-neighbors) – for each friend, get their friends. That can be huge, so typically they limit to certain criteria (like intersection with similar companies or something). A combination of offline and LIquid online query might be used – offline to generate initial candidates, online to check if they’re still relevant or to fetch additional graph features (like how many mutual connections you have).
- **Connection Recommendations in Page:** On profile pages, “People also viewed” might leverage graph as well (people similar to this person, which often correlates with graph distance).
- **Content Distribution:** LinkedIn might utilize the graph for viral content – e.g., when someone likes a post by a 2nd degree, they need to identify all relevant 2nd degree viewers (like, “I see a post because my connection liked it” – here I’m 2nd degree to the original poster). LIquid can help gather those edges (my connection’s connection = original poster) and ensure proper filtering.
- **Graph Analytics:** For internal analytics, LinkedIn might run graph algorithms via LIquid or via offline. For example, finding the largest connected components (should be one big one given everyone’s connected via some hops), or identifying clusters by industry, etc., to inform product decisions. LIquid’s focus is more on serving, but it could accelerate some analytic queries too (especially if they are bounded).
- **Trust and Safety:** Graph analysis helps detect fake accounts (they often have telltale graph patterns, like connecting to many unconnected people or forming isolated clusters). Real-time graph queries can be used in scoring the trust of new accounts or content – e.g., if an account’s connections are all very low connection count individuals, maybe it's suspicious. This requires quick graph feature extraction which LIquid can provide.

**Graph and Search Intersection:** LinkedIn’s search uses graph data for result personalization (ranking closer connections higher) and filtering (like “1st connections who work at X”). There’s an interesting interplay: the **Galene search engine** could call LIquid for each query result to compute degree if needed, but that might be slow if done naively. More likely, they index the degree-of-connection as part of search index (for the person issuing search, precompute who in the index is 1st/2nd/3rd degree). That precomputation is essentially a graph traversal. Possibly, LinkedIn precomputes for each member a Bloom filter or list of connections to check degree quickly in search. Alternatively, they might integrate search and graph: the “Social distance” can be computed by doing the traversal in parallel with search retrieval. This is complex; maybe they simply filter by connection using a join in the query (like incorporate the list of 1st connection IDs into the search query as a filter or boost).

**GraphQL for Graph?** LinkedIn’s public APIs likely do not expose the full graph for privacy reasons. But internally, they might have GraphQL queries that allow fetching connected entities easily. However, more often they’ll use LIquid’s Datalog directly or a library API.

**Maintaining Graph Consistency:** With 2 million QPS on LIquid ([QCon San Francisco 2023 | LIquid: A Large-Scale Relational Graph Database](https://qconsf.com/presentation/oct2023/liquid-large-scale-relational-graph-database#:~:text=We%20describe%20LIquid,this%20in%20memory%20at%20scale)), consistency is tricky. Suppose two people connect – that edge should appear for both ends. If the write goes to one partition, it must go to the partition for the other user as well. Possibly, they hash by user ID and ensure both IDs belong to the same partition (if they did something like each partition holds a range of IDs, then one connection often falls in two partitions if the IDs are in different ranges). They might solve it by **storing each edge twice** (once keyed by each endpoint). Many graph DBs do that for undirected edges. So each partition is responsible for edges where the “owner” of the edge is within its range. For an undirected connection between A and B, they might designate the lower-ID or something as the owner to avoid duplication, but that complicates retrieval from the other side. More likely: each user’s adjacency list (all connections) is stored with that user as owner. So to add a connection, they add entry to A’s list in partition P and to B’s list in partition Q. That’s two writes (which can be done transactionally if using a distributed transaction or in a eventual consistent manner via an event).

Given LinkedIn’s need for durability, they might still use an underlying store like Espresso or MySQL to hold the ground truth of connections, and LIquid’s in-memory index is built on top of that (so if LIquid restarts, it reloads from the DB). Or LIquid might itself manage persistence (maybe writing to RocksDB or a custom log). They said it’s a complete implementation of relational model, not necessarily that it’s built on an existing RDBMS, but possibly on RocksDB or something.

**Scale Stats:** The QCon note said ~15TB graph, 2M QPS ([QCon San Francisco 2023 | LIquid: A Large-Scale Relational Graph Database](https://qconsf.com/presentation/oct2023/liquid-large-scale-relational-graph-database#:~:text=We%20describe%20LIquid,this%20in%20memory%20at%20scale)), and the blog said 270B edges ([How LIquid Connects Everything So Our Members Can Do Anything](https://www.linkedin.com/blog/engineering/graph-systems/how-liquid-connects-everything-so-our-members-can-do-anything#:~:text=By%20hosting%20LinkedIn%E2%80%99s%20Economic%20Graph%2C,people%20join%20and%20use%20LinkedIn)). 270B edges at 15TB implies ~56 bytes per edge on average stored, which is plausible given some indexing and metadata. 2M QPS likely includes a lot of simple lookups (like retrieving connections of a user, which is done whenever you display mutual connections or list connections on a profile, etc.).

**Graph Traversal Efficiency:** LIquid can do multi-hop queries fast, but 2nd degree of a user with 500 connections who each have 500 could be 250k potential nodes – a bit heavy to ship around. But if asking “who are the mutual connections between X and Y?”, that’s intersection of two sets (neighbors of X ∩ neighbors of Y). If both have 500 connections, that’s easy to compute in memory. If one has 500 and the other 1000, still fine. If one has 10k, maybe a bit more but still okay. LIquid’s indexing likely makes set intersection efficient (maybe via bitsets or sorted lists intersection).

**API for Developers:** LinkedIn’s engineers use LIquid’s Datalog queries to build features. For example, a Datalog query to get 2nd degree connections might look like:

```
SELECT C2
FROM Connections(A, C1) AND Connections(C1, C2)
WHERE A = :viewer AND NOT Connections(A, C2)
```

(This is pseudo-Datalog). The query planner knows how to join these efficiently, likely by expanding neighbors of A, then expanding neighbors of each neighbor and checking.

The introduction of a high-level query language means engineers can ask complex questions of the graph without manually coding graph traversals, which improves productivity when building new features that exploit the Economic Graph.

In conclusion, LinkedIn’s social graph infrastructure, embodied by LIquid, provides a **fast, scalable platform for querying the network of connections and relationships**. It supports both user-facing features (mutual connections, suggestions) and internal logic (like feed relevance or security checks). By investing in a specialized graph database, LinkedIn ensures that even as the network grows, operations on the graph remain performant – crucial for keeping the “degrees-of-separation” features and network-driven recommendations working in real-time.

Next, we will shift from the data and algorithm-heavy aspects of the architecture to the practical side of running all this: how LinkedIn handles continuous integration and deployment of software, containerization, and the strategies they use to manage changes in such a large system.

## CI/CD Pipelines, Containerization, and Deployment Strategies

Operating a platform at LinkedIn’s scale requires robust practices for **continuous integration (CI)** and **continuous deployment (CD)**. LinkedIn’s engineering culture has embraced a move towards true continuous deployment, where code changes are automatically tested and rolled out to production frequently (multiple times a day) ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=As%20an%20engineer%2C%20your%20goal,in%20the%20production%20environment%20automatically)) ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=At%20LinkedIn%2C%20values%20like%20%E2%80%9CMembers,and%20drives%20the%20following%20advantages)). This section discusses how LinkedIn builds, tests, and deploys software, and the infrastructure enabling those processes (including containerization and orchestration).

**Development Ecosystem:** LinkedIn has thousands of microservices and many teams contributing code. To avoid chaos, they’ve built a cohesive development ecosystem:

- **Monorepo vs Polyrepo:** LinkedIn historically used a **monolithic code repository** for many of its services (especially when they were mostly Java, using Ant/Ivy then Gradle). This facilitates code sharing and refactoring across services. It’s likely LinkedIn still has a large monorepo for most backend code, with common libraries used by all services. There may be separate repos for specialized things (front-end code, data pipelines, etc.), but a unified repo is implied by how they manage builds (they can run tests across everything).
- **Automated Builds:** Every commit triggers an automated build and test run. LinkedIn probably uses Jenkins or an internal equivalent at massive scale to handle build pipelines. Given their size, they might have a distributed build system or caching mechanism (Google’s Bazel or similar, or Gradle’s build cache) to speed up compilation/testing.
- **Automated Testing:** They emphasize catching regression defects on every commit via automated tests ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=%2A%20High,maintainable%20products)). This suggests a comprehensive test suite (unit, integration tests). LinkedIn likely invests in **service virtualization** for tests – services might run in an in-memory mode or use stubs so that integration tests can verify inter-service contracts without deploying everything.
- **Quality Gates:** A commit must pass all tests and perhaps static analysis (LinkedIn uses tools for code quality, style, etc.) before it can be merged/deployed. They might also do automated performance testing for critical changes, given how performance is key (like run new code through a stress test environment).

**Continuous Deployment Pipeline:** LinkedIn’s journey to CD was detailed in a 2019 blog ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=As%20an%20engineer%2C%20your%20goal,in%20the%20production%20environment%20automatically)) ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=LinkedIn%20is%20powered%20by%20thousands,to%20our%20members%20and%20customers)). Key elements:

- Each commit that passes tests is a candidate for deployment to production. LinkedIn aims to deploy small increments frequently, rather than big releases.
- They have built internal tools to coordinate deployments across microservices. Possibly an internal **deployment portal** where developers can see the status of their services, push new versions, and monitor the rollout.
- **Canary Releases:** It’s likely LinkedIn employs canarying – deploying a new version to a small percentage of servers or users first. For example, roll out to 1% of traffic in one data center, monitor for errors or metric changes, then roll to more servers.
- **Automated Rollback:** If an issue is detected (error rates up, alarms trigger), the system can rollback to the previous version quickly. This may be manual or auto depending on severity.
- **Feature Flagging:** Many new features are behind **feature flags (aka dynamic configuration)** toggled via LinkedIn’s XLNT experimentation platform. This allows code to be deployed “dark” (inactive) and then gradually enabled for users via config changes, which are much easier to revert than a code deploy. It also separates deployment from release – you can deploy a feature and turn it on later.
- **Deployment Frequency:** LinkedIn aimed to get to the point where they could deploy any service at any time. In practice, some teams might deploy daily or even multiple times a day. The blog suggests “multiple deployments over the course of a day” became routine ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=At%20LinkedIn%2C%20values%20like%20%E2%80%9CMembers,and%20drives%20the%20following%20advantages)), which is a big culture change from older models of weekly or monthly releases.
- **Self-Service and Ownership:** Each team is responsible for their microservice. The CI/CD pipeline automates pushing their changes live, but teams must write tests and monitor their service. LinkedIn encourages engineers to be on-call for their code, which motivates building robust tests and using safe deployment practices.

**Containerization and Orchestration:**

- LinkedIn, for a long time, ran services on VMs or bare metal using a custom deployment system. In recent years, they have embraced **containers (Docker)** as many companies have, for consistency from dev to prod and better resource utilization.
- **Kubernetes or Internal PaaS:** It’s not publicly confirmed, but likely LinkedIn uses Kubernetes or a similar orchestrator to manage microservice deployment. If not Kubernetes, they may have extended Apache Helix/YARN as a container scheduler. There are hints: InfoQ articles about multi-runtime microservices and K8s often mention big companies adopting K8s around 2018-2020. We saw nothing explicit in blogs, but given industry trends, a large portion of LinkedIn services are probably containerized. Microsoft’s influence might also steer towards Kubernetes (AKS) in Azure eventually, but as far as known LinkedIn still runs in its own data centers.
- **Why containers for LinkedIn:** Containers provide environment consistency (works the same on dev laptop or prod host), isolation (packaging dependencies so different services don’t conflict on a machine), and agility (faster spin-up, easier scaling). LinkedIn’s microservices likely each have a Docker image build as part of CI, which is then deployed.
- **Scheduling & Clustering:** If using Kubernetes, they’d have multiple clusters possibly per data center, with tens of thousands of pods running microservices. They would define deployment manifests for each service.
  - If not K8s, they could use an internal scheduler: for example, LinkedIn wrote a paper on a “self-defined programmable data center” and have used Mesos or Helix in context of certain systems (like Samza on YARN). They might have built their own scheduling layer on top of YARN for long-running services (YARN was extended beyond Hadoop in some orgs). Or they might be using Azure Service Fabric due to Microsoft’s acquisition (though Service Fabric is less common outside MS).
- **Infrastructure as Code:** LinkedIn probably employs config-as-code for defining service deployment settings (CPU, memory requirements, etc.). This integrates with their CI pipeline – e.g., a service’s repo might contain a deployment descriptor that the pipeline uses to deploy it to test and prod.

**Multi-Data Center Deployment:**

- LinkedIn must deploy across multiple data centers. Typically, they do staged rollouts: update one DC at a time to avoid global impact.
- **No downtime:** They use rolling deployments so that not all instances of a service go down at once. Load balancers (or D2 client discovery) take nodes out as they update and route traffic to remaining nodes.
- **Data consistency during deploy:** With multi-master databases like Espresso, deploying a new version that changes schema requires coordination. LinkedIn likely has robust schema evolution processes (backward-compatible changes, dual-reading old and new until cutover).
- **Traffic draining:** They probably drain traffic from a server before updating it (especially for stateful components). For stateless ones, just killing and restarting container might suffice if LB health checks are in place.

**Tooling and Dashboard:**

- LinkedIn likely has internal dashboards to monitor deployment status (which services are at which version, any deployments in progress, etc.). Possibly an internal tool akin to Spinnaker or their own UI.
- They also integrate deployment with metrics – a deployment might automatically start extra monitoring or compare metrics from before/after deployment.
- They mentioned values like “Members first” and “Intelligent risk” guiding them to do CD but carefully ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=At%20LinkedIn%2C%20values%20like%20%E2%80%9CMembers,and%20drives%20the%20following%20advantages)). So likely every deploy is monitored against key member experience metrics; if sign-in drops or latency rises, that’s a red flag.

**Blue-Green Deploy or Rolling?:** They likely do **rolling deploys** in small batches rather than full blue-green (which requires double capacity). Possibly for some critical systems they might do blue-green if needed, but at their scale doubling capacity is expensive. Rolling with canaries is enough usually.

**Continuous Integration of Data Pipeline Code:** It’s worth noting CD at LinkedIn extends beyond just web services. Data pipeline code (Spark jobs, Samza jobs) also is continuously integrated and deployed. For Samza, a new job version can be rolled out in YARN similarly. They might even treat data flows as versioned “services” in terms of config. This holistic approach means a commit that affects both online and offline components can be rolled out in sync, reducing mismatch issues.

**Developer Productivity and Release Cycle:** By automating deployment, LinkedIn achieved higher developer productivity – engineers can focus on coding and writing tests, and rely on the platform to get their code live quickly and safely ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=the%20components%20of%20our%20development,in%20the%20production%20environment%20automatically)) ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=At%20LinkedIn%2C%20values%20like%20%E2%80%9CMembers,and%20drives%20the%20following%20advantages)). This shorter feedback loop (code to production in hours instead of weeks) also enables faster experimentation and fixes. Their blog touted improved velocity and safer releases (small changes mean if something goes wrong, it’s easier to pinpoint and revert) ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=%2A%20High,maintainable%20products)) ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=and%20provides%20early%20feedback%20on,as%20the%20pipeline%20is%20automated)).

**Containerization Impact:**

- LinkedIn’s use of containers also makes local development easier (simulate a stack in Docker), and environment parity reduces “works on my machine” problems.
- It also helps in scaling infrastructure – they can bin-pack multiple containers on the same VM to utilize resources effectively, guided by their performance tuning knowledge (their engineering blog often touches on JVM performance, GC tuning ([Technical Talks - Kafka at Scale: Multi-Tier Architectures](https://engineering.linkedin.com/kafka/technical-talks-kafka-scale-multi-tier-architectures#:~:text=Architectures%20engineering,them%20more%20productive%20and)), etc., and containerization requires ensuring those tunings still apply in cgroups).
- Possibly, they also use containers to run multiple versions of a service concurrently (for example, to do shadow traffic testing – sending a copy of traffic to a new version running alongside the current one to verify behavior before switching it live).

**Mobile Deployment:** Not directly CI/CD, but LinkedIn’s mobile apps (client side) have their own release cycle (App Store updates). To mitigate the slower mobile update cycle, LinkedIn relies heavily on feature flags and server-driven UI configurations in the app. That way, they can enable/disable features without forcing users to update the app. LinkedIn might also use techniques like **binary patching** or **instant updates** for some content in the app (though App Store rules limit code push). This is less about system architecture and more about product release strategy.

In summary, LinkedIn’s CI/CD and deployment infrastructure is built to **handle rapid, reliable releases across a vast microservice landscape**. Automation is key: from testing every commit to automatically rolling it out and verifying it in production. Containerization and (presumably) Kubernetes help manage the complexity of deploying so many services consistently. By achieving continuous deployment, LinkedIn can iterate quickly on features and improvements, which is essential in a competitive landscape.

With code deployment covered, we now look at how LinkedIn ensures everything runs smoothly in production – through observability, monitoring, and incident response.

## Observability, Monitoring, and Incident Response

At LinkedIn’s scale, having strong observability is non-negotiable. Millions of events per second and thousands of services mean that traditional manual monitoring won’t cut it – they need automated, intelligent systems to detect anomalies, alert engineers, and help them diagnose issues. LinkedIn has developed a multi-layered **monitoring and alerting ecosystem**, including metrics, logs, traces, and specialized tools like **ThirdEye** for anomaly detection ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=At%20LinkedIn%2C%20we%20have%20many,solve%20this%20problem%2C%20we%20created)) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)).

**Metrics and Monitoring Systems:**

- LinkedIn collects a **wide spectrum of metrics** from its applications: system metrics (CPU, memory, GC times), service metrics (request rates, latencies, error counts), business metrics (signups, clicks, job applications), etc. These metrics are generated in all parts of the stack (services, mobile apps, data pipelines).
- The metrics pipeline we discussed under Kafka is central: metrics are emitted continuously (often via a library like Yammer Metrics or similar), buffered and sent (perhaps to Kafka via an agent), and then aggregated.
- Historically, LinkedIn used **InGraphs** (internal metrics platform) to store time-series metrics data. The mention of “ingraphs” ([Model health assurance platform at LinkedIn](https://www.linkedin.com/blog/engineering/analytics/model-health-assurance-at-linkedin#:~:text=Model%20health%20assurance%20platform%20at,monitors%20drift%20in%20prediction)) suggests an internal time-series store for operational metrics, possibly backed by a time-series database or a combination of Kafka + Samza + Espresso/Relational DB. They may have integrated it with Pinot for slicing and dicing, or they may use something like OpenTSDB.
- Metrics are visualized on dashboards. LinkedIn likely has a UI for engineers to create graphs of various metrics, set thresholds, etc. They might use Grafana or a custom UI on top of their time-series store.
- For example, a dashboard for the Feed service might show the number of feed requests, the p99 latency, the number of posts generated per minute, etc., across data centers. SREs and developers use these for real-time monitoring and historical analysis.

**ThirdEye – Anomaly Detection and Analytics:**

- **ThirdEye** is LinkedIn’s unified monitoring platform introduced to correlate metrics across domains ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=At%20LinkedIn%2C%20we%20have%20many,solve%20this%20problem%2C%20we%20created)). It provides **real-time anomaly detection** on metrics and a UI for root cause analysis ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=by%20one%20system%20will%20go,this%20problem%2C%20we%20created%20ThirdEye)) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)).
- ThirdEye ingests metrics from many sources (including InGraphs and business dashboards) and applies anomaly detection algorithms (e.g., seasonality models, thresholds, machine learning) to flag unusual deviations ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=At%20LinkedIn%2C%20we%20have%20many,solve%20this%20problem%2C%20we%20created)) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)). For instance, if signups drop by 30% suddenly or if profile view count spikes abnormally, ThirdEye would catch that.
- It then allows interactive analysis: LinkedIn uses **Pinot** under the hood of ThirdEye to slice metrics by dimensions (e.g., by region, by app version) to help find root causes ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)). For example, if overall page views dropped, Pinot might reveal it’s mostly from a specific country -> could indicate a regional outage or ISP issue.
- ThirdEye is **collaborative**: multiple teams can see and comment on anomalies, linking to known issues or correlating with deployments. This reduces the time to identify the cause of problems that span subsystems.
- It monitors both system metrics and high-level business metrics in one place ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=At%20LinkedIn%2C%20we%20have%20many,solve%20this%20problem%2C%20we%20created)), bridging the gap between “the site is up” and “the site is healthy from a user perspective”.
- ThirdEye was open sourced and can be used generally for anomaly detection. It leverages LinkedIn’s data infrastructure, showing how they bring together analytics (Pinot’s OLAP querying) with monitoring needs ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)).

**Logging and Distributed Tracing:**

- Every service likely logs to a central system. LinkedIn’s logging might be built on a combination of Kafka and Hadoop (for storage and analysis). Logs are crucial in debugging issues after the fact or tracing them in real-time.
- **Log indexing/search:** Possibly uses Elasticsearch or Solr in an ELK-like stack, or maybe Splunk, to search logs quickly when investigating (though managing that at LinkedIn’s volume is challenging, they might instead parse logs into structured events and analyze with Hadoop/Pinot). Even if not full-text indexing, they at least collect logs by request IDs.
- **Request Tracking:** LinkedIn likely passes a **correlation or trace ID** through calls (especially via Rest.li and D2). This allows them to reconstruct a request’s path through microservices. They built an internal tool (mentioned as “call graphs that profile requests in real time” – possibly referring to an earlier version of distributed tracing) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Software%20engineers%20from%20the%20social,away%20from%20a%20Lambda%20architecture)).
  - There’s mention of “Real-time distributed tracing for website performance using InCapacity” ([Real-time distributed tracing for website performance and efficiency ...](https://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency#:~:text=Real,graphs%20that%20profile%20requests)) – likely an internal tracing system to capture the call graph and timing for random samples of requests. This helps see where time is spent in a multi-service call (like feed generation spanning services).
  - If not a full-fledged open-source tracer (like Jaeger or Zipkin), they have something custom that logs each service call with a trace ID and time, and then tools (like ThirdEye or others) to piece these together.
  - Distributed traces help pinpoint if a slowdown is due to one service or if it’s network, etc., which is key in an architecture with hundreds of dependencies per request.
- **Alerts and On-Call:**
  - LinkedIn’s monitoring is tied to an alerting system. They likely use something like PagerDuty or a custom paging system. Possibly they have their own on-call management tool (some companies like Pinterest open sourced “OpsGenie” or similar; but it’s likely they integrate with common solutions).
  - Alerts are set on key metrics (error rates, latency, throughput, as well as business KPIs). If an anomaly is detected by ThirdEye (like a metric deviating significantly), it can create an incident alert if above certain severity.
  - Each service has defined SLOs (service level objectives) and if they’re violated (e.g., error rate > X% for Y minutes), on-call engineers are paged.
  - LinkedIn’s culture of “continuous deployment” means incidents might be triggered by code deploys; thus their alerting likely correlates with deployment events. Possibly the deployment system automatically does a quick check of key metrics post-deploy, and if it notices a jump, it might auto rollback or alert.
- **Incident Response:**
  - LinkedIn likely has a **global Network Operations Center (NOC)** or similar that monitors major site health indicators 24/7. But also each team rotates on-call for their services. When something goes wrong, relevant teams join a bridge.
  - They likely follow industry best practices: run **post-mortems** for incidents, root cause them (with ThirdEye and log analysis support), and feed those learnings back into improving detection or prevention.
  - They have internal tooling for incident management (tracking, communications). Possibly an internal “Incident Dashboard” linking all anomalies, ongoing incidents, recent deploys, etc., to speed up figuring out what went wrong.

**Security Monitoring:** Under Trust & Safety, they also monitor unusual patterns related to security (e.g., bursts of account login failures, scraping activity). ThirdEye can flag certain metrics (like traffic spikes hitting certain endpoints, or suspicious patterns of new accounts). They likely integrate with specialized security systems too, but at metric level many security events look like anomalies.

**User Experience Monitoring:** They gather data from the client side as well – e.g., **RUM (Real User Monitoring)** via the JS in the site or the app sending timing info, to measure actual page load times in the field and errors that might not be seen server-side (like a JavaScript exception). This flows into their monitoring so they see client-side issues (e.g., a buggy front-end release causing high JS errors).

**Capacity and Performance Management:** Observability also means planning capacity:

- LinkedIn tracks usage trends of each service to forecast when to add more instances or hardware. They use metrics like CPU utilization and queue times. Possibly an internal tool (maybe “InCapacity” referenced) helps simulate or recommend capacity adjustments.
- They do **load testing** in staging environments to validate that new changes don’t degrade performance. Tools might replay production traffic to a staging cluster (with logs or Kafka events) to see how new code handles it.
- **GC and JVM tuning:** They’ve written about garbage collection optimizations for high-throughput services ([Garbage Collection Optimization for High-Throughput and Low ...](https://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications#:~:text=Garbage%20Collection%20Optimization%20for%20High,systematic%20method%20to%20tame)). They likely have profiling in production that samples things like GC pauses, heap usage, etc., aggregated to see if any service version has memory leaks or issues.

**Open-Source Monitoring Tools:** LinkedIn open-sourced ThirdEye ([ThirdEye - LinkedIn's Business-wide monitoring platform - SlideShare](https://www.slideshare.net/slideshow/thirdeye-linkedins-businesswide-monitoring-platform/178412377#:~:text=ThirdEye%20,cause%20analysis%20across%2050%2B)). They also have contributed to projects like **Burrow** (a Kafka consumer lag monitor) which they built to monitor that Samza and other consumers are keeping up with Kafka. They use **Cruise Control** to automatically fix Kafka partition skew without human intervention ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=,usage%20monitor%20called%20%E2%80%9CBean%20Counter%E2%80%9D)). These kinds of tools remove a lot of manual ops overhead.

**Dashboard Integration:** They probably unify dashboards such that any engineer can see their service health at a glance – error rate, 99th latency, request rate, dependency health (are all your downstreams up?), and recent deploys. This holistic view can be critical in debugging an issue quickly (for instance, if a dependency is down, your errors spike – a good dashboard immediately highlights that correlation).

**Response times:** Their monitoring systems are real-time or near real-time. ThirdEye is described as **real-time monitoring** ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)). Likely data is available to alert on within seconds to a minute. This quick detection plus automated rollback means incidents can sometimes be resolved before many users notice (e.g., if a bad deploy goes out at midnight and triggers errors, by 00:05 it might be rolled back automatically due to alerts).

**Communication:** For widespread incidents (site issues), LinkedIn’s incident response might include internal comms and updating a status page for external awareness if needed. While not specifically architectural, it's part of running a large site.

In summary, LinkedIn’s observability architecture consists of:

- **Metrics collection and storage** (InGraphs, Pinot).
- **Automated anomaly detection** (ThirdEye) for catching issues across system and business metrics ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)).
- **Logging and tracing** for deep diagnosis of requests and issues.
- **Alerting and incident management** integrated with these systems to promptly involve engineers.
- **Continuous improvement** by analyzing incidents and adding new monitors or safeguards (fitting with their “Members first” value by minimizing downtime impact ([Learnings from the journey to continuous deployment](https://www.linkedin.com/blog/engineering/optimization/learnings-from-the-journey-to-continuous-deployment#:~:text=At%20LinkedIn%2C%20values%20like%20%E2%80%9CMembers,and%20drives%20the%20following%20advantages))).

This robust monitoring framework allows LinkedIn to run a complex system with confidence, catching problems early and understanding system behavior thoroughly. Next, we’ll touch on security and compliance architecture, which ensures that LinkedIn’s data and services remain secure and adhere to regulations.

## Security Architecture and Compliance

Security and compliance are critical aspects of LinkedIn’s architecture, given the sensitive personal data it holds and global regulations it must follow. LinkedIn’s security architecture is multi-layered, addressing application security, data protection, and compliance monitoring. While specific implementations are often internal, we can outline the main strategies LinkedIn employs.

**Application and Network Security:**

- **Perimeter Defense:** LinkedIn’s data centers likely have strong network segmentation. Services are inside a protected network; at the edge they use Apache Traffic Server and HAProxy not just for performance but also for security (throttling, filtering malicious patterns) ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Beyond%20the%20application%20code%2C%20we%E2%80%99ve,server%20side%20rendering%2C%20and%20more)). Firewalls and DDoS protection (potentially via parent company Microsoft’s infrastructure) guard against external attacks.
- **Service Authentication and Authorization:** Internal service-to-service calls may use signed tokens or mutual authentication. LinkedIn could use an internal PKI to issue certificates for each service instance (so that only authorized services communicate). They might also leverage something like a service mesh (if they use Kubernetes, Istio might provide mTLS, but not sure if they do). They ensure that, for example, only the profile service can update profile data, etc., by partitioning privileges.
- **Public API Security:** LinkedIn’s public APIs (mostly for partners, or the older REST API) require OAuth 2.0 and have strict permission scopes. Rate limiting is enforced at the API gateway.
- **Data Encryption:** Sensitive data (like passwords) is hashed (LinkedIn uses strong hashing, probably bcrypt or similar). Data at rest in databases is likely encrypted (especially PII fields), at least in backups or certain storage. They said “data encryption” is an industry best practice they follow ([[PDF] Information Security FAQs for LinkedIn and Its Services](https://legal.linkedin.com/content/dam/legal/LinkedIn-Information-Security-Packet-2022.pdf#:~:text=,physical%20security%2C%20and%20secure)).
- **Communication Encryption:** All external traffic is over HTTPS/TLS. Internally, they likely also encrypt sensitive channels. Since they own the network, some internal traffic might be plain for performance, but any cross-DC traffic is surely encrypted via TLS or VPN tunnels.
- **Secure Development Lifecycle:** LinkedIn trains developers on secure coding. They have static code analysis scanning commits for known vulnerability patterns. They also do regular security reviews for new features and use external penetration testing. The Black Hat talk reference ([Debrief from Black Hat - The Tactical Application Security Program](https://engineering.linkedin.com/blog/2015/08/debrief-from-black-hat-the-tactical-application-security-program#:~:text=Program%20engineering,%C2%B7%20Be%20Consultative)) suggests an advanced internal security program focusing on tactical improvements (like quickly patching libraries, scanning for vulnerabilities in dependencies, etc.).
- **Dependency Management:** They track open source libs for CVEs. Possibly integrated with build to warn if a library has a known vulnerability and needs an update.

**Compliance (Data Privacy) Architecture:**

- LinkedIn has to comply with GDPR, CCPA, etc., which mandate things like data erasure on request, purpose limitation, etc. Their **Data Management and Compliance platform** (CMON) is designed for this ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=how%20we%20protect%20member%20data,that%20we%20store%20and%20analyze)) ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=with%20security%2C%20legal%2C%20and%20executives,our%20commitment%20to%20our%20members)).
- **CMON (Compliance Monitoring) Platform:** As described, CMON scans all systems for compliance issues ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=LinkedIn%20uses%20Kafka%2C%20Espresso%2C%20Hadoop%2C,our%20commitment%20to%20our%20members)). For example, it can detect if a dataset contains PII it shouldn’t, or if data marked for deletion still persists. It aggregates metadata about data: who owns it, what PII it contains, retention policy, etc. ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=The%20CMON%20platform%20leverages%20an,system%20behaviors%20operate%20as%20expected)). It likely ties into a central metadata catalog (like a data dictionary) for all datasets.
- **Data Access Controls:** Access to user data is restricted by job function. LinkedIn probably uses an internal system to manage which services or users (employees) can query certain data. For example, an engineer may not directly query production user data unless authorized for a specific purpose. Production data access might be logged and audited via CMON.
- **Data Deletion and Subject Rights:** When a member deletes their account or requests data export, LinkedIn’s systems coordinate to fulfill this:
  - They likely have a **master data deletion service** that issues delete or anonymize commands to all relevant services (profile, connections, posts, messages, etc.). Integration with CMON helps know all places the user’s data lives ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=how%20we%20protect%20member%20data,that%20we%20store%20and%20analyze)) ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=with%20security%2C%20legal%2C%20and%20executives,our%20commitment%20to%20our%20members)). Data might be “hard deleted” or “anonymized” (like replace name with placeholder but keep content for aggregate stats).
  - They meet timelines mandated by laws by automating this across the platform.
  - Data portability (export) is also automated, collecting data from various services and compiling it.
- **Privacy by Design:** LinkedIn’s architecture for new features includes privacy reviews to ensure minimum necessary data is used and stored.
- **Encryption & Key Management:** They likely encrypt sensitive data at rest. For example, messages or InMail content might be encrypted on disk to reduce risk if disks are stolen or an insider tries to access raw DB. Key management could be done through an internal HSM or Azure Key Vault (if integrated with MS).
- **Monitoring for Compliance:** CMON doesn’t just statically scan; it appears to run continuously, raising alerts if it finds unexpected data or if a service is non-compliant ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=The%20CMON%20platform%20leverages%20an,system%20behaviors%20operate%20as%20expected)) ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=Our%20approach%20is%20a%20unique,informed%20and%20prevent%20issues)). E.g., if a service collects a new type of personal data and doesn’t have a retention policy, CMON flags it. It enforces integration – new engineering efforts must integrate with CMON’s pipelines ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=Our%20approach%20is%20a%20unique,informed%20and%20prevent%20issues)), indicating compliance is a required aspect of launching new features.
- **Auditing:** They produce audit logs for data access. Possibly ThirdEye monitors access patterns too – e.g., if an internal tool suddenly requests a lot of profile data, that’s flagged for possible misuse.
- **Secure Data Architecture:** The data itself is architected to isolate especially sensitive data. For instance, LinkedIn might separate “identity data” (name, email) from “activity data” (posts, likes) in different stores, limiting which services have access to both, thereby reducing risk of a single compromise yielding everything.

**User Security Features:** LinkedIn provides features like two-factor authentication to users, which means their auth architecture supports TOTP/SMS 2FA. The authentication service is integrated with their web and mobile clients. Additionally, they track account logins (ThirdEye could alert unusual login locations – probably not directly but by feeding into a risk engine).

- They likely have a **risk assessment service** that, when a login happens, computes risk (based on IP, device, velocity, etc.) and might challenge the user or notify if suspicious. That service is fed by data from across LinkedIn (past logins, global threat intel).

**Infrastructure Security:** They also secure the infrastructure:

- Regular OS patching (which continuous deployment can help with, by recreating containers on updated base images).
- Limiting admin access – using jump hosts, multi-factor for engineers to access servers, etc.
- Possibly employing container security scanning (checking Docker images for vulnerabilities).

**Microsoft Integration:** Since being owned by Microsoft, LinkedIn likely gained access to some of Microsoft’s security tech (like advanced threat analytics, etc.), but they operate somewhat independently. However, data cross-sharing would be heavily controlled due to privacy commitments (LinkedIn has separate privacy policies from Microsoft). There were statements that LinkedIn runs largely isolated from Microsoft’s cloud, although integration might slowly increase.

**Operational Security:**

- LinkedIn’s security team runs regular drills (maybe chaos engineering style for security, or “red team” exercises).
- The mention of “tactical security program” ([Debrief from Black Hat - The Tactical Application Security Program](https://engineering.linkedin.com/blog/2015/08/debrief-from-black-hat-the-tactical-application-security-program#:~:text=Program%20engineering,%C2%B7%20Be%20Consultative)) implies they integrate security tasks into daily development, e.g., security champions in dev teams, and focusing on practical quick-wins (like fixing vulns, providing secure frameworks so devs don’t implement auth incorrectly, etc.).

**Compliance certifications:** LinkedIn likely achieves certifications (SOC2, ISO27001) which require architectural controls and documentation of all these processes. The architecture is designed to produce evidence (logs, audit trails) needed for such compliance.

In essence, LinkedIn’s architecture incorporates **security and privacy by design**:

- Data is protected via encryption, access control, and rigorous monitoring.
- Services authenticate and authorize interactions, minimizing overexposure of data.
- Compliance is automated using the CMON platform that provides unprecedented visibility into data usage and ensures adherence to policies ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=how%20we%20protect%20member%20data,that%20we%20store%20and%20analyze)) ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=with%20security%2C%20legal%2C%20and%20executives,our%20commitment%20to%20our%20members)).
- The environment is instrumented to detect and respond to threats, with anomalies in usage triggering investigations.
- The combination of preventive measures (like encryption, least privilege) and detective measures (like ThirdEye anomaly detection on security metrics and CMON compliance scanning) helps LinkedIn maintain member trust and meet regulatory requirements ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=how%20we%20protect%20member%20data,that%20we%20store%20and%20analyze)) ([Data Management and Compliance](https://engineering.linkedin.com/teams/data/analytics-platform-apps/big-data-engineering/data-management#:~:text=track%2C%20annotate%2C%20and%20aggregate%20metadata,that%20we%20store%20and%20analyze)).

Having covered security and compliance, we’ll proceed to LinkedIn’s experimentation and A/B testing infrastructure, which allows them to innovate and optimize features scientifically.

## A/B Testing Infrastructure and Experimentation Platform

LinkedIn attributes much of its product evolution to a robust **experimentation culture**, enabled by its internal A/B testing platform known as **XLNT** (pronounced “Excellent”) or more recently **T-REX** (Targeting and Ramping EXperimentation) ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=At%20any%20given%20time%2C%20LinkedIn%E2%80%99s,a%20notification%20system%2C%20and%20a)) ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=Operation%20at%20such%20a%20scale,and%20a%20seamless%20UI%20experience)). This platform is deeply integrated into LinkedIn’s architecture, allowing teams to test new features, algorithms, or UI changes on subsets of users and measure the impact on key metrics in a scientifically valid way.

**Scale of Experimentation:** LinkedIn runs an enormous number of experiments concurrently – up to **41,000 A/B tests at the same time on over 700 million members** ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=At%20any%20given%20time%2C%20LinkedIn%E2%80%99s,a%20notification%20system%2C%20and%20a)). This scale is staggering and necessitates a very systematic approach:

- Experiments vary from small UI tweaks to major algorithm changes. Many are minor (hence the large number), e.g., testing copy text or button color in certain flows, while others are large like a new recommender system.
- The platform ensures users can be in multiple experiments without interference (if tests are unrelated) and avoids users being in conflicting experiments (if two tests target the same outcome or UI, they need coordination).

**Experimentation Platform (XLNT/T-REX) Architecture:**

- **Targeting Service:** A key component is deciding who sees which variant. LinkedIn’s platform includes a **targeting system** where experiments can define target criteria (e.g., “20% of English-speaking members in North America who use iOS app”). The targeting service evaluates each user against active experiments to assign them consistently. It likely uses hashing of user IDs for random assignment to ensure statistical evenness.
- **Experiment Activation:** Within the application code, experiments are accessed via feature flags or experiment frameworks. For example, in code:
  ```java
  if (ExperimentManager.isVariant("NewFeedRanking", "TreatmentA")) {
      // use new algorithm
  } else {
      // use old
  }
  ```
  The ExperimentManager consults the user’s assignments (which might be cached or fetched from an experimentation service) to return true/false or variant identity. This lookup has to be very fast (sub-millisecond) and available for every request. Possibly they push experiment assignments to an in-memory cache at edge or in a cookie.
- **Ramping and Phases:** T-REX stands for Targeting, Ramping, and Experimentation. **Ramping** means gradually increasing the audience. The platform likely automates ramp-ups: e.g., start with 1%, then 5%, 10%, ... to mitigate risk. The system monitors metrics as it ramps; if something bad is detected, it halts ramp or aborts the experiment.
- **Dynamic Configuration:** The experimentation platform doubles as a **feature flag system**, providing dynamic config and the ability to turn things on/off quickly (particularly for kill-switching features causing issues).
- **Metrics and Analysis Pipelines:** The success of an A/B test depends on measuring metrics. LinkedIn’s platform automatically tracks a standard set of metrics for every experiment (user engagement, clicks, etc.) and also allows experiment owners to specify custom metrics.
  - This ties into their data pipeline: user actions are tagged with experiment IDs (e.g., the tracking events in Kafka have fields indicating which variant of which experiments the user was in during that action) ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=match%20at%20L473%20FollowFeed%20performs,input%20to%20the%20relevance%20algorithms)). This allows attribution of outcomes to experiments.
  - They likely use Hadoop or Pinot to aggregate experiment results. Given 41k concurrent experiments, manual analysis is impossible, so the platform has an **automatic analysis engine** that computes statistical significance for each metric per experiment, generating results dashboards ([XLNT Platform: Driving A/B Testing at LinkedIn](https://engineering.linkedin.com/ab-testing/xlnt-platform-driving-ab-testing-linkedin#:~:text=XLNT%20allows%20for%20easy%20design,crucial%20in%20popularizing%20A%2FB%20tests)). The platform probably highlights significant wins or losses and may even auto-stop experiments that are clearly negative.
  - The platform likely supports advanced experimental designs: multi-variant tests, ramp holdouts (to see long-term effects), and perhaps sequential testing methods to reach significance faster.
- **User Layering and Experiment Hierarchy:** With so many experiments, LinkedIn uses “layering” or “streams” to avoid collision. Often they segment experiments by domain (e.g., feed experiments vs messaging experiments) so that one user can be in many experiments as long as those experiments are in different product areas and don’t interact. They documented how the platform evolved to handle growth: from simple systems to a complex platform that includes configuration, notification, etc. ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=Operation%20at%20such%20a%20scale,and%20a%20seamless%20UI%20experience)) ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=delivery%20system%20with%20a%20UI,and%20a%20seamless%20UI%20experience)).
- **Notification and UI:** There’s a UI for product managers and engineers to set up experiments (choose target, set variants, define metrics) and to monitor results. The system automates a lot so experimenters can focus on hypothesis and results.
- **Integration with Deployment:** New code can be dark-launched and only turned on via experiment flag for a small percent. Also, experiments often coincide with a deploy of the test feature – the platform might integrate with CI to ensure the code is out before enabling the flag. Possibly link to deployment: e.g., gradually ramping an experiment might be similar to gradually ramping a deployment, so they could coordinate those (but usually, the code is fully deployed, just disabled/enabled per user via the flag, which is simpler).
- **Experimentation Culture:** The platform and culture allow any feature change to be tested. This encourages data-driven decisions rather than HIPPO (highest paid person’s opinion). Engineers can run low-risk experiments easily. However, such heavy experimentation can risk “metric fixation,” so LinkedIn’s approach includes running long-term experiments and being careful about metric interpretation.

**Use Cases of Experiments:**

- Launching a new homepage algorithm: test it on 1% vs old to ensure it improves engagement.
- UI redesign: roll out new look to 5% and compare interactions.
- Email frequency: test sending more emails vs less to see impact on retention.
- Infrastructure changes: sometimes even infrastructure updates might be tested (e.g., try a new compression algorithm for images and see if page load time improves and if user engagement is affected).
- Monetization tweaks: like pricing changes or ad ranking changes A/B tested to maximize revenue without hurting user metrics.

**Scale Considerations:** Serving experiment assignments for every user across web and mobile might require a distributed key-value store of user->variant mapping. Possibly they generate assignments using a pure deterministic function (so it’s stateless): e.g., hash(user + experimentId) modulo 2 = variant. Many A/B systems do this (it guarantees a user remains in the same variant and yields desired ratio). For targeting beyond random selection, they might filter user attributes first, then hash for assignment. It could be done on the fly in the app or at request time, which avoids storing assignments at scale. The advantage is no storage needed, the drawback is less flexibility (can’t easily reassign individuals after start, etc.). Likely LinkedIn’s system is advanced enough to handle persistent assignments especially for long-running experiments or those that need specific grouping (e.g., group certain users together).

- Given mention of "automatic analysis crucial in popularizing A/B tests" ([XLNT Platform: Driving A/B Testing at LinkedIn](https://engineering.linkedin.com/ab-testing/xlnt-platform-driving-ab-testing-linkedin#:~:text=XLNT%20allows%20for%20easy%20design,crucial%20in%20popularizing%20A%2FB%20tests)), the platform’s analysis is highly automated. They have a **statistics engine** presumably using sequential testing or t-tests, etc., with large N, significance is usually reached quickly (LinkedIn’s traffic is huge).
- It also likely accounts for novel experiment designs (multi-arm bandits, etc.) for things like recommendations where they continuously explore.

**T-REX Evolution:** The 2020 blog series shows how the platform grew with the company:

- Early on it was a simple “LiX” framework with a UI, then it expanded to handle **dynamic config** (targeting beyond random splits), then to **Insight and reporting pipelines** and **notification systems** ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=demand%20and%20external%20forces%20have,and%20a%20seamless%20UI%20experience)) ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=delivery%20system%20with%20a%20UI,and%20a%20seamless%20UI%20experience)).
- They mention **external forces** like GDPR that could require excluding users from certain tests or ensuring informed consent if needed (some experiments might need a notice).
- The platform has become central to release – features roll out via T-REX by default.

**Integration with Member Experience:** If a variant turned out to be poor, users in that variant might have a suboptimal experience until it’s turned off. The platform mitigates harm by ramping slowly, and ethically, they likely stop experiments that degrade experience significantly beyond a threshold. For positive changes, experiments justify rolling out improvements to everyone.

**Learning and Sharing:** The results of experiments are captured in their data warehouse. LinkedIn can analyze trends across experiments (like what kinds of changes typically improve engagement, etc.). This builds organizational knowledge. Possibly they feed aggregated learnings to an AI to suggest experiment ideas (some companies are doing that).

**Continuous Experimentation:** With continuous deployment, continuous experimentation is a complement. Every change is ideally an experiment as well. LinkedIn’s platform likely encourages small experiments frequently, which combined with CD gives an engine for rapid improvement.

In essence, LinkedIn’s experimentation platform is a **sophisticated, scalable system** that handles:

- User targeting and random assignment at massive scale ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=At%20any%20given%20time%2C%20LinkedIn%E2%80%99s,a%20notification%20system%2C%20and%20a)).
- Automated ramping and safe rollout (like a traffic router for features).
- Data collection and automated statistical analysis for quick feedback ([XLNT Platform: Driving A/B Testing at LinkedIn](https://engineering.linkedin.com/ab-testing/xlnt-platform-driving-ab-testing-linkedin#:~:text=XLNT%20allows%20for%20easy%20design,crucial%20in%20popularizing%20A%2FB%20tests)).
- Integration with feature flagging such that code can be toggled on/off per user seamlessly.
- Running tens of thousands of concurrent tests without collisions by structuring experiments and user segments carefully ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=At%20any%20given%20time%2C%20LinkedIn%E2%80%99s,a%20notification%20system%2C%20and%20a)) ([Our evolution towards T-REX: The prehistory of experimentation infrastructure at LinkedIn](https://www.linkedin.com/blog/engineering/ab-testing-experimentation/our-evolution-towards-t-rex-the-prehistory-of-experimentation-i#:~:text=delivery%20system%20with%20a%20UI,and%20a%20seamless%20UI%20experience)).

This enables LinkedIn to optimize everything from UI to algorithms in a data-driven way, and was key to many incremental improvements that compound into a significantly better product.

Finally, after covering experimentation, we will wrap up by summarizing how open-source technologies have underpinned many of these architecture components and how LinkedIn’s contributions have influenced the broader industry.

## Public and Internal APIs (Including GraphQL)

LinkedIn provides a set of **public APIs** for partners and developers, and also uses APIs internally between clients and services. Over time, the style of APIs has evolved from custom REST and RPC calls to more standardized approaches like **Rest.li** (their REST+JSON framework) and recently **GraphQL** for certain use cases. This section explores LinkedIn’s API architecture both for internal consumption and external integration.

**Internal APIs – Rest.li and Microservice APIs:**

- As discussed, LinkedIn built **Rest.li** as a RESTful service framework at scale ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=When%20we%20transformed%20from%20Leo,Restful%20API%20model%20%2028)). Internally, nearly all service-to-service communication uses Rest.li’s JSON/HTTP APIs. These are well-defined resource-oriented endpoints. Rest.li abstracts details like query formats, pagination, batching, and error handling, so developers have a consistent way to build APIs ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=this%2C%20we%20built%20out%20a,Restful%20API%20model%20%2028)).
- **Dynamic Discovery (D2):** Instead of hardcoding API endpoints, clients use D2 which finds where a service is running ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)). For developers, calling an API is just like calling a local proxy – the framework hides the network lookup.
- **Thrift/Protobuf:** While JSON is primary, for some high-performance internal calls LinkedIn might use binary protocols. Rest.li initially was JSON, but I recall it could also support binary (via projections or rest codecs). They also use Avro for data in Kafka, but for real-time APIs likely JSON is fine given modern hardware.
- **Scale of internal API calls:** We saw that by 2015, LinkedIn had **100 billion Rest.li calls per day** ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Today%2C%20LinkedIn%20has%20over%20975,day%20across%20all%20our%20datacenters)). These calls can be cross-service or from frontend to mid-tier. The consistency of using Rest.li for everything means that whether a call is coming from web front-end to a mid-tier or from one backend service to another, it’s handled similarly, easing debugging and development.
- **API Governance:** With nearly a thousand resources by 2015 ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Today%2C%20LinkedIn%20has%20over%20975,day%20across%20all%20our%20datacenters)), LinkedIn probably has API governance to maintain backwards compatibility. Rest.li encourages adding new fields without breaking old clients (using optional fields and default behaviors).
- **GraphQL internally?:** Internally, LinkedIn did not initially use GraphQL widely because Rest.li served the need. However, there might be some adoption now for specific cases (for example, for new mobile client development, GraphQL can reduce round trips – though Rest.li supports batch requests, GraphQL might allow more flexibility). The GraphQL blog though was about external APIs for partners ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)), implying internal microservices still primarily use Rest.li or gRPC. They might be exploring gRPC in some contexts too, but no public mention.

**Public APIs:**

- LinkedIn historically had a broad REST API for third-party developers (with OAuth). Around 2015 they significantly **restricted open API usage**, limiting it mostly to **partnership integrations** (like certain approved partners in recruitment, sales, etc., plus some open endpoints like Share on LinkedIn).
- The public REST API endpoints that remain (like profile retrieval, posting shares) are accessed via OAuth 2.0 tokens and have strict throttling.
- Many of LinkedIn’s external use-cases now are via official partner programs (e.g., companies can integrate LinkedIn Learning or Ads via APIs).
- The need for flexible queries for partners led LinkedIn to consider GraphQL externally. As per the 2022 blog, they built a **GraphQL Gateway for integrations** ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)).

**GraphQL at LinkedIn:**

- **Use Case:** LinkedIn’s partnerships often required custom APIs to fetch various LinkedIn data in one go (for example, a partner might want to retrieve a company’s LinkedIn page stats and recent posts and employees’ info in one query). Previously, LinkedIn would create a new REST endpoint for each integration need (option #1 mentioned) or add more fields to existing ones (option #2) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=Every%20customer%20looking%20to%20integrate,ROI%20use%20cases%20could%20overcome)). This was slow and costly.
- By adopting **GraphQL**, LinkedIn allowed partners to query multiple data types in one request, specifying exactly what fields they need ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=Every%20customer%20looking%20to%20integrate,ROI%20use%20cases%20could%20overcome)). This accelerated development by an estimated **90%** for new API use cases since they didn’t have to implement new endpoints for each variation ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=LinkedIn%E2%80%99s%20GraphQL%20journey%20for%20integrations,we%20accelerated%20development%20by%2090)) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=Every%20customer%20looking%20to%20integrate,ROI%20use%20cases%20could%20overcome)).
- **Architecture:** They likely set up a GraphQL service (maybe using Apollo or similar) as an interface for external developers. This GraphQL layer then calls internal Rest.li services to fetch data and composes the result.
  - For example, a GraphQL query might request `member(id: "123") { firstName, lastName, positions { companyName, title } }`. The GraphQL resolver will call the Profile service for basic info and the Positions service (or use an existing combined API if available) to get job info, then combine.
  - LinkedIn needed to ensure GraphQL doesn’t overload their system (GraphQL queries can be expensive if not controlled). They likely have **query complexity limits** or required queries to use certain predetermined patterns for efficiency.
- **Internal adoption of GraphQL:** The blog focuses on external integration. However, after building that, LinkedIn might find GraphQL useful internally for their own web/mobile front-ends. For instance, the mobile app might query a GraphQL endpoint to get a feed plus profile highlights in one call, rather than multiple REST calls. It’s plausible LinkedIn’s mobile “Voyager” architecture considered GraphQL for efficiency. If so, they would implement an internal GraphQL service that aggregates multiple Rest.li endpoints for the app. There’s no direct confirmation in the blog, but many organizations have moved their frontends to GraphQL, and LinkedIn could too.
- **GraphQL Advantages Realized:** The result was fewer distinct APIs to maintain and faster time to support partner needs ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=Every%20customer%20looking%20to%20integrate,ROI%20use%20cases%20could%20overcome)). GraphQL’s flexibility meant they could expose a broader schema and let partners self-serve their data needs (within allowed scopes).
- **Security in GraphQL:** LinkedIn ensures data permissions are enforced in resolvers. For example, just because GraphQL allows querying connections doesn’t mean any partner can see any user’s connections – the GraphQL layer would enforce same rules as REST API did (only certain data with appropriate tokens).

**API Evolution Example:** LinkedIn’s own website uses internal APIs. Historically, loading the homepage would call a variety of REST endpoints (profile summary, feed fetch, notifications count, etc.). They evolved to a more consolidated approach (e.g., a single `FeedFetch` call that got multiple pieces). Possibly now they use GraphQL from the web client to get a bunch of initial data in one go. They might not publicly confirm, but given trends, it’s likely.

**GraphQL Journey Outcome:** The internal blog reported a major acceleration in delivering new external API solutions by using GraphQL ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=LinkedIn%E2%80%99s%20GraphQL%20journey%20for%20integrations,we%20accelerated%20development%20by%2090)). This suggests that now when a new partner needs some data, they mostly just have to add a GraphQL query on the consumer side (the GraphQL schema might already include the needed types or they extend it easily) rather than engineering a whole pipeline for it.

**OpenAPI and Standards:** LinkedIn might also utilize standard API description languages (Rest.li has its own IDL, and GraphQL has its schema). They could generate client libraries from these, simplifying integration for internal teams and external devs.

**API Rate Limiting and Quotas:** Both internal and external APIs have quotas. External ones obviously have tight rate limits to prevent abuse. Internal ones too – for example, if one service calls another extremely rapidly (maybe due to a bug), D2 or the service itself might enforce a limit to avoid cascading failure. They likely have circuit breakers as discussed, which interplay with the API usage.

**Deprecation and Versioning:**

- With thousands of API endpoints, LinkedIn must manage versioning. Rest.li supports optional projection of fields, etc., to avoid version explosion. They can add fields without breaking old clients (old clients just ignore new JSON fields). If major changes needed, they might version the resource.
- For public APIs, they have explicit version numbers in URLs for major changes.
- GraphQL versioning is trickier (best practice is usually not to version GraphQL but evolve the schema carefully). They can add new fields, deprecate old ones gradually. GraphQL introspection can list deprecated fields to signal partners to migrate.

**APIs and Open Source:** Rest.li itself was open-sourced by LinkedIn (it’s on GitHub, and a few companies use it). GraphQL contributions – LinkedIn may contribute to the GraphQL community by sharing how they scaled it, but GraphQL itself is open spec.

In summary, LinkedIn’s API architecture features:

- **Rest.li** for internal microservice communication, enabling consistent high-volume REST APIs across services ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=When%20we%20transformed%20from%20Leo,Restful%20API%20model%20%2028)).
- **Dynamic discovery and client-side load balancing** via D2 to manage service endpoints at scale ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=clients%20utilizing%20Python%2C%20Ruby%2C%20Node,scalability%20of%20each%20service%20API)).
- **Public APIs** locked down (Continuing summary on public/internal APIs)

- **Public APIs** locked down for selective use, now increasingly served via a powerful GraphQL interface to meet partner needs flexibly ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=Every%20customer%20looking%20to%20integrate,ROI%20use%20cases%20could%20overcome)). This has **significantly accelerated external API development (by ~90%)** by reducing the need for custom endpoints ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=LinkedIn%E2%80%99s%20GraphQL%20journey%20for%20integrations,we%20accelerated%20development%20by%2090)) ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=accelerated%20development%20by%2090)).
- **GraphQL adoption** for integrations (and possibly internal clients) to allow fetching exactly the needed data with a single request, improving efficiency and developer experience ([LinkedIn’s GraphQL journey for integrations and partnerships](https://www.linkedin.com/blog/engineering/infrastructure/linkedin-s-graphql-journey-for-integrations-and-partnerships#:~:text=developing%C2%A0a%C2%A0platform,in%20support%20of%20our%20customers)).
- A strong focus on **backwards compatibility** and **consistency** so that changes in services don't break clients – achieved through careful API versioning, use of tolerant readers/writers (as in Avro, JSON), and automated test suites for APIs.

Through these API strategies, LinkedIn ensures that whether it's the mobile app calling a dozen microservices, or a third-party partner pulling analytics data, the interactions are efficient, secure, and maintainable. The combination of Rest.li for internal robustness and GraphQL for external flexibility demonstrates how LinkedIn tailors its API approach to different audiences while building on a solid microservice API foundation.

## Open-Source Technologies Developed or Adopted by LinkedIn

LinkedIn has been both a heavy adopter of open-source technologies and a major contributor to the open-source community. In fact, several core pieces of LinkedIn’s architecture were created in-house and later open-sourced, subsequently becoming industry-standard components in their own right. This section highlights some key open-source technologies LinkedIn developed or embraced, and how they fit into the overall architecture:

- **Apache Kafka:** Kafka is perhaps LinkedIn’s most famous open-source contribution. Developed initially to unify LinkedIn’s growing set of data pipelines, it became the central pub-sub system for streaming data ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=As%20the%20site%20grew%2C%20more,over%20%2025%20500%20billion)). Kafka’s design for high throughput and durability was proven at LinkedIn (with **500 billion events per day** even by 2015 ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=concept%20of%20a%20commit%20log,handles%20well%20over%20%2025)), scaling to trillions later). LinkedIn open-sourced Kafka in 2011, and it joined Apache, now used globally. LinkedIn continues to run a customized Kafka at huge scale (7 trillion messages/day) and contributes improvements (like Kafka Monitor and Cruise Control for self-balancing) ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=LinkedIn%E2%80%99s%20scale,surpassed%207%20trillion%20per%20day)) ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=operability%20challenges%20for%20our%20overall,running%20in%20LinkedIn%E2%80%99s%20production%20environment)). Kafka’s impact on LinkedIn’s architecture is immense – it underpins everything from activity streams to metric logs, and its **commit log paradigm** influenced how LinkedIn designs data flow (immutable event streams feeding various systems).

- **Apache Samza:** Samza was LinkedIn’s answer for distributed stream processing on Kafka data ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20built%20data%20infrastructure%20that,can%20provision%20this%20infra%20automatically)). Co-created with Apache YARN, it provides a framework to build stateful stream processors that consume from Kafka and output to various stores. Samza allowed LinkedIn to handle real-time computations (like those needed for WVYP and other features) with scalability and fault-tolerance (leveraging Kafka’s replay and YARN’s resource management) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Apache%20Samza%20was%20initially%20developed,and%20Zhu%20explain%20their%20choice)). Open-sourced and Apache-incubated, Samza is used outside LinkedIn (though faces competition from Spark Streaming and Flink). Within LinkedIn, Samza’s integration with Kafka and the internal ecosystem made it a go-to for streaming needs, and LinkedIn engineers continue to refine it (e.g., adding Beam API support) ([LinkedIn Migrates away from Lambda Architecture to Reduce Complexity - InfoQ](https://www.infoq.com/news/2020/12/linkedin-lambda-architecture/#:~:text=Apache%20Samza%20was%20initially%20developed,and%20Zhu%20explain%20their%20choice)).

- **Apache Pinot:** Pinot was developed at LinkedIn as a distributed OLAP datastore for real-time analytics queries (especially site metrics and user-facing analytics like profile view counts) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)). It was open-sourced and entered Apache incubation via contributions from LinkedIn (and others like Uber). Pinot’s ability to do **slice-and-dice of large multi-dimensional data with low latency** is key for LinkedIn’s monitoring (ThirdEye) and products (e.g., “who viewed your profile” insights) ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)). By open-sourcing Pinot, LinkedIn spurred its adoption for analytics dashboards in many companies.

- **LIquid (Graph Database):** While not (yet) open-sourced as a package, LinkedIn has shared considerable detail about LIquid ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Editor%E2%80%99s%20note%3A%20In%20this%20two,free%20shared)) ([LIquid: The soul of a new graph database, Part 1](https://www.linkedin.com/blog/engineering/graph-systems/liquid-the-soul-of-a-new-graph-database-part-1#:~:text=Introducing%20LIquid%2C%20LinkedIn%E2%80%99s%20in,database)), and even presented it at QCon ([QCon San Francisco 2023 | LIquid: A Large-Scale Relational Graph Database](https://qconsf.com/presentation/oct2023/liquid-large-scale-relational-graph-database#:~:text=We%20describe%20LIquid,this%20in%20memory%20at%20scale)). It’s a unique system that hasn’t been released publicly, but LinkedIn’s publications contribute to academic and industry knowledge on graph databases. If LinkedIn ever open-sourced LIquid, it could be a significant contribution to graph processing at scale (right now, its influence is through shared concepts and learnings).

- **Rest.li:** LinkedIn open-sourced Rest.li, the REST framework that standardizes service APIs ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=When%20we%20transformed%20from%20Leo,Restful%20API%20model%20%2028)). Rest.li includes a rich client and server framework, D2 discovery, and support for dynamic schemas. While it’s a more niche project (used by LinkedIn and a few others like Pinterest), it demonstrates LinkedIn’s willingness to share internal tools. Rest.li’s concepts (like client-side load balancing and data templates) influenced other API frameworks.

- **Apache Helix:** Helix is a cluster management framework that was developed at LinkedIn (for automating partition assignment, failover in distributed systems). It became an Apache project. LinkedIn used Helix to manage resources in systems like Espresso (datastore) and maybe in Samza. Helix’s idea of a “cluster state machine” fed into later systems (even Kubernetes shares some similar goals for managing cluster state).

- **Apache Gobblin:** Gobblin, an ingestion framework, was open-sourced by LinkedIn to handle **ETL of all internal/external datasets** into HDFS or other stores ([Gobblin' Big Data With Ease | LinkedIn Engineering](https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease#:~:text=Gobblin%27%20Big%20Data%20With%20Ease,datasets%20through%20a%20single%20framework)). It is used to integrate data from various sources into LinkedIn’s data lake. Now at Apache, Gobblin is used for data pipelines in some organizations.

- **Project Voldemort:** Voldemort was an early LinkedIn open-source key-value storage (inspired by Amazon Dynamo) used for read-mostly access to precomputed data ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=reduce%20the%20load%20altogether%20by,with%20precomputed%20results%20when%20appropriate)). It was widely known in NoSQL circles in the early 2010s. Though LinkedIn has since moved to Venice for those use cases, Voldemort had an impact (you can see conceptual lineage in systems like Cassandra and in Venice itself).

- **Azka**ban: Azkaban (named after the Harry Potter prison) is LinkedIn’s batch workflow scheduler for Hadoop jobs, open-sourced early on. It’s used to orchestrate complex sequences of Hadoop jobs. While technologies like Airflow have become popular, Azkaban is still used at LinkedIn and some other companies, showing LinkedIn’s early influence in big data tooling.

- **Dr. Elephant:** A more niche tool, but LinkedIn open-sourced **Dr. Elephant** for Hadoop and Spark performance tuning. It analyzes job logs to suggest how to improve memory settings or parallelism. This was a valuable contribution for Hadoop users aiming to optimize jobs (and LinkedIn used it to optimize their own offline workflows).

- **ThirdEye:** As discussed, ThirdEye was open-sourced by LinkedIn as a platform for **anomaly detection on time-series** ([ThirdEye - LinkedIn's Business-wide monitoring platform - SlideShare](https://www.slideshare.net/slideshow/thirdeye-linkedins-businesswide-monitoring-platform/178412377#:~:text=ThirdEye%20,cause%20analysis%20across%2050%2B)). Companies needing similar monitoring capabilities can leverage it (it integrates with Pinot, also a LinkedIn contribution).

- **Others:** LinkedIn has contributed or initiated many other projects:
  - **Burrow** (Kafka consumer lag monitoring).
  - **Cruise Control** (Kafka load balancing automation) ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=,usage%20monitor%20called%20%E2%80%9CBean%20Counter%E2%80%9D)).
  - **Peegit** (I think a gradle plugin for dependency analysis).
  - **Tony** (TensorFlow on YARN) for ML jobs ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=setup%20across%20multiple%20datacenters%20and,provides%20very%20cheap%20storage)).
  - **Feathr** (feature store) recently open-sourced to LF AI Data Foundation ([Open sourcing Feathr – LinkedIn's feature store for productive ...](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-feathr--linkedin-s-feature-store-for-productive-m#:~:text=,management%20and%20improve%20developer%20productivity)).
  - **Dust.js** (templating for front-end, LinkedIn’s early UI work).
  - **PyGradle** (a Python build system using Gradle).

LinkedIn not only built these for itself but released them, often donating to Apache or Linux Foundation, which in turn garners community improvements. This open-source engagement has strategic advantages:

- It allows LinkedIn to influence the direction of technologies critical to them (ensuring Kafka evolves to meet their needs, for example).
- By open-sourcing, they attract talent interested in these technologies and gain credibility in the engineering community.

**Adoption of External Open-Source:** LinkedIn also adopts many open-source tools:

- They use **Apache Lucene** in search services ([FollowFeed: LinkedIn's Feed Made Faster and Smarter](https://www.linkedin.com/blog/engineering/feed/followfeed-linkedin-s-feed-made-faster-and-smarter#:~:text=Image%3A%20Sensei%3A%20Previous%20Feed%20Infrastructure)), **Jetty**/Netty for web servers likely, **MySQL** for some data, **Zookeeper** for D2 and Kafka coordination, etc.
- They’ve likely adopted newer tech over time: perhaps **Kubernetes** (though not public, as reasoned, likely yes internally by now), **Prometheus** (maybe not, if they have their own metrics stack), and front-end frameworks (Ember.js was used, maybe now React for web).
- On the AI side, using **TensorFlow** and **PyTorch** (with TonY to run them on YARN) ([This is the Architecture Powering Machine Learning at LinkedIn](https://www.linkedin.com/pulse/architecture-powering-machine-learning-linkedin-jesus-rodriguez#:~:text=setup%20across%20multiple%20datacenters%20and,provides%20very%20cheap%20storage)) and leveraging any open-source ML libraries.
- For GraphQL, they possibly used **Apollo GraphQL** server or built on open libraries for that integration.
- They also incorporate open libraries in microservices (Guice, Jackson JSON, etc.). They mention Java and also “Python, Ruby, Node.js, C++” being used in some services ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=Restful%20API%20model%20across%20the,company)) – presumably via REST clients. That implies adopting open-source stacks for non-Java services as well.

**Open-Source Collaboration:** LinkedIn’s culture includes participating in open-source conferences (Kafka Summit, ApacheCon, etc.), which helps ensure the tools they rely on remain robust. For instance, their Kafka engineers collaborate with Confluent and the Apache community to add features needed for multi-tenancy and security that LinkedIn cares about ([How LinkedIn customizes Apache Kafka for 7 trillion messages per day](https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages#:~:text=operability%20challenges%20for%20our%20overall,running%20in%20LinkedIn%E2%80%99s%20production%20environment)).

In summary, LinkedIn’s major open-source technologies and contributions include:

- **Kafka** – streaming backbone of modern architectures ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=result%20was%20the%20development%20of,handles%20well%20over%20%2025)).
- **Samza** – real-time stream processing framework ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=We%E2%80%99ve%20built%20data%20infrastructure%20that,can%20provision%20this%20infra%20automatically)).
- **Pinot** – real-time OLAP analytics store ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20builds%20upon%20Apache%20Pinot%2C,business%20metrics%20generated%20at%20LinkedIn)).
- **Rest.li** – REST API framework enabling microservices at scale ([A Brief History of Scaling LinkedIn | LinkedIn Engineering](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin#:~:text=this%2C%20we%20built%20out%20a,Restful%20API%20model%20%2028)).
- **Venice** – derived data store for AI/ML features (open-sourced in 2022) ([Open Sourcing Venice: LinkedIn’s Derived Data Platform](https://www.linkedin.com/blog/engineering/open-source/open-sourcing-venice-linkedin-s-derived-data-platform#:~:text=We%20are%20proud%20to%20announce,batch%20and%20stream%20processing%20jobs)).
- **Gobblin** – universal data ingestion framework ([Gobblin' Big Data With Ease | LinkedIn Engineering](https://engineering.linkedin.com/data-ingestion/gobblin-big-data-ease#:~:text=Gobblin%27%20Big%20Data%20With%20Ease,datasets%20through%20a%20single%20framework)).
- **Azkaban** – workflow scheduler for big data pipelines.
- **ThirdEye** – monitoring and anomaly detection platform ([Introducing ThirdEye: LinkedIn’s Business-Wide Monitoring Platform](https://www.linkedin.com/blog/engineering/analytics/introducing-thirdeye-linkedins-business-wide-monitoring-platfor#:~:text=ThirdEye%20is%20a%20comprehensive%20platform,analysis%20results%20through%20user%20interaction)).
- and numerous support tools (Helix, Burrow, Cruise Control, Dr. Elephant, etc.).

These open-source projects not only solved LinkedIn’s immediate challenges but also influenced the architectures of many other companies who adopted them. They showcase LinkedIn’s engineering prowess and its philosophy of sharing solutions with the wider community, which in turn often results in a stronger ecosystem around the technologies LinkedIn bets on.

---

Finally, bringing it all together, the architecture of LinkedIn as we have dissected is an interplay of all these components – from global data centers running microservices communicating via Rest.li, to data streams flowing through Kafka into offline and real-time processors, feeding intelligent products powered by machine learning models, all monitored and improved through sophisticated tooling and experimentation. LinkedIn’s architecture has evolved significantly over the years, but its core principles remain: **distributed, scalable, resilient systems; a commitment to performance; a data-driven mindset; and leveraging both homegrown and open-source technology to connect the world’s professionals**.
